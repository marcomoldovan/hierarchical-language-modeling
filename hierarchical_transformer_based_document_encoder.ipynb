{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hierarchical-transformer-based-document-encoder.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "kpwbDUkHtLu9",
        "-B019eaBuLMx",
        "oItvz3k-uFSp",
        "gQzW6fxmtWEN",
        "gmX65TyjwxM-",
        "wO88nttwP0hi",
        "Ll4GDtfJKg76",
        "ZYYMKT3ILK5j",
        "YmWFH0wZQMeF",
        "Pw55fnznsNp5",
        "UmWmUTykN22p",
        "2Vx6Jhu7oRW_",
        "W8MVxIXFoYEI",
        "xYWwncTdp0sg",
        "Qgh7LvX2RWME",
        "kmJT1yF3i9k8",
        "LvdSZfthl-QN",
        "aFQ_HrliRfml",
        "ue_TauSD2gWp",
        "oyPDLz_kN7uE",
        "pq3B99GAzqT3",
        "Ika4rqTyztVJ",
        "bPOCt2ECzw9Q",
        "ah6qLiazN-xP",
        "aVbDO-544Hb6",
        "ZJqgfSbh4OYN",
        "7IVJdy-_qAPt",
        "F3evzCMPzdeL",
        "PSXwRXFRziOi",
        "YleJunzjQSh-",
        "_DriQikaMnxv",
        "wBnJKkLtc7YE",
        "-Tj1ZTCxp-Tq",
        "5IO_CxT4c1Kb",
        "icXGFw3kbKlY",
        "ALE7RCZ0y3B2",
        "3BDd9NdYtigI",
        "wm_2yiWg0hGO",
        "pcvB0xjtQtNE",
        "NlY-Oj9GX-v8",
        "gnjX9LYNahuY",
        "-qRaLtACAi-S",
        "c4vLsZn8fp9m",
        "1NS1cnQeFn4_",
        "0RtbFbHdFq18",
        "XBc__W4vyL5U"
      ],
      "machine_shape": "hm",
      "mount_file_id": "1b4pp9mpqeumjRnUEMkhywInYeic480uk",
      "authorship_tag": "ABX9TyPkBZPYyl3f6wfJEPIJhfsW",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcomoldovan/hierarchical-text-encoder/blob/master/hierarchical_transformer_based_document_encoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz5UHkyUWWrx"
      },
      "source": [
        "# **Hierarchical Transformer-based Text Encoder** \n",
        "\n",
        "We present the multi-purpose Hierarchical Attention-based Text Encoder (HATE), a language model for learning long-form document representations and contextualized sentence representations. Our model employs a Transformer module on two separate levels with the goal of capturing contextual information between words as well as between sentences. Our model offers theoretical plausibility from a linguistic point of view as we are essentially trying to model multi-level compositionality. We introduce a pre-training scheme that uses masked language modeling on two levels in order to train the model to capture general linguistic features. After that we fine-tune our model on a document ranking task using the MS MARCO dataset which allows us to compare our performance to many other models. We detect baseline-beating performance in our fine-tuned models but suspect severe undertraining. Later in this work we will explore the contextual interactions and resulting representations visually."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6LstRZsO_YI"
      },
      "source": [
        "# **1.** Introduction\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpwbDUkHtLu9"
      },
      "source": [
        "# **2.** Background"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-B019eaBuLMx"
      },
      "source": [
        "## 2.1 Statistical foundations of machine learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRjb0HdQRGPW"
      },
      "source": [
        "(1) https://towardsdatascience.com/the-statistical-foundations-of-machine-learning-973c356a95f\n",
        "\n",
        "More on regression, classification, etc. in a probabilistic context. Show that these problems are essentially MAP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oItvz3k-uFSp"
      },
      "source": [
        "## 2.2 Mathematics of optimization for deep learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACsq9qRNRK7V"
      },
      "source": [
        "(1) https://towardsdatascience.com/the-mathematics-of-optimization-for-deep-learning-11af2b1fda30\n",
        "\n",
        "- Optimization: (1) visualizing loss landscape https://arxiv.org/pdf/1712.09913.pdf\n",
        "- Momentum based optimizers\n",
        "- Dropout (1) https://arxiv.org/pdf/1207.0580.pdf\n",
        "- Batch normalization (1) https://arxiv.org/abs/1502.03167 (2) https://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf\n",
        "- Weight initialization\n",
        "- Reguralization (1) NLP specific: https://mlexplained.com/2018/03/02/regularization-techniques-for-natural-language-processing-with-code-examples/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQzW6fxmtWEN"
      },
      "source": [
        "## 2.3 Representation Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejoRhg9wWkOK"
      },
      "source": [
        "Touch briefly on the theory of representation learning, independent of language. Main focus on the Bengio paper: https://arxiv.org/pdf/1206.5538.pdf Also refer to representation learning slides from DL&AI course from last semester as an appropriate introduction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmX65TyjwxM-"
      },
      "source": [
        "## 2.4 Language Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxwbJYXIQUpq"
      },
      "source": [
        "- Language modeling via auto-encoding or auto-regressive methods in general\n",
        "\n",
        "\n",
        "- Embeddings in Language Models (1) https://jalammar.github.io/skipgram-recommender-talk/Text (2) https://dspace.mit.edu/handle/1721.1/118079\n",
        "-  Word embeddings (1) https://ruder.io/word-embeddings-1/index.html (2) https://ruder.io/word-embeddings-softmax/index.html (3) https://ruder.io/secret-word2vec/index.html (4) https://ruder.io/word-embeddings-2017/index.html (5) https://jalammar.github.io/illustrated-word2vec/ (6) Glove https://mlexplained.com/2018/04/29/paper-dissected-glove-global-vectors-for-word-representation-explained/ (7) ELMo https://mlexplained.com/2018/06/15/paper-dissected-deep-contextualized-word-representations-explained/ (8) https://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html (9) https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html#loss-functions\n",
        "- Sentence embeddings (1) https://supernlp.github.io/2018/11/26/sentreps/ (2) https://mlexplained.com/2017/12/28/an-overview-of-sentence-embedding-methods/ (3) https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a\n",
        "- Document Embeddings (1) https://towardsdatascience.com/document-embedding-techniques-fed3e7a6a25d#1409 (2) https://graphaware.com/nlp/2018/09/03/advanced-document-representation.html\n",
        "\n",
        "- General early language models that are based on DL: RNNs/LSTM (touch shortly) (1) https://arxiv.org/pdf/1312.6026.pdf (2) https://distill.pub/2019/memorization-in-rnns/\n",
        "- On the diffictuly of training recurrent neural networks https://arxiv.org/pdf/1211.5063.pdf\n",
        "- Transition to attention mechanism, at first in RNNs\n",
        "- Why these large DL-based models are so important for transfer learning in NLP (1) https://ruder.io/transfer-learning/ (2) https://thegradient.pub/nlp-imagenet/ (3) very linguistic study of word embeddings for transfer tasks https://arxiv.org/pdf/1903.08855.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wO88nttwP0hi"
      },
      "source": [
        "# **3.** Related Work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZLIP6O_KRQP"
      },
      "source": [
        "## 3.1 Attention and why it's all you need"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACcPMJQLOvQU"
      },
      "source": [
        "- Attention is all you need (1) https://mlexplained.com/2017/12/29/attention-is-all-you-need-explained/ (2) https://arxiv.org/pdf/1706.03762.pdf (3) https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec (4) https://blog.floydhub.com/attention-mechanism/ (5) https://jalammar.github.io/illustrated-transformer/ (6) https://distill.pub/2016/augmented-rnns/ (7) https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html\n",
        "- BERT (1) https://mlexplained.com/2019/01/07/paper-dissected-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-explained/ (2) https://arxiv.org/pdf/1810.04805.pdf (3) https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/ (4) https://jalammar.github.io/illustrated-bert/\n",
        "- What Does BERT Look At? An Analysis of BERT’s Attention \n",
        "https://arxiv.org/pdf/1906.04341.pdf https://arxiv.org/abs/1909.10430v2\n",
        "- BERT raw embeddings https://arxiv.org/abs/1909.00512v1 https://towardsdatascience.com/examining-berts-raw-embeddings-fd905cb22df7\n",
        "- Sentence embeddings revisited: BERT methods -> transition to intuition for my document representation model\n",
        "- Call-back to section 2.4 where I touch on classical embeddings and compare to deep, high-parameter, attention based models such as BERT\n",
        "- Why these large pretrained models are so important for transfer learning in NLP (1) https://ruder.io/transfer-learning/ (2) https://thegradient.pub/nlp-imagenet/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ll4GDtfJKg76"
      },
      "source": [
        "## 3.2 Hierarchical Attention-Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R44qFjnbj6o5"
      },
      "source": [
        "- SMITH (1) https://arxiv.org/pdf/2004.12297v1.pdf (2) https://github.com/dmolony3/SMITH\n",
        "- HIBERT (1) https://arxiv.org/pdf/1905.06566.pdf (2) https://github.com/liangsi03/hibert_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYYMKT3ILK5j"
      },
      "source": [
        "## 3.3 Information Retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhoR7CDFbCS2"
      },
      "source": [
        "- TANDA: Transfer and Adapt Pre-Trained Transformer Modelsfor Answer Sentence Selection https://arxiv.org/pdf/1911.04118.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmWFH0wZQMeF"
      },
      "source": [
        "# **4.** The Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pw55fnznsNp5"
      },
      "source": [
        "#### Basic imports and installs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBIOm4PPUx39"
      },
      "source": [
        "Introduce all necessary top-level imports and install the transformer module from huggingface"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hv0fZAmgRgTt",
        "outputId": "dced8b6c-3e26-424b-ff5e-e511634da2cf"
      },
      "source": [
        "!pip install transformers #installs transformer module from huggingface\n",
        "!pip install datasets #installs dataset module from huggingface\n",
        "!pip install tokenizers #installs tokenizer module from huggingface\n",
        "!pip install torchsummary #for easy inspection of model/layer sizes and shapes\n",
        "!pip install bertviz #for attention weight visualization\n",
        "\n",
        "import re\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk\n",
        "from nltk import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as f\n",
        "import torchtext.datasets\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import transformers\n",
        "from transformers import BertConfig, BertModel, BertForMaskedLM\n",
        "from transformers.models.bert.modeling_bert import BertPreTrainedModel\n",
        "from transformers import configuration_utils\n",
        "from transformers.activations import ACT2FN \n",
        "from transformers.file_utils import ModelOutput\n",
        "from transformers.modeling_utils import (PreTrainedModel, \n",
        "                                         apply_chunking_to_forward, \n",
        "                                         find_pruneable_heads_and_indices,\n",
        "                                         prune_linear_layer,)\n",
        "from transformers.configuration_utils import PretrainedConfig\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "from torchsummary import summary\n",
        "\n",
        "from bertviz import head_view\n",
        "from bertviz import model_view\n",
        "from bertviz import neuron_view\n",
        "\n",
        "import plotly\n",
        "import plotly.graph_objs as go\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from pprint import pprint\n",
        "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\r\u001b[K     |▏                               | 10kB 24.3MB/s eta 0:00:01\r\u001b[K     |▎                               | 20kB 29.8MB/s eta 0:00:01\r\u001b[K     |▌                               | 30kB 24.6MB/s eta 0:00:01\r\u001b[K     |▋                               | 40kB 18.0MB/s eta 0:00:01\r\u001b[K     |▉                               | 51kB 10.5MB/s eta 0:00:01\r\u001b[K     |█                               | 61kB 12.0MB/s eta 0:00:01\r\u001b[K     |█▏                              | 71kB 10.7MB/s eta 0:00:01\r\u001b[K     |█▎                              | 81kB 11.9MB/s eta 0:00:01\r\u001b[K     |█▍                              | 92kB 12.1MB/s eta 0:00:01\r\u001b[K     |█▋                              | 102kB 10.0MB/s eta 0:00:01\r\u001b[K     |█▊                              | 112kB 10.0MB/s eta 0:00:01\r\u001b[K     |██                              | 122kB 10.0MB/s eta 0:00:01\r\u001b[K     |██                              | 133kB 10.0MB/s eta 0:00:01\r\u001b[K     |██▎                             | 143kB 10.0MB/s eta 0:00:01\r\u001b[K     |██▍                             | 153kB 10.0MB/s eta 0:00:01\r\u001b[K     |██▌                             | 163kB 10.0MB/s eta 0:00:01\r\u001b[K     |██▊                             | 174kB 10.0MB/s eta 0:00:01\r\u001b[K     |██▉                             | 184kB 10.0MB/s eta 0:00:01\r\u001b[K     |███                             | 194kB 10.0MB/s eta 0:00:01\r\u001b[K     |███▏                            | 204kB 10.0MB/s eta 0:00:01\r\u001b[K     |███▍                            | 215kB 10.0MB/s eta 0:00:01\r\u001b[K     |███▌                            | 225kB 10.0MB/s eta 0:00:01\r\u001b[K     |███▋                            | 235kB 10.0MB/s eta 0:00:01\r\u001b[K     |███▉                            | 245kB 10.0MB/s eta 0:00:01\r\u001b[K     |████                            | 256kB 10.0MB/s eta 0:00:01\r\u001b[K     |████▏                           | 266kB 10.0MB/s eta 0:00:01\r\u001b[K     |████▎                           | 276kB 10.0MB/s eta 0:00:01\r\u001b[K     |████▌                           | 286kB 10.0MB/s eta 0:00:01\r\u001b[K     |████▋                           | 296kB 10.0MB/s eta 0:00:01\r\u001b[K     |████▊                           | 307kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████                           | 317kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████                           | 327kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 337kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 348kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 358kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 368kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 378kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████                          | 389kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 399kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 409kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 419kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 430kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 440kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████                         | 450kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 460kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 471kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 481kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 491kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 501kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 512kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 522kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 532kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 542kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 552kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 563kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████                       | 573kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████                       | 583kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 593kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 604kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 614kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 624kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 634kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████                      | 645kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 655kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 665kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 675kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 686kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 696kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 706kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 716kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 727kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 737kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 747kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 757kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████                    | 768kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████                    | 778kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 788kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 798kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 808kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 819kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 829kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 839kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 849kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 860kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 870kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 880kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 890kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 901kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 911kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 921kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 931kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 942kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 952kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 962kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 972kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 983kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 993kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 1.0MB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 1.0MB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████                | 1.0MB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████                | 1.0MB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 1.0MB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 1.1MB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 1.1MB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 1.1MB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 1.1MB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.1MB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 1.1MB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 1.1MB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 1.1MB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 1.1MB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 1.1MB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.2MB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 1.2MB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 1.2MB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 1.2MB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 1.2MB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 1.2MB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.2MB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.2MB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 1.2MB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.2MB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.3MB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 1.3MB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.3MB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.3MB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.3MB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.3MB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.3MB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.3MB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.3MB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.4MB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.4MB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.4MB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.4MB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.4MB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.4MB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.4MB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.4MB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.4MB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.4MB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.5MB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.5MB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.5MB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.5MB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.5MB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.5MB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.5MB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.5MB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.5MB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.5MB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.6MB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.6MB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.6MB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.6MB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.6MB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.6MB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.6MB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.6MB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.6MB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.6MB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.7MB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.7MB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.7MB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.7MB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.7MB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.7MB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.7MB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.7MB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.7MB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.8MB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.8MB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.8MB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.8MB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.8MB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.8MB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.8MB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.8MB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.8MB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.8MB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.9MB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.9MB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.9MB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.9MB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.9MB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.9MB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.9MB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.9MB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.9MB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.9MB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 2.0MB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 2.0MB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 2.0MB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 2.0MB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 2.0MB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 2.0MB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 2.0MB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 2.0MB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 2.0MB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 2.0MB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.1MB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.1MB 10.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 53.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/cd/342e584ee544d044fb573ae697404ce22ede086c9e87ce5960772084cad0/sacremoses-0.0.44.tar.gz (862kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 62.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.44-cp37-none-any.whl size=886084 sha256=227a1df1a8a70270f182bc580b8b27839fbe2c6d787d978ea6ad5c339308576c\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/fb/c0/13ab4d63d537658f448366744654323077c4d90069b6512f3c\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.44 tokenizers-0.10.2 transformers-4.5.1\n",
            "Collecting datasets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/90/43b396481a8298c6010afb93b3c1e71d4ba6f8c10797a7da8eb005e45081/datasets-1.5.0-py3-none-any.whl (192kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.11.1)\n",
            "Collecting fsspec\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/11/f7689b996f85e45f718745c899f6747ee5edb4878cadac0a41ab146828fa/fsspec-0.9.0-py3-none-any.whl (107kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 47.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/4f/0a862cad26aa2ed7a7cd87178cbbfa824fc1383e472d63596a0d018374e7/xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 52.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets) (3.10.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n",
            "Collecting huggingface-hub<0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: fsspec, xxhash, huggingface-hub, datasets\n",
            "Successfully installed datasets-1.5.0 fsspec-0.9.0 huggingface-hub-0.0.8 xxhash-2.0.2\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.7/dist-packages (0.10.2)\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.7/dist-packages (1.5.1)\n",
            "Collecting bertviz\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/8b/f4226c75b35df80504ef41399fc1569b550332e3e4796618e5669c91af55/bertviz-1.0.0-py3-none-any.whl (162kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 9.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from bertviz) (4.41.1)\n",
            "Requirement already satisfied: transformers>=2.0 in /usr/local/lib/python3.7/dist-packages (from bertviz) (4.5.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from bertviz) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from bertviz) (2.23.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 14.3MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/b3/8c889dd3d5ae47a9c4468cc20ef980adc4a16f06f0937ab33f78b58b5eda/boto3-1.17.53-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 36.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0 in /usr/local/lib/python3.7/dist-packages (from bertviz) (1.8.1+cu101)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers>=2.0->bertviz) (20.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.0->bertviz) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=2.0->bertviz) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers>=2.0->bertviz) (0.0.44)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers>=2.0->bertviz) (3.10.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.0->bertviz) (0.10.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->bertviz) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->bertviz) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->bertviz) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->bertviz) (1.24.3)\n",
            "Collecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting botocore<1.21.0,>=1.20.53\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/4e/232e261b739534e216f28d935a06c44840221c3476ebcdb411cd0fc2bf16/botocore-1.20.53-py2.py3-none-any.whl (7.4MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4MB 30.0MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/89/0cb4e92c239e6425b9b0035227b8cdf9d3d098a5c9e95632c3815df63a09/s3transfer-0.3.7-py2.py3-none-any.whl (73kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 10.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0->bertviz) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers>=2.0->bertviz) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=2.0->bertviz) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=2.0->bertviz) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=2.0->bertviz) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers>=2.0->bertviz) (3.4.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.53->boto3->bertviz) (2.8.1)\n",
            "\u001b[31mERROR: botocore 1.20.53 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: sentencepiece, jmespath, botocore, s3transfer, boto3, bertviz\n",
            "Successfully installed bertviz-1.0.0 boto3-1.17.53 botocore-1.20.53 jmespath-0.10.0 s3transfer-0.3.7 sentencepiece-0.1.95\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmWmUTykN22p"
      },
      "source": [
        "## 4.1 Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65pD0jHzthMl"
      },
      "source": [
        "#### Internal Embedding Lookup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqn5_6P2ezri"
      },
      "source": [
        "class EmbeddingsLookup(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    # Initialize the lookup matrix for input IDs, positional embeddings and token types\n",
        "    self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
        "    self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "    self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
        "    \n",
        "    # Adds to Layer Normalization and Dropout on inital word embeddings\n",
        "    self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "    self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n",
        "    self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
        "    self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
        "\n",
        "  def forward(self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None):\n",
        "    if input_ids is not None:\n",
        "      input_shape = input_ids.size()\n",
        "    else:\n",
        "      input_shape = inputs_embeds.size()[:-1]\n",
        "\n",
        "    seq_length = input_shape[1]\n",
        "\n",
        "    if position_ids is None:\n",
        "      position_ids = self.position_ids[:, :seq_length]\n",
        "\n",
        "    if token_type_ids is None:\n",
        "      token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n",
        "\n",
        "    if inputs_embeds is None:\n",
        "      inputs_embeds = self.word_embeddings(input_ids)\n",
        "    token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "\n",
        "    embeddings = inputs_embeds + token_type_embeddings\n",
        "    if self.position_embedding_type == \"absolute\":\n",
        "      position_embeddings = self.position_embeddings(position_ids)\n",
        "      embeddings += position_embeddings\n",
        "    embeddings = self.LayerNorm(embeddings)\n",
        "    embeddings = self.dropout(embeddings)\n",
        "    return embeddings"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zz7SXP6JoDlN"
      },
      "source": [
        "#### Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jfz_Kk9voObB"
      },
      "source": [
        "class EncoderMLP(nn.Module):\n",
        "  def __init__(self, config, preembedded_hidden_size):\n",
        "    super().__init__()\n",
        "\n",
        "    num_steps = 3\n",
        "    step_size = (preembedded_hidden_size-config.hidden_size)//num_steps\n",
        "\n",
        "    intermediary_1 = config.hidden_size + (num_steps-1)*step_size\n",
        "    intermediary_2 = config.hidden_size + (num_steps-2)*step_size\n",
        "\n",
        "    self.encoder = nn.Sequential(\n",
        "        nn.Linear(preembedded_hidden_size, intermediary_1),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(intermediary_1, intermediary_2),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(intermediary_2, config.hidden_size)\n",
        "    )\n",
        "\n",
        "  def forward(self, hidden_states):\n",
        "    outputs = self.encoder(hidden_states)\n",
        "    return outputs\n",
        "\n",
        "\n",
        "class DecoderMLP(nn.Module):\n",
        "  def __init__(self, config, preembedded_hidden_size):\n",
        "    super().__init__()\n",
        "\n",
        "    num_steps = 3\n",
        "    step_size = (preembedded_hidden_size-config.hidden_size)//num_steps\n",
        "\n",
        "    intermediary_1 = config.hidden_size + (num_steps-1)*step_size\n",
        "    intermediary_2 = config.hidden_size + (num_steps-2)*step_size\n",
        "\n",
        "    self.decoder = nn.Sequential(\n",
        "        nn.Linear(config.hidden_size, intermediary_2),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(intermediary_2, intermediary_1),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(intermediary_1, preembedded_hidden_size)\n",
        "    )\n",
        "\n",
        "  def forward(self, hidden_states):\n",
        "    outputs = self.decoder(hidden_states)\n",
        "    return outputs"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Vx6Jhu7oRW_"
      },
      "source": [
        "#### Encoder Stack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VOKcXtf4gml"
      },
      "source": [
        "class BaseModelOutputWithCrossAttentions(ModelOutput):\n",
        "    \"\"\"\n",
        "    Base class for model's outputs, with potential hidden states and attentions.\n",
        "    Args:\n",
        "        last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`):\n",
        "            Sequence of hidden-states at the output of the last layer of the model.\n",
        "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
        "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
        "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
        "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
        "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
        "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
        "            sequence_length, sequence_length)`.\n",
        "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
        "            heads.\n",
        "        cross_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` and ``config.add_cross_attention=True`` is passed or when ``config.output_attentions=True``):\n",
        "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
        "            sequence_length, sequence_length)`.\n",
        "            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n",
        "            weighted average in the cross-attention heads.\n",
        "    \"\"\"\n",
        "\n",
        "    last_hidden_state: torch.FloatTensor = None\n",
        "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
        "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
        "    cross_attentions: Optional[Tuple[torch.FloatTensor]] = None"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AICiXa9JfZ59"
      },
      "source": [
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
        "      raise ValueError(\n",
        "        \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "        \"heads (%d)\" % (config.hidden_size, config.num_attention_heads)\n",
        "      )\n",
        "\n",
        "    self.num_attention_heads = config.num_attention_heads\n",
        "    self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "    self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "    self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "    self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "    self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "    self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "    self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
        "    if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
        "      self.max_position_embeddings = config.max_position_embeddings\n",
        "      self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n",
        "\n",
        "  def transpose_for_scores(self, x):\n",
        "    new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "    x = x.view(*new_x_shape)\n",
        "    return x.permute(0, 2, 1, 3)\n",
        "\n",
        "  def forward(self,\n",
        "              hidden_states,\n",
        "              attention_mask=None,\n",
        "              head_mask=None,\n",
        "              encoder_hidden_states=None,\n",
        "              encoder_attention_mask=None,\n",
        "              output_attentions=False):\n",
        "    \n",
        "    mixed_query_layer = self.query(hidden_states)\n",
        "\n",
        "    # If this is instantiated as a cross-attention module, the keys\n",
        "    # and values come from an encoder; the attention mask needs to be\n",
        "    # such that the encoder's padding tokens are not attended to.\n",
        "    if encoder_hidden_states is not None:\n",
        "        mixed_key_layer = self.key(encoder_hidden_states)\n",
        "        mixed_value_layer = self.value(encoder_hidden_states)\n",
        "        attention_mask = encoder_attention_mask\n",
        "    else:\n",
        "        mixed_key_layer = self.key(hidden_states)\n",
        "        mixed_value_layer = self.value(hidden_states)\n",
        "\n",
        "    query_layer = self.transpose_for_scores(mixed_query_layer)    \n",
        "    key_layer = self.transpose_for_scores(mixed_key_layer)\n",
        "    value_layer = self.transpose_for_scores(mixed_value_layer)\n",
        "\n",
        "    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "\n",
        "    if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
        "      seq_length = hidden_states.size()[1]\n",
        "      position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n",
        "      position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n",
        "      distance = position_ids_l - position_ids_r\n",
        "      positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n",
        "      positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n",
        "\n",
        "      if self.position_embedding_type == \"relative_key\":\n",
        "        relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
        "        attention_scores = attention_scores + relative_position_scores\n",
        "      elif self.position_embedding_type == \"relative_key_query\":\n",
        "        relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
        "        relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n",
        "        attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n",
        "\n",
        "    attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "    if attention_mask is not None:\n",
        "      # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
        "      attention_scores = attention_scores + attention_mask\n",
        "\n",
        "    # Normalize the attention scores to probabilities.\n",
        "    attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "\n",
        "    # This is actually dropping out entire tokens to attend to, which might\n",
        "    # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "    attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "    # Mask heads if we want to\n",
        "    if head_mask is not None:\n",
        "      attention_probs = attention_probs * head_mask\n",
        "\n",
        "    context_layer = torch.matmul(attention_probs, value_layer)\n",
        "\n",
        "    context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "    new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "    context_layer = context_layer.view(*new_context_layer_shape)\n",
        "\n",
        "    outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
        "    return outputs\n",
        "  \n",
        "\n",
        "class SelfOutput(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "    self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "    self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "  def forward(self, hidden_states, input_tensor):\n",
        "    hidden_states = self.dense(hidden_states)\n",
        "    hidden_states = self.dropout(hidden_states)\n",
        "    hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "    return hidden_states\n",
        "\n",
        "\n",
        "class AttentionModule(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.self = SelfAttention(config)\n",
        "    self.output = SelfOutput(config)\n",
        "    self.pruned_heads = set()\n",
        "\n",
        "  def prune_head(self, heads):\n",
        "    if len(heads) == 0:\n",
        "      return\n",
        "    heads, index = find_pruneable_heads_and_indices(                            \n",
        "      heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n",
        "    )\n",
        "    # Prune linear layers\n",
        "    self.self.query = prune_linear_layer(self.self.query, index)\n",
        "    self.self.key = prune_linear_layer(self.self.key, index)\n",
        "    self.self.value = prune_linear_layer(self.self.value, index)\n",
        "    self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n",
        "\n",
        "    # Update hyper params and store pruned heads\n",
        "    self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n",
        "    self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n",
        "    self.pruned_heads = self.pruned_heads.union(heads)\n",
        "\n",
        "  def forward(self,\n",
        "              hidden_states,\n",
        "              attention_mask=None,\n",
        "              head_mask=None,\n",
        "              encoder_hidden_states=None,\n",
        "              encoder_attention_mask=None,\n",
        "              output_attentions=False):\n",
        "    self_outputs = self.self(\n",
        "            hidden_states,\n",
        "            attention_mask,\n",
        "            head_mask,\n",
        "            encoder_hidden_states,\n",
        "            encoder_attention_mask,\n",
        "            output_attentions)\n",
        "    attention_output = self.output(self_outputs[0], hidden_states)\n",
        "    outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
        "    return outputs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "    if isinstance(config.hidden_act, str):\n",
        "      self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
        "    else:\n",
        "      self.intermediate_act_fn = config.hidden_act\n",
        "\n",
        "  def forward(self, hidden_states):\n",
        "    hidden_states = self.dense(hidden_states)\n",
        "    hidden_states = self.intermediate_act_fn(hidden_states)\n",
        "    return hidden_states\n",
        "\n",
        "\n",
        "class Output(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "    self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "    self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "  def forward(self, hidden_states, input_tensor):\n",
        "    hidden_states = self.dense(hidden_states)\n",
        "    hidden_states = self.dropout(hidden_states)\n",
        "    hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "    return hidden_states\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class EncoderLayer (nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
        "    self.seq_len_dim = 1\n",
        "    self.attention = AttentionModule(config)\n",
        "    self.is_decoder = config.is_decoder\n",
        "    self.add_cross_attention = config.add_cross_attention\n",
        "    if self.add_cross_attention:\n",
        "      assert self.is_decoder, f\"{self} should be used as a decoder model if cross attention is added\"\n",
        "      self.crossattention = AttentionModule(config)\n",
        "    self.intermediate = FeedForward(config)\n",
        "    self.output = Output(config)\n",
        "\n",
        "  def forward(self,\n",
        "              hidden_states,\n",
        "              attention_mask=None,\n",
        "              head_mask=None,\n",
        "              encoder_hidden_states=None,\n",
        "              encoder_attention_mask=None,\n",
        "              output_attentions=False):\n",
        "    \n",
        "    self_attention_outputs = self.attention(hidden_states,\n",
        "                                            attention_mask,\n",
        "                                            head_mask,\n",
        "                                            output_attentions=output_attentions)\n",
        "    attention_output = self_attention_outputs[0]\n",
        "    outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
        "    if self.is_decoder and encoder_hidden_states is not None:\n",
        "      assert hasattr(self, \"crossattention\"), f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers by setting `config.add_cross_attention=True`\"\n",
        "      cross_attention_outputs = self.crossattention(attention_output,\n",
        "                                                    attention_mask,\n",
        "                                                    head_mask,\n",
        "                                                    encoder_hidden_states,\n",
        "                                                    encoder_attention_mask,\n",
        "                                                    output_attentions)\n",
        "      attention_output = cross_attention_outputs[0]\n",
        "      outputs = outputs + cross_attention_outputs[1:]  # add cross attentions if we output attention weights\n",
        "\n",
        "    layer_output = apply_chunking_to_forward(self.feed_forward_chunk,           # don't forget this!!\n",
        "                                             self.chunk_size_feed_forward, \n",
        "                                             self.seq_len_dim, attention_output)\n",
        "      \n",
        "    outputs = (layer_output,) + outputs\n",
        "    return outputs\n",
        "\n",
        "  def feed_forward_chunk(self, attention_output):\n",
        "    intermediate_output = self.intermediate(attention_output)\n",
        "    layer_output = self.output(intermediate_output, attention_output)\n",
        "    return layer_output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class EncoderStack(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.layer = nn.ModuleList([EncoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "  def forward(self,\n",
        "              hidden_states,\n",
        "              attention_mask=None,\n",
        "              head_mask=None,\n",
        "              encoder_hidden_states=None,\n",
        "              encoder_attention_mask=None,\n",
        "              output_attentions=False,\n",
        "              output_hidden_states=False,\n",
        "              return_dict=True):\n",
        "    all_hidden_states = () if output_hidden_states else None\n",
        "    all_self_attentions = () if output_attentions else None\n",
        "    all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
        "    for i, layer_module in enumerate(self.layer):\n",
        "      if output_hidden_states:\n",
        "        all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "      \n",
        "      layer_head_mask = head_mask[i] if head_mask is not None else None\n",
        "\n",
        "      if getattr(self.config, \"gradient_checkpointing\", False):\n",
        "        def create_custom_forward(module):\n",
        "          def custom_forward(*inputs):\n",
        "            return module(*inputs, output_attentions)\n",
        "          return custom_forward\n",
        "        layer_outputs = torch.utils.checkpoint.checkpoint(create_custom_forward(layer_module),\n",
        "                                                          hidden_states,\n",
        "                                                          attention_mask,\n",
        "                                                          layer_head_mask,\n",
        "                                                          encoder_hidden_states,\n",
        "                                                          encoder_attention_mask,)\n",
        "      else:\n",
        "        layer_outputs = layer_module(hidden_states,\n",
        "                                     attention_mask,\n",
        "                                     layer_head_mask,\n",
        "                                     encoder_hidden_states,\n",
        "                                     encoder_attention_mask,\n",
        "                                     output_attentions,)\n",
        "      \n",
        "      hidden_states = layer_outputs[0]\n",
        "      if output_attentions:\n",
        "        all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
        "        if self.config.add_cross_attention:\n",
        "          all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
        "\n",
        "    if output_hidden_states:\n",
        "      all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "    if not return_dict:\n",
        "      return tuple(v\n",
        "                   for v in [hidden_states, all_hidden_states, all_self_attentions, all_cross_attentions]\n",
        "                   if v is not None)\n",
        "    \n",
        "    return BaseModelOutputWithCrossAttentions(last_hidden_state=hidden_states,\n",
        "                                              hidden_states=all_hidden_states,\n",
        "                                              attentions=all_self_attentions,\n",
        "                                              cross_attentions=all_cross_attentions,)\n",
        "    \n",
        "\n",
        "class Pooler(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "    self.activation = nn.Tanh()\n",
        "\n",
        "  def forward(self, hidden_states):\n",
        "    # We \"pool\" the model by simply taking the hidden state corresponding\n",
        "    # to the first token.\n",
        "    first_token_tensor = hidden_states[:, 0]\n",
        "    pooled_output = self.dense(first_token_tensor)\n",
        "    pooled_output = self.activation(pooled_output)\n",
        "    return pooled_output"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8MVxIXFoYEI"
      },
      "source": [
        "#### Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8AsEjOE9tOo"
      },
      "source": [
        "class BaseModelOutputWithPoolingAndCrossAttentions(ModelOutput):\n",
        "  \"\"\"\n",
        "  Base class for model's outputs that also contains a pooling of the last hidden states.\n",
        "  Args:\n",
        "    last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`):\n",
        "      Sequence of hidden-states at the output of the last layer of the model.\n",
        "    pooler_output (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, hidden_size)`):\n",
        "      Last layer hidden-state of the first token of the sequence (classification token) further processed by a\n",
        "      Linear layer and a Tanh activation function. The Linear layer weights are trained from the next sentence\n",
        "      prediction (classification) objective during pretraining.\n",
        "    hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
        "      Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
        "      of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
        "      Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
        "    attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
        "      Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
        "      sequence_length, sequence_length)`.\n",
        "      Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
        "      heads.\n",
        "    cross_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` and ``config.add_cross_attention=True`` is passed or when ``config.output_attentions=True``):\n",
        "      Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
        "      sequence_length, sequence_length)`.\n",
        "      Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n",
        "      weighted average in the cross-attention heads.\n",
        "  \"\"\"\n",
        "  last_hidden_state: torch.FloatTensor = None\n",
        "  pooler_output: torch.FloatTensor = None\n",
        "  hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
        "  attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
        "  cross_attentions: Optional[Tuple[torch.FloatTensor]] = None"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXxk31lYWr0Y"
      },
      "source": [
        "class TransformerPreTrainedModel(PreTrainedModel):\n",
        "  \"\"\"\n",
        "  An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n",
        "  models.\n",
        "  \"\"\"\n",
        "  config_class = BertConfig\n",
        "  base_model_prefix = \"bert\"\n",
        "  _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
        "\n",
        "  def _init_weights(self, module):\n",
        "    \"\"\" Initialize the weights \"\"\"\n",
        "    if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "      module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "    elif isinstance(module, nn.LayerNorm):\n",
        "      module.bias.data.zero_()\n",
        "      module.weight.data.fill_(1.0)\n",
        "    if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "      module.bias.data.zero_()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lcr5I4EZcWdu"
      },
      "source": [
        "class TransformerConfig(PretrainedConfig):\n",
        "    \"\"\"\n",
        "    This is the configuration class to store the configuration of a TransformerModel. \n",
        "    It is used to instantiate a BERT model according to the specified arguments,\n",
        "    defining the model architecture. Instantiating a configuration with the defaults \n",
        "    will yield a similar configuration to that of the BERT `bert-base-uncased \n",
        "    <https://huggingface.co/bert-base-uncased>`__ architecture. Configuration objects \n",
        "    inherit from :class:`~transformers.PretrainedConfig` and can be used to control \n",
        "    the model outputs. Read the documentation from :class:`~transformers.PretrainedConfig` \n",
        "    for more information.\n",
        "    Args:\n",
        "        vocab_size (:obj:`int`, `optional`, defaults to 30522):\n",
        "            Vocabulary size of the BERT model. Defines the number of different tokens that can be represented by the\n",
        "            :obj:`inputs_ids` passed when calling :class:`~transformers.BertModel` or\n",
        "            :class:`~transformers.TFBertModel`.\n",
        "        hidden_size (:obj:`int`, `optional`, defaults to 768):\n",
        "            Dimensionality of the encoder layers and the pooler layer.\n",
        "        num_hidden_layers (:obj:`int`, `optional`, defaults to 12):\n",
        "            Number of hidden layers in the Transformer encoder.\n",
        "        num_attention_heads (:obj:`int`, `optional`, defaults to 12):\n",
        "            Number of attention heads for each attention layer in the Transformer encoder.\n",
        "        intermediate_size (:obj:`int`, `optional`, defaults to 3072):\n",
        "            Dimensionality of the \"intermediate\" (often named feed-forward) layer in the Transformer encoder.\n",
        "        hidden_act (:obj:`str` or :obj:`Callable`, `optional`, defaults to :obj:`\"gelu\"`):\n",
        "            The non-linear activation function (function or string) in the encoder and pooler. If string,\n",
        "            :obj:`\"gelu\"`, :obj:`\"relu\"`, :obj:`\"silu\"` and :obj:`\"gelu_new\"` are supported.\n",
        "        hidden_dropout_prob (:obj:`float`, `optional`, defaults to 0.1):\n",
        "            The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\n",
        "        attention_probs_dropout_prob (:obj:`float`, `optional`, defaults to 0.1):\n",
        "            The dropout ratio for the attention probabilities.\n",
        "        max_position_embeddings (:obj:`int`, `optional`, defaults to 512):\n",
        "            The maximum sequence length that this model might ever be used with. Typically set this to something large\n",
        "            just in case (e.g., 512 or 1024 or 2048).\n",
        "        type_vocab_size (:obj:`int`, `optional`, defaults to 2):\n",
        "            The vocabulary size of the :obj:`token_type_ids` passed when calling :class:`~transformers.BertModel` or\n",
        "            :class:`~transformers.TFBertModel`.\n",
        "        initializer_range (:obj:`float`, `optional`, defaults to 0.02):\n",
        "            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n",
        "        layer_norm_eps (:obj:`float`, `optional`, defaults to 1e-12):\n",
        "            The epsilon used by the layer normalization layers.\n",
        "        gradient_checkpointing (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
        "            If True, use gradient checkpointing to save memory at the expense of slower backward pass.\n",
        "        position_embedding_type (:obj:`str`, `optional`, defaults to :obj:`\"absolute\"`):\n",
        "            Type of position embedding. Choose one of :obj:`\"absolute\"`, :obj:`\"relative_key\"`,\n",
        "            :obj:`\"relative_key_query\"`. For positional embeddings use :obj:`\"absolute\"`. For more information on\n",
        "            :obj:`\"relative_key\"`, please refer to `Self-Attention with Relative Position Representations (Shaw et al.)\n",
        "            <https://arxiv.org/abs/1803.02155>`__. For more information on :obj:`\"relative_key_query\"`, please refer to\n",
        "            `Method 4` in `Improve Transformer Models with Better Relative Position Embeddings (Huang et al.)\n",
        "            <https://arxiv.org/abs/2009.13658>`__.\n",
        "    Examples::\n",
        "        >>> from transformers import BertModel, BertConfig\n",
        "        >>> # Initializing a BERT bert-base-uncased style configuration\n",
        "        >>> configuration = BertConfig()\n",
        "        >>> # Initializing a model from the bert-base-uncased style configuration\n",
        "        >>> model = BertModel(configuration)\n",
        "        >>> # Accessing the model configuration\n",
        "        >>> configuration = model.config\n",
        "    \"\"\"\n",
        "    model_type = \"bert\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size=30522,\n",
        "        hidden_size=768,\n",
        "        num_hidden_layers=12,\n",
        "        num_attention_heads=12,\n",
        "        intermediate_size=3072,\n",
        "        hidden_act=\"gelu\",\n",
        "        hidden_dropout_prob=0.1,\n",
        "        attention_probs_dropout_prob=0.1,\n",
        "        max_position_embeddings=512,\n",
        "        type_vocab_size=2,\n",
        "        initializer_range=0.02,\n",
        "        layer_norm_eps=1e-12,\n",
        "        pad_token_id=0,\n",
        "        gradient_checkpointing=False,\n",
        "        position_embedding_type=\"absolute\",\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(pad_token_id=pad_token_id, **kwargs)\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        self.hidden_act = hidden_act\n",
        "        self.intermediate_size = intermediate_size\n",
        "        self.hidden_dropout_prob = hidden_dropout_prob\n",
        "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
        "        self.max_position_embeddings = max_position_embeddings\n",
        "        self.type_vocab_size = type_vocab_size\n",
        "        self.initializer_range = initializer_range\n",
        "        self.layer_norm_eps = layer_norm_eps\n",
        "        self.gradient_checkpointing = gradient_checkpointing\n",
        "        self.position_embedding_type = position_embedding_type"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6A_DB9Tf1aP4"
      },
      "source": [
        "class TransformerBase(BertPreTrainedModel):\n",
        "  \"\"\"\n",
        "  The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n",
        "  cross-attention is added between the self-attention layers, following the architecture described in `Attention is\n",
        "  all you need <https://arxiv.org/abs/1706.03762>`__ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\n",
        "  Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.\n",
        "  To behave as an decoder the model needs to be initialized with the :obj:`is_decoder` argument of the configuration\n",
        "  set to :obj:`True`. To be used in a Seq2Seq model, the model needs to initialized with both :obj:`is_decoder`\n",
        "  argument and :obj:`add_cross_attention` set to :obj:`True`; an :obj:`encoder_hidden_states` is then expected as an\n",
        "  input to the forward pass.\n",
        "  \"\"\"\n",
        "  def __init__(self, config, add_pooling_layer=True):\n",
        "    super().__init__(config)\n",
        "    self.config = config\n",
        "\n",
        "    self.embeddings = EmbeddingsLookup(config)\n",
        "    self.encoder = EncoderStack(config)\n",
        "\n",
        "    self.pooler = Pooler(config) if add_pooling_layer else None\n",
        "\n",
        "    self.init_weights() # Don't forget this\n",
        "\n",
        "  def get_input_embeddings(self):\n",
        "    return self.embeddings.word_embeddings\n",
        "\n",
        "  def set_input_embeddings(self, value):\n",
        "    self.embeddings.word_embeddings = value\n",
        "\n",
        "  def _prune_heads(self, heads_to_prune):\n",
        "    \"\"\"\n",
        "    Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n",
        "    class PreTrainedModel\n",
        "    \"\"\"\n",
        "    for layer, heads in heads_to_prune.items():\n",
        "      self.encoder.layer[layer].attention.prune_heads(heads)\n",
        "\n",
        "  def forward(self,\n",
        "              input_ids=None,\n",
        "              attention_mask=None,\n",
        "              token_type_ids=None,\n",
        "              position_ids=None,\n",
        "              head_mask=None,\n",
        "              inputs_embeds=None,\n",
        "              encoder_hidden_states=None,\n",
        "              encoder_attention_mask=None,\n",
        "              output_attentions=None,\n",
        "              output_hidden_states=None,\n",
        "              return_dict=None,):\n",
        "    \"\"\"\n",
        "    encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
        "      Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
        "      the model is configured as a decoder.\n",
        "    encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "      Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
        "      the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n",
        "      - 1 for tokens that are **not masked**,\n",
        "      - 0 for tokens that are **masked**.\n",
        "    \"\"\"\n",
        "    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "    output_hidden_states = output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "    if input_ids is not None and inputs_embeds is not None:\n",
        "      raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
        "    elif input_ids is not None:\n",
        "      input_shape = input_ids.size()\n",
        "    elif inputs_embeds is not None:\n",
        "      input_shape = inputs_embeds.size()[:-1]\n",
        "    else:\n",
        "      raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
        "\n",
        "    device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
        "\n",
        "    if attention_mask is None:\n",
        "      attention_mask = torch.ones(input_shape, device=device)\n",
        "    if token_type_ids is None:\n",
        "      token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
        "\n",
        "    # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
        "    # ourselves in which case we just need to make it broadcastable to all heads.\n",
        "    extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device) # can we throw this away?\n",
        "\n",
        "    # If a 2D or 3D attention mask is provided for the cross-attention\n",
        "    # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
        "    if self.config.is_decoder and encoder_hidden_states is not None:\n",
        "      encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
        "      encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
        "      if encoder_attention_mask is None:\n",
        "        encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
        "      encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
        "    else:\n",
        "      encoder_extended_attention_mask = None\n",
        "\n",
        "    # Prepare head mask if needed\n",
        "    # 1.0 in head_mask indicate we keep the head\n",
        "    # attention_probs has shape bsz x n_heads x N x N\n",
        "    # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
        "    # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
        "    head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
        "\n",
        "    embedding_output = self.embeddings(input_ids=input_ids,\n",
        "                                       position_ids=position_ids,\n",
        "                                       token_type_ids=token_type_ids,\n",
        "                                       inputs_embeds=inputs_embeds)\n",
        "    encoder_outputs = self.encoder(embedding_output,\n",
        "                                   attention_mask=extended_attention_mask,\n",
        "                                   head_mask=head_mask,\n",
        "                                   encoder_hidden_states=encoder_hidden_states,\n",
        "                                   encoder_attention_mask=encoder_extended_attention_mask,\n",
        "                                   output_attentions=output_attentions,\n",
        "                                   output_hidden_states=output_hidden_states,\n",
        "                                   return_dict=return_dict,)\n",
        "    sequence_output = encoder_outputs[0]\n",
        "    pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
        "\n",
        "    if not return_dict:\n",
        "      return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
        "\n",
        "    return BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=sequence_output,\n",
        "                                                        pooler_output=pooled_output,\n",
        "                                                        hidden_states=encoder_outputs.hidden_states,\n",
        "                                                        attentions=encoder_outputs.attentions,\n",
        "                                                        cross_attentions=encoder_outputs.cross_attentions,)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYWwncTdp0sg"
      },
      "source": [
        "## 4.2 Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qgh7LvX2RWME"
      },
      "source": [
        "### 4.2.1 Input Masking Algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmJT1yF3i9k8"
      },
      "source": [
        "#### Token Masking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3pECwdHBae9"
      },
      "source": [
        "# word_mask_probability = 0.15\n",
        "# replace_with_mask_probability = 0.8\n",
        "# replace_randomly_probability = 0.1\n",
        "# keep_token_probability = 0.1\n",
        "\n",
        "def mask_input_ids(inputs: torch.tensor,\n",
        "                   #tokenizer: transformers.BertTokenizerFast,\n",
        "                   special_tokens_mask: Optional[torch.Tensor] = None,\n",
        "                   word_mask_probability = 0.15,\n",
        "                   replace_with_mask_probability = 0.8,\n",
        "                   replace_randomly_probability = 0.1,\n",
        "                   keep_token_probability = 0.1\n",
        "                   ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "  \"\"\"\n",
        "  We specifiy the probability with which to mask token for the language modeling\n",
        "  task. Generally 15% of tokens are considered for masking. If we just mask \n",
        "  naively then a problem arises: some masked token will never actually have been \n",
        "  seen at fine-tuning. The solution is to not replace the token with [MASK] 100%\n",
        "  of the time, instead:\n",
        "  - 80% of the time, replace the token with [MASK]\n",
        "    went to the store -> went to the [MASK]\n",
        "  - 10% of the time, replace random token\n",
        "    went to the store -> went to the running\n",
        "  - 10% of the time, keep same\n",
        "    went to the store -> went to the store\n",
        "  The same principle is also appilicable with masked sentence prediction, only\n",
        "  that we have to establish a sentence vocabulary beforehand\n",
        "\n",
        "  Args:\n",
        "    inputs: tensor, containing all the token IDs\n",
        "    special_tokens_mask: tensor, denotes whether a token is a word [0] or a \n",
        "      special token [1], [CLS] tokens and padding tokens are all counted as \n",
        "      special tokens. This will be used to create a mask so that only actual\n",
        "      words are considered for random masking\n",
        "\n",
        "  Returns:\n",
        "    masked_inputs:\n",
        "    labels:\n",
        "  \"\"\"\n",
        "  labels = inputs.clone()\n",
        "  # Tensor that hold the probability values for the Bernoulli function\n",
        "  probability_matrix = torch.full(inputs.shape, word_mask_probability)\n",
        "\n",
        "  tokenizer = transformers.BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "\n",
        "  # Get special token indices in order to exclude special tokens from masking\n",
        "  if special_tokens_mask is None:\n",
        "    special_tokens_mask = [\n",
        "      tokenizer.get_special_tokens_mask(entry, already_has_special_tokens=True) for entry in labels.tolist()\n",
        "    ]\n",
        "    special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
        "  else:\n",
        "    special_tokens_mask = special_tokens_mask.bool()\n",
        "\n",
        "  # Fill the probability matrix with 0.0 values where there are special tokens\n",
        "  probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
        "  # Draws from a bernoulli distribution where probability_matrix holds the \n",
        "  # probablitites for drawing the binary random number. The probablity matrix\n",
        "  # was previously filled with 0.0 values where special tokens are present so\n",
        "  # that only tokens containing words/sentences are considered\n",
        "  masked_indices = torch.bernoulli(probability_matrix).bool()\n",
        "  # In order to compute the loss only on the masked indices all the unmasked\n",
        "  # tokens in the label tensor are set to -100\n",
        "  labels[~masked_indices] = -100\n",
        "\n",
        "  # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
        "  indices_replaced = torch.bernoulli(torch.full(labels.shape, replace_with_mask_probability)).bool() & masked_indices\n",
        "  # Since we're dealing with tensors with numerical values we convert the [MASK]\n",
        "  # token right back to its token_id representation\n",
        "  inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
        "\n",
        "  # 10% of the time, we replace masked input tokens with random word\n",
        "  indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
        "  random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n",
        "  inputs[indices_random] = random_words[indices_random]\n",
        "\n",
        "  return (inputs, labels)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvdSZfthl-QN"
      },
      "source": [
        "#### Embedding Masking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FELb70M2Ibnf"
      },
      "source": [
        "def mask_input_embeddings(input_embeddings: torch.tensor,\n",
        "                          special_embeddings_mask: torch.tensor,\n",
        "                          device,\n",
        "                          sentence_mask_probability = 0.15):\n",
        "  \"\"\"\n",
        "  Randomly masks sentences with a probability of 15%. The masked sentence\n",
        "  embeddings are replaced with a random tensor and the original embedding will\n",
        "  be stored in a labels tensor that has the same size as the input tensor. The\n",
        "  ground truth embedding will sit at the same position as is did in the input\n",
        "  tensor to make it easier to identify the correct ground truth for loss\n",
        "  computing.\n",
        "\n",
        "  Args:\n",
        "    input_embeddings: A torch.tensor containing all sentence embeddings computed\n",
        "      by the Sentence Model for a given batch. The size of the tensor is\n",
        "      [batch_size, max_doc_length, embedding_size]. Note that the documents are\n",
        "      already padded to the length of the longest document in the batch.\n",
        "    special_embeddings_mask: A torch.tensor of the same size as input_embeddings\n",
        "      [batch_size, max_doc_length] which hold 0s where there is a real sentence \n",
        "      present and 1s where there is a special token embedding, that includes \n",
        "      CLS, SEP and PAD tokens.\n",
        "  Returns:\n",
        "    masked_input_embeddings: Same shape as input embeddings, only that it holds\n",
        "      a random tensor wherever a sentence embedding was masked.\n",
        "    label_embeddings: Same shape as the masked_input_embeddings but all entries \n",
        "      are filled with 0s except where there is a masked sentence embedding. That\n",
        "      entry will be filled with the original input embedding.\n",
        "    label_mask: torch.BoolTensor\n",
        "  \"\"\"\n",
        "  masked_input_embeddings = input_embeddings.clone()\n",
        "  label_embeddings = torch.zeros_like(input_embeddings)\n",
        "  label_mask = torch.zeros_like(special_embeddings_mask)\n",
        "\n",
        "  probability_matrix = torch.full(special_embeddings_mask.shape, sentence_mask_probability, device=device)\n",
        "\n",
        "  probability_matrix.masked_fill_(special_embeddings_mask.bool(), value=0.0)\n",
        "\n",
        "  masked_indices = torch.bernoulli(probability_matrix).bool()\n",
        "\n",
        "  # Choose a random index per document to mask in case nothing was randomly masked \n",
        "  # via the Bernoulli distribution (will return None, which will lead to an error\n",
        "  # when we want to manipulate the Tensors inside the loss function)\n",
        "  if torch.sum(masked_indices.long()).item() == 0:\n",
        "    forced_mask_indexes = []\n",
        "    for document in special_embeddings_mask:\n",
        "      document_list = document.tolist()\n",
        "      real_indexes = [i for i, x in enumerate(document_list) if x == 0]\n",
        "      single_choice_per_doc = random.choice(real_indexes)\n",
        "      forced_mask_indexes.append(single_choice_per_doc)\n",
        "    for forced_index, previously_masked_doc in zip(forced_mask_indexes, masked_indices):\n",
        "      previously_masked_doc[forced_index] = True\n",
        "\n",
        "  document_counter = 0\n",
        "  sentence_counter = 0\n",
        "\n",
        "  for document in input_embeddings:\n",
        "    sentence_counter = 0\n",
        "    for sentence in document:\n",
        "      if masked_indices[document_counter][sentence_counter]:\n",
        "        label_embeddings[document_counter][sentence_counter] = input_embeddings[document_counter][sentence_counter]\n",
        "        label_mask[document_counter][sentence_counter] = 1.0\n",
        "        masked_input_embeddings[document_counter][sentence_counter] = torch.randn_like(input_embeddings[document_counter][sentence_counter])\n",
        "      sentence_counter += 1\n",
        "    document_counter += 1\n",
        "\n",
        "  label_embeddings[~masked_indices] = 0\n",
        "  label_mask = torch.Tensor.bool(label_mask)\n",
        "\n",
        "  return (input_embeddings, masked_input_embeddings, label_embeddings, label_mask)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PG4skgOqPbxd"
      },
      "source": [
        "- Algorithm for masking sentences\n",
        "- Extracting the masked token embedding from the DocumentEncoder\n",
        "- Compare vs ground truth and define loss function on it (use huggingface optimizer??)\n",
        "from transformers import AdamW #example\n",
        "optimizer = AdamW(...)\n",
        "https://huggingface.co/transformers/training.html\n",
        "- SMITH adds loss from sentence encoder + document encoder, make both trainable simultaneously???"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFQ_HrliRfml"
      },
      "source": [
        "### 4.2.2 Language Modeling Head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zC2Kn-tug0bk"
      },
      "source": [
        "Define the LM Head(s) and its loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ue_TauSD2gWp"
      },
      "source": [
        "#### Word Level Language Modeling Head"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFclI3Hu210g"
      },
      "source": [
        "class PredictionHeadTransform(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "    if isinstance(config.hidden_act, str):\n",
        "      self.transform_act_fn = ACT2FN[config.hidden_act]\n",
        "    else:\n",
        "      self.transform_act_fn = config.hidden_act\n",
        "    self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "\n",
        "  def forward(self, hidden_states):\n",
        "    hidden_states = self.dense(hidden_states)\n",
        "    hidden_states = self.transform_act_fn(hidden_states)\n",
        "    hidden_states = self.LayerNorm(hidden_states)\n",
        "    return hidden_states\n",
        "\n",
        "\n",
        "\n",
        "class LMPredictionHead(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.transform = PredictionHeadTransform(config)\n",
        "\n",
        "    # The output weights are the same as the input embeddings, but there is\n",
        "    # an output-only bias for each token.\n",
        "    self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
        "\n",
        "    self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n",
        "\n",
        "    # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`\n",
        "    self.decoder.bias = self.bias\n",
        "\n",
        "  def forward(self, hidden_states):\n",
        "    hidden_states = self.transform(hidden_states)\n",
        "    hidden_states = self.decoder(hidden_states)\n",
        "    return hidden_states\n",
        "\n",
        "\n",
        "\n",
        "class OnlyMLMHead(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.predictions = LMPredictionHead(config)\n",
        "\n",
        "  def forward(self, sequence_output):\n",
        "    prediction_scores = self.predictions(sequence_output)\n",
        "    return prediction_scores"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17qoRvtU2oEL"
      },
      "source": [
        "#### Sentence Level Language Modeling Head"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wstkTR6hOicR"
      },
      "source": [
        "class SentencePredictionHeadOutput(ModelOutput):\n",
        "  logits = None\n",
        "  probabilites = None\n",
        "  log_probabilities = None\n",
        "  labels_one_hot = None\n",
        "  per_example_loss_distance=None\n",
        "  per_example_loss_product=None\n",
        "  loss_distance=None\n",
        "  loss_product=None\n",
        "  loss_variance_distance=None\n",
        "  loss_variance_product=None"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XiNeDnD22bV"
      },
      "source": [
        "class SentencePredictionHead(nn.Module):\n",
        "  def __init__(self, config, preembedded_hidden_size):\n",
        "    super().__init__()\n",
        "    self.dense = nn.Linear(in_features=preembedded_hidden_size, out_features=preembedded_hidden_size)\n",
        "    self.LayerNorm = nn.LayerNorm(preembedded_hidden_size, eps=config.layer_norm_eps) # Compare to the word-level LM Head\n",
        "    self.config = config\n",
        "\n",
        "  def forward(self, masked_sentence_prediction, label_embeddings, label_mask):\n",
        "    \"\"\"\n",
        "    In order to compute the sentence-level prediction loss we apply a similar\n",
        "    loss function as during the word-level masked word prediction tast. Since\n",
        "    we don't have a fixed size vocabulary over the training sentences we have\n",
        "    to build a dynamic sentence vocabulary.\n",
        "    Args:\n",
        "      masked_sentence_prediction [batch_size, max_doc_length, hidden_size]:\n",
        "      label_embeddings [batch_size, max_doc_length, hidden_size]:\n",
        "      label_mask [batch_size, max_doc_length, hidden_size]:\n",
        "    Returns:\n",
        "      per_batch_sentence_loss:\n",
        "      per_example_sentence_loss:\n",
        "    \"\"\"\n",
        "    device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    label_mask = torch.Tensor.bool(label_mask)\n",
        "    predictions = masked_sentence_prediction.clone()\n",
        "    # Zero out all sentence embeddings that aren't at a masked position\n",
        "    predictions[~label_mask] = 0.0\n",
        "    label_embeddings[~label_mask] = 0.0\n",
        "    \n",
        "    # Tensors will have size [batch_size * padded_doc_length, hidden_size]\n",
        "    predictions = torch.reshape(predictions, (predictions.size()[0] * predictions.size()[1], -1)) \n",
        "    label_embeddings = torch.reshape(label_embeddings, (label_embeddings.size()[0] * label_embeddings.size()[1], -1))\n",
        "    label_mask = torch.reshape(label_mask, (label_mask.size()[0] * label_mask.size()[1], -1))\n",
        "\n",
        "    output_embedding_list = []\n",
        "    label_embedding_list = []\n",
        "    label_mask_index =  0\n",
        "\n",
        "    for mask_index in label_mask:\n",
        "      if mask_index.item():\n",
        "        output_embedding_list.append(predictions[label_mask_index])\n",
        "        label_embedding_list.append(label_embeddings[label_mask_index])\n",
        "      label_mask_index += 1\n",
        "\n",
        "    output_embeddings = torch.stack(output_embedding_list, dim=0)\n",
        "    label_embeddings = torch.stack(label_embedding_list, dim=0)\n",
        "\n",
        "\n",
        "    output_embeddings = self.dense(output_embeddings)\n",
        "    output_embeddings = self.LayerNorm(output_embeddings)\n",
        "\n",
        "    # TODO add bias like in SMITH? (smith/layers.py)\n",
        "\n",
        "    logits = torch.matmul(output_embeddings, torch.transpose(input=label_embeddings, dim0=0, dim1=1))\n",
        "    probabilities = f.softmax(logits, dim=1)\n",
        "    log_probabilities = f.log_softmax(logits, dim=1)\n",
        "    labels_one_hot = torch.diag(torch.Tensor([1] * log_probabilities.size()[0]))\n",
        "    labels_one_hot = labels_one_hot.to(device)\n",
        "\n",
        "    # Computes the pairwise distance between each row of the inputs, meaning that\n",
        "    # it computes the elementwise difference between probabilities and labels and\n",
        "    # sums them up per one prediction (row).\n",
        "    per_example_loss_distance = f.pairwise_distance(probabilities, labels_one_hot, p=1.0, keepdim=False)\n",
        "    # Another option is to compute the pairwise product per element of the log_probs\n",
        "    # and the one hot labels and sum them per prediction (romw). This emphasizes\n",
        "    # very bad predictions less and keeps the loss smaller.\n",
        "    per_example_loss_product = -torch.sum(log_probabilities * labels_one_hot, 1)\n",
        "\n",
        "    #Shape: [1]\n",
        "    numerator_distance = torch.sum(per_example_loss_distance)\n",
        "    numerator_product = torch.sum(per_example_loss_product)\n",
        "\n",
        "    # Shape: [1], small fraction added so we never divide by 0\n",
        "    denominator =  labels_one_hot.size()[0] + 1e-5\n",
        "\n",
        "    # Shape: [1]\n",
        "    loss_distance = numerator_distance / denominator\n",
        "    loss_product = numerator_product / denominator\n",
        "\n",
        "    # Shape: [1]\n",
        "    loss_variance_distance = torch.var(per_example_loss_distance)\n",
        "    loss_variance_product = torch.var(per_example_loss_product)\n",
        "\n",
        "    return SentencePredictionHeadOutput(logits=logits,\n",
        "                                        probabilites=probabilities,\n",
        "                                        log_probabilities=log_probabilities,\n",
        "                                        labels_one_hot=labels_one_hot,\n",
        "                                        per_example_loss_distance=per_example_loss_distance,\n",
        "                                        per_example_loss_product=per_example_loss_product,\n",
        "                                        loss_distance=loss_distance,\n",
        "                                        loss_product=loss_product,\n",
        "                                        loss_variance_distance=loss_variance_distance,\n",
        "                                        loss_variance_product=loss_variance_product)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGoLVfY2i1P8"
      },
      "source": [
        "### 4.2.3 Learning to Rank"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsX-qb0Ar8Kj"
      },
      "source": [
        "Ranking code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyPDLz_kN7uE"
      },
      "source": [
        "## 4.3 Sentence Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pq3B99GAzqT3"
      },
      "source": [
        "#### Sentence Model Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgNBK8LN2aho"
      },
      "source": [
        "class SentenceModelingOutput(ModelOutput): #inherits from the huggingface class\n",
        "  \"\"\"\n",
        "    Return object for Sentence Model.\n",
        "\n",
        "    Args:\n",
        "      loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`labels` is provided):\n",
        "        Masked language modeling (MLM) loss.\n",
        "      logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, config.vocab_size)`):\n",
        "        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
        "      hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
        "        Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
        "        of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
        "        Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
        "      last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`):\n",
        "        Sequence of hidden-states at the output of the last layer of the model.\n",
        "      attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
        "        Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
        "        sequence_length, sequence_length)`.\n",
        "        Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
        "        heads.\n",
        "  \"\"\"\n",
        "  loss: Optional[torch.FloatTensor] = None\n",
        "  logits: torch.FloatTensor = None\n",
        "  hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
        "  last_hidden_state: torch.FloatTensor = None\n",
        "  attentions: Optional[Tuple[torch.FloatTensor]] = None"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ika4rqTyztVJ"
      },
      "source": [
        "#### Sentence Model Confuguration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wgmalynCeMr"
      },
      "source": [
        "class HATESentenceModelConfig(PretrainedConfig):\n",
        "    \"\"\"\n",
        "    This is the configuration class to store the configuration of a TransformerModel. \n",
        "    It is used to instantiate a BERT model according to the specified arguments,\n",
        "    defining the model architecture. Instantiating a configuration with the defaults \n",
        "    will yield a similar configuration to that of the BERT `bert-base-uncased \n",
        "    <https://huggingface.co/bert-base-uncased>`__ architecture. Configuration objects \n",
        "    inherit from :class:`~transformers.PretrainedConfig` and can be used to control \n",
        "    the model outputs. Read the documentation from :class:`~transformers.PretrainedConfig` \n",
        "    for more information.\n",
        "    Args:\n",
        "        vocab_size (:obj:`int`, `optional`, defaults to 30522):\n",
        "            Vocabulary size of the BERT model. Defines the number of different tokens that can be represented by the\n",
        "            :obj:`inputs_ids` passed when calling :class:`~transformers.BertModel` or\n",
        "            :class:`~transformers.TFBertModel`.\n",
        "        hidden_size (:obj:`int`, `optional`, defaults to 768):\n",
        "            Dimensionality of the encoder layers and the pooler layer.\n",
        "        num_hidden_layers (:obj:`int`, `optional`, defaults to 12):\n",
        "            Number of hidden layers in the Transformer encoder.\n",
        "        num_attention_heads (:obj:`int`, `optional`, defaults to 12):\n",
        "            Number of attention heads for each attention layer in the Transformer encoder.\n",
        "        intermediate_size (:obj:`int`, `optional`, defaults to 3072):\n",
        "            Dimensionality of the \"intermediate\" (often named feed-forward) layer in the Transformer encoder.\n",
        "        hidden_act (:obj:`str` or :obj:`Callable`, `optional`, defaults to :obj:`\"gelu\"`):\n",
        "            The non-linear activation function (function or string) in the encoder and pooler. If string,\n",
        "            :obj:`\"gelu\"`, :obj:`\"relu\"`, :obj:`\"silu\"` and :obj:`\"gelu_new\"` are supported.\n",
        "        hidden_dropout_prob (:obj:`float`, `optional`, defaults to 0.1):\n",
        "            The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\n",
        "        attention_probs_dropout_prob (:obj:`float`, `optional`, defaults to 0.1):\n",
        "            The dropout ratio for the attention probabilities.\n",
        "        max_position_embeddings (:obj:`int`, `optional`, defaults to 512):\n",
        "            The maximum sequence length that this model might ever be used with. Typically set this to something large\n",
        "            just in case (e.g., 512 or 1024 or 2048).\n",
        "        type_vocab_size (:obj:`int`, `optional`, defaults to 2):\n",
        "            The vocabulary size of the :obj:`token_type_ids` passed when calling :class:`~transformers.BertModel` or\n",
        "            :class:`~transformers.TFBertModel`.\n",
        "        initializer_range (:obj:`float`, `optional`, defaults to 0.02):\n",
        "            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n",
        "        layer_norm_eps (:obj:`float`, `optional`, defaults to 1e-12):\n",
        "            The epsilon used by the layer normalization layers.\n",
        "        gradient_checkpointing (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
        "            If True, use gradient checkpointing to save memory at the expense of slower backward pass.\n",
        "        position_embedding_type (:obj:`str`, `optional`, defaults to :obj:`\"absolute\"`):\n",
        "            Type of position embedding. Choose one of :obj:`\"absolute\"`, :obj:`\"relative_key\"`,\n",
        "            :obj:`\"relative_key_query\"`. For positional embeddings use :obj:`\"absolute\"`. For more information on\n",
        "            :obj:`\"relative_key\"`, please refer to `Self-Attention with Relative Position Representations (Shaw et al.)\n",
        "            <https://arxiv.org/abs/1803.02155>`__. For more information on :obj:`\"relative_key_query\"`, please refer to\n",
        "            `Method 4` in `Improve Transformer Models with Better Relative Position Embeddings (Huang et al.)\n",
        "            <https://arxiv.org/abs/2009.13658>`__.\n",
        "    Examples::\n",
        "        >>> from transformers import BertModel, BertConfig\n",
        "        >>> # Initializing a BERT bert-base-uncased style configuration\n",
        "        >>> configuration = BertConfig()\n",
        "        >>> # Initializing a model from the bert-base-uncased style configuration\n",
        "        >>> model = BertModel(configuration)\n",
        "        >>> # Accessing the model configuration\n",
        "        >>> configuration = model.config\n",
        "    \"\"\"\n",
        "    model_type = \"bert\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size=30522,\n",
        "        hidden_size=128,\n",
        "        num_hidden_layers=2,\n",
        "        num_attention_heads=2,\n",
        "        intermediate_size=512,\n",
        "        hidden_act=\"gelu\",\n",
        "        hidden_dropout_prob=0.1,\n",
        "        attention_probs_dropout_prob=0.1,\n",
        "        max_position_embeddings=512,\n",
        "        type_vocab_size=2,\n",
        "        initializer_range=0.02,\n",
        "        layer_norm_eps=1e-12,\n",
        "        pad_token_id=0,\n",
        "        gradient_checkpointing=False,\n",
        "        position_embedding_type=\"absolute\",\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(pad_token_id=pad_token_id, **kwargs)\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        self.hidden_act = hidden_act\n",
        "        self.intermediate_size = intermediate_size\n",
        "        self.hidden_dropout_prob = hidden_dropout_prob\n",
        "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
        "        self.max_position_embeddings = max_position_embeddings\n",
        "        self.type_vocab_size = type_vocab_size\n",
        "        self.initializer_range = initializer_range\n",
        "        self.layer_norm_eps = layer_norm_eps\n",
        "        self.gradient_checkpointing = gradient_checkpointing\n",
        "        self.position_embedding_type = position_embedding_type"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPOCt2ECzw9Q"
      },
      "source": [
        "#### Sentence Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1QYoqpgRlef"
      },
      "source": [
        "class HATESentenceModel(TransformerPreTrainedModel):\n",
        "  \n",
        "  _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
        "  _keys_to_ignore_on_load_missing = [r\"position_ids\", r\"predictions.decoder.bias\"]\n",
        "\n",
        "  \n",
        "  def __init__(self, config, pretrain_sentence_model):\n",
        "    super().__init__(config)\n",
        "\n",
        "    if config.is_decoder:\n",
        "      logger.warning(\n",
        "        \"If you want to use this model for masked LM make sure `config.is_decoder=False` for \"\n",
        "        \"bi-directional self-attention.\"\n",
        "        )\n",
        "      \n",
        "    if pretrain_sentence_model:\n",
        "      self.transformer = TransformerBase(config, add_pooling_layer=False)\n",
        "    else:\n",
        "      self.transformer = transformers.BertModel.from_pretrained('google/bert_uncased_L-2_H-128_A-2')\n",
        "      for param in self.transformer.base_model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    self.cls = OnlyMLMHead(config)\n",
        "\n",
        "\n",
        "  def get_output_embeddings(self):\n",
        "    return self.cls.predictions.decoder\n",
        "\n",
        "  def set_output_embeddings(self, new_embeddings):\n",
        "    self.cls.predictions.decoder = new_embeddings\n",
        "\n",
        "  def forward(self,\n",
        "              input_ids=None,\n",
        "              attention_mask=None,\n",
        "              token_type_ids=None,\n",
        "              position_ids=None,\n",
        "              head_mask=None,\n",
        "              inputs_embeds=None,\n",
        "              encoder_hidden_states=None,\n",
        "              encoder_attention_mask=None,\n",
        "              labels=None,\n",
        "              output_attentions=None,\n",
        "              output_hidden_states=None,\n",
        "              return_dict=None,):\n",
        "    # TODO replace batch_size with document_length in here in the docfile\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      inputs_ids (torch.LongTensor of shape (batch_size, sequence_length)):\n",
        "        Indices of input sequence tokens in the vocabulary.\n",
        "      attention_mask (torch.FloatTensor of shape (batch_size, sequence_length), optional):\n",
        "        Mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]:\n",
        "        - 1 for tokens that are not masked,\n",
        "        - 0 for tokens that are masked.\n",
        "      token_type_ids  (torch.LongTensor of shape (batch_size, sequence_length), optional):\n",
        "        Segment token indices to indicate first and second portions of the inputs. Indices are selected in [0, 1]:\n",
        "        - 0 corresponds to a sentence A token,\n",
        "        - 1 corresponds to a sentence B token.\n",
        "      position_ids (torch.LongTensor of shape (batch_size, sequence_length), optional):\n",
        "        Indices of positions of each input sequence tokens in the position embeddings. \n",
        "        Selected in the range [0, config.max_position_embeddings - 1].\n",
        "      head_mask (torch.FloatTensor of shape (num_heads,) or (num_layers, num_heads), optional):\n",
        "        Mask to nullify selected heads of the self-attention modules. Mask values selected in [0, 1]:\n",
        "        - 1 indicates the head is not masked,\n",
        "        - 0 indicates the head is masked.\n",
        "      inputs_embeds (torch.FloatTensor of shape (batch_size, sequence_length, hidden_size), optional):\n",
        "        Optionally, instead of passing input_ids you can choose to directly pass\n",
        "         an embedded representation. This is useful if you want more control over \n",
        "         how to convert input_ids indices into associated vectors than the model’s \n",
        "         internal embedding lookup matrix.\n",
        "      encoder_hidden_states (torch.FloatTensor of shape (batch_size, sequence_length, hidden_size), optional):\n",
        "        Sequence of hidden-states at the output of the last layer of the encoder. \n",
        "        Used in the cross-attention if the model is configured as a decoder.\n",
        "      encoder_attention_mask (torch.FloatTensor of shape (batch_size, sequence_length), optional):\n",
        "        Mask to avoid performing attention on the padding token indices of the encoder \n",
        "        input. This mask is used in the cross-attention if the model is configured \n",
        "        as a decoder. Mask values selected in [0, 1]:\n",
        "        - 1 for tokens that are not masked,\n",
        "        - 0 for tokens that are masked.\n",
        "      labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "        Labels for computing the masked language modeling loss. Indices should be in ``[-100, 0, ...,\n",
        "        config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are ignored\n",
        "        (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``\n",
        "      output_attentions (bool, optional): \n",
        "        Whether or not to return the attentions tensors of all attention layers. \n",
        "        See attentions under returned tensors for more detail.\n",
        "      output_hidden_states (bool, optional):\n",
        "        Whether or not to return the hidden states of all layers. See hidden_states \n",
        "        under returned tensors for more detail.\n",
        "      return_dict (bool, optional):\n",
        "        Whether or not to return a ModelOutput instead of a plain tuple.\n",
        "    Returns:\n",
        "      SentenceModelingOutput:\n",
        "    \"\"\"\n",
        "    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "    outputs = self.transformer(input_ids,\n",
        "                               attention_mask=attention_mask,\n",
        "                               token_type_ids=token_type_ids,\n",
        "                               position_ids=position_ids,\n",
        "                               head_mask=head_mask,\n",
        "                               inputs_embeds=inputs_embeds,\n",
        "                               encoder_hidden_states=encoder_hidden_states,\n",
        "                               encoder_attention_mask=encoder_attention_mask,\n",
        "                               output_attentions=output_attentions,\n",
        "                               output_hidden_states=output_hidden_states,\n",
        "                               return_dict=return_dict)\n",
        "    \n",
        "    sequence_output = outputs[0]\n",
        "    prediction_scores = self.cls(sequence_output)\n",
        "\n",
        "    masked_lm_loss = None\n",
        "    if labels is not None:\n",
        "      loss_fct = nn.CrossEntropyLoss()  # -100 index = padding token\n",
        "      masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
        "\n",
        "    if not return_dict:\n",
        "      output = (prediction_scores,) + outputs[2:]\n",
        "      return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n",
        "\n",
        "    return SentenceModelingOutput(loss=masked_lm_loss,\n",
        "                                  logits=prediction_scores,\n",
        "                                  hidden_states=outputs.hidden_states,\n",
        "                                  last_hidden_state=sequence_output,\n",
        "                                  attentions=outputs.attentions,)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ah6qLiazN-xP"
      },
      "source": [
        "## 4.4 Document Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVbDO-544Hb6"
      },
      "source": [
        "#### Document Model Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0lUc4tXBnAv"
      },
      "source": [
        "class DocumentModelingOutput(ModelOutput):\n",
        "  \"\"\"\n",
        "    Return object for Document Model.\n",
        "\n",
        "    Args:\n",
        "      loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`labels` is provided):\n",
        "        Masked language modeling (MLM) loss.\n",
        "      logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, config.vocab_size)`):\n",
        "        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
        "      hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
        "        Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
        "        of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
        "        Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
        "      last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`):\n",
        "        Sequence of hidden-states at the output of the last layer of the model.\n",
        "      attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
        "        Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
        "        sequence_length, sequence_length)`.\n",
        "        Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
        "        heads.\n",
        "  \"\"\"\n",
        "  per_example_loss_distance: Optional[torch.FloatTensor] =None\n",
        "  per_example_loss_product: Optional[torch.FloatTensor] =None\n",
        "  loss_distance: Optional[torch.FloatTensor] = None\n",
        "  loss_product: Optional[torch.FloatTensor] = None\n",
        "  loss_variance_distance = None\n",
        "  loss_variance_product = None\n",
        "  logits: torch.FloatTensor = None\n",
        "  hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
        "  last_hidden_state: torch.FloatTensor = None\n",
        "  attentions: Optional[Tuple[torch.FloatTensor]] = None"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJqgfSbh4OYN"
      },
      "source": [
        "#### Document Model Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72NE5FLzBkuu"
      },
      "source": [
        "class HATEDocumentModelConfig(PretrainedConfig):\n",
        "    \"\"\"\n",
        "    This is the configuration class to store the configuration of a TransformerModel. \n",
        "    It is used to instantiate a BERT model according to the specified arguments,\n",
        "    defining the model architecture. Instantiating a configuration with the defaults \n",
        "    will yield a similar configuration to that of the BERT `bert-base-uncased \n",
        "    <https://huggingface.co/bert-base-uncased>`__ architecture. Configuration objects \n",
        "    inherit from :class:`~transformers.PretrainedConfig` and can be used to control \n",
        "    the model outputs. Read the documentation from :class:`~transformers.PretrainedConfig` \n",
        "    for more information.\n",
        "    Args:\n",
        "        vocab_size (:obj:`int`, `optional`, defaults to 30522):\n",
        "            Vocabulary size of the BERT model. Defines the number of different tokens that can be represented by the\n",
        "            :obj:`inputs_ids` passed when calling :class:`~transformers.BertModel` or\n",
        "            :class:`~transformers.TFBertModel`.\n",
        "        hidden_size (:obj:`int`, `optional`, defaults to 768):\n",
        "            Dimensionality of the encoder layers and the pooler layer.\n",
        "        num_hidden_layers (:obj:`int`, `optional`, defaults to 12):\n",
        "            Number of hidden layers in the Transformer encoder.\n",
        "        num_attention_heads (:obj:`int`, `optional`, defaults to 12):\n",
        "            Number of attention heads for each attention layer in the Transformer encoder.\n",
        "        intermediate_size (:obj:`int`, `optional`, defaults to 3072):\n",
        "            Dimensionality of the \"intermediate\" (often named feed-forward) layer in the Transformer encoder.\n",
        "        hidden_act (:obj:`str` or :obj:`Callable`, `optional`, defaults to :obj:`\"gelu\"`):\n",
        "            The non-linear activation function (function or string) in the encoder and pooler. If string,\n",
        "            :obj:`\"gelu\"`, :obj:`\"relu\"`, :obj:`\"silu\"` and :obj:`\"gelu_new\"` are supported.\n",
        "        hidden_dropout_prob (:obj:`float`, `optional`, defaults to 0.1):\n",
        "            The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\n",
        "        attention_probs_dropout_prob (:obj:`float`, `optional`, defaults to 0.1):\n",
        "            The dropout ratio for the attention probabilities.\n",
        "        max_position_embeddings (:obj:`int`, `optional`, defaults to 512):\n",
        "            The maximum sequence length that this model might ever be used with. Typically set this to something large\n",
        "            just in case (e.g., 512 or 1024 or 2048).\n",
        "        type_vocab_size (:obj:`int`, `optional`, defaults to 2):\n",
        "            The vocabulary size of the :obj:`token_type_ids` passed when calling :class:`~transformers.BertModel` or\n",
        "            :class:`~transformers.TFBertModel`.\n",
        "        initializer_range (:obj:`float`, `optional`, defaults to 0.02):\n",
        "            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n",
        "        layer_norm_eps (:obj:`float`, `optional`, defaults to 1e-12):\n",
        "            The epsilon used by the layer normalization layers.\n",
        "        gradient_checkpointing (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
        "            If True, use gradient checkpointing to save memory at the expense of slower backward pass.\n",
        "        position_embedding_type (:obj:`str`, `optional`, defaults to :obj:`\"absolute\"`):\n",
        "            Type of position embedding. Choose one of :obj:`\"absolute\"`, :obj:`\"relative_key\"`,\n",
        "            :obj:`\"relative_key_query\"`. For positional embeddings use :obj:`\"absolute\"`. For more information on\n",
        "            :obj:`\"relative_key\"`, please refer to `Self-Attention with Relative Position Representations (Shaw et al.)\n",
        "            <https://arxiv.org/abs/1803.02155>`__. For more information on :obj:`\"relative_key_query\"`, please refer to\n",
        "            `Method 4` in `Improve Transformer Models with Better Relative Position Embeddings (Huang et al.)\n",
        "            <https://arxiv.org/abs/2009.13658>`__.\n",
        "    Examples::\n",
        "        >>> from transformers import BertModel, BertConfig\n",
        "        >>> # Initializing a BERT bert-base-uncased style configuration\n",
        "        >>> configuration = BertConfig()\n",
        "        >>> # Initializing a model from the bert-base-uncased style configuration\n",
        "        >>> model = BertModel(configuration)\n",
        "        >>> # Accessing the model configuration\n",
        "        >>> configuration = model.config\n",
        "    \"\"\"\n",
        "    model_type = \"bert\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size=30522,\n",
        "        hidden_size=128,\n",
        "        num_hidden_layers=2,\n",
        "        num_attention_heads=2,\n",
        "        intermediate_size=512,\n",
        "        hidden_act=\"gelu\",\n",
        "        hidden_dropout_prob=0.1,\n",
        "        attention_probs_dropout_prob=0.1,\n",
        "        max_position_embeddings=512,\n",
        "        type_vocab_size=2,\n",
        "        initializer_range=0.02,\n",
        "        layer_norm_eps=1e-12,\n",
        "        pad_token_id=0,\n",
        "        gradient_checkpointing=False,\n",
        "        position_embedding_type=\"absolute\",\n",
        "        pretrained_weights=None,\n",
        "        finetuned_weights=None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(pad_token_id=pad_token_id, **kwargs)\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        self.hidden_act = hidden_act\n",
        "        self.intermediate_size = intermediate_size\n",
        "        self.hidden_dropout_prob = hidden_dropout_prob\n",
        "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
        "        self.max_position_embeddings = max_position_embeddings\n",
        "        self.type_vocab_size = type_vocab_size\n",
        "        self.initializer_range = initializer_range\n",
        "        self.layer_norm_eps = layer_norm_eps\n",
        "        self.gradient_checkpointing = gradient_checkpointing\n",
        "        self.position_embedding_type = position_embedding_type\n",
        "        self.pretrained_weights = pretrained_weights,\n",
        "        self.finetuned_weights = finetuned_weights"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1834mgo4S5k"
      },
      "source": [
        "#### Document Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIlngzQ-R91S"
      },
      "source": [
        "class HATEDocumentModel(TransformerPreTrainedModel):\n",
        "\n",
        "  _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
        "  _keys_to_ignore_on_load_missing = [r\"position_ids\", r\"predictions.decoder.bias\"]\n",
        "\n",
        "  def __init__(self, config, preembedded_hidden_size=128):\n",
        "    super().__init__(config)\n",
        "\n",
        "    self.preembedded_hidden_size = preembedded_hidden_size\n",
        "    self.config = config\n",
        "\n",
        "    if not preembedded_hidden_size == config.hidden_size:\n",
        "      self.encoder = EncoderMLP(config, preembedded_hidden_size)\n",
        "      self.decoder = DecoderMLP(config, preembedded_hidden_size)\n",
        "\n",
        "    self.transformer = TransformerBase(config, add_pooling_layer=False)\n",
        "    self.cls = SentencePredictionHead(config, preembedded_hidden_size)\n",
        "    \n",
        "  def forward(self,\n",
        "              input_ids=None,\n",
        "              attention_mask=None,\n",
        "              token_type_ids=None,\n",
        "              position_ids=None,\n",
        "              head_mask=None,\n",
        "              inputs_embeds=None,\n",
        "              encoder_hidden_states=None,\n",
        "              encoder_attention_mask=None,\n",
        "              labels_embeddings=None,\n",
        "              labels_mask=None,\n",
        "              output_attentions=None,\n",
        "              output_hidden_states=None,\n",
        "              return_dict=None,):\n",
        "    # TODO replace batch_size with document_length in here in the docfile\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      inputs_ids (torch.LongTensor of shape (batch_size, sequence_length)):\n",
        "        Indices of input sequence tokens in the vocabulary.\n",
        "      attention_mask (torch.FloatTensor of shape (batch_size, sequence_length), optional):\n",
        "        Mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]:\n",
        "        - 1 for tokens that are not masked,\n",
        "        - 0 for tokens that are masked.\n",
        "      token_type_ids  (torch.LongTensor of shape (batch_size, sequence_length), optional):\n",
        "        Segment token indices to indicate first and second portions of the inputs. Indices are selected in [0, 1]:\n",
        "        - 0 corresponds to a sentence A token,\n",
        "        - 1 corresponds to a sentence B token.\n",
        "      position_ids (torch.LongTensor of shape (batch_size, sequence_length), optional):\n",
        "        Indices of positions of each input sequence tokens in the position embeddings. \n",
        "        Selected in the range [0, config.max_position_embeddings - 1].\n",
        "      head_mask (torch.FloatTensor of shape (num_heads,) or (num_layers, num_heads), optional):\n",
        "        Mask to nullify selected heads of the self-attention modules. Mask values selected in [0, 1]:\n",
        "        - 1 indicates the head is not masked,\n",
        "        - 0 indicates the head is masked.\n",
        "      inputs_embeds (torch.FloatTensor of shape (batch_size, sequence_length, hidden_size), optional):\n",
        "        Optionally, instead of passing input_ids you can choose to directly pass\n",
        "         an embedded representation. This is useful if you want more control over \n",
        "         how to convert input_ids indices into associated vectors than the model’s \n",
        "         internal embedding lookup matrix.\n",
        "      encoder_hidden_states (torch.FloatTensor of shape (batch_size, sequence_length, hidden_size), optional):\n",
        "        Sequence of hidden-states at the output of the last layer of the encoder. \n",
        "        Used in the cross-attention if the model is configured as a decoder.\n",
        "      encoder_attention_mask (torch.FloatTensor of shape (batch_size, sequence_length), optional):\n",
        "        Mask to avoid performing attention on the padding token indices of the encoder \n",
        "        input. This mask is used in the cross-attention if the model is configured \n",
        "        as a decoder. Mask values selected in [0, 1]:\n",
        "        - 1 for tokens that are not masked,\n",
        "        - 0 for tokens that are masked.\n",
        "      labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "        Labels for computing the masked language modeling loss. Indices should be in ``[-100, 0, ...,\n",
        "        config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are ignored\n",
        "        (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``\n",
        "      output_attentions (bool, optional): \n",
        "        Whether or not to return the attentions tensors of all attention layers. \n",
        "        See attentions under returned tensors for more detail.\n",
        "      output_hidden_states (bool, optional):\n",
        "        Whether or not to return the hidden states of all layers. See hidden_states \n",
        "        under returned tensors for more detail.\n",
        "      return_dict (bool, optional):\n",
        "        Whether or not to return a ModelOutput instead of a plain tuple.\n",
        "    Returns:\n",
        "      DocumentModelingOutput:\n",
        "    \"\"\"\n",
        "\n",
        "    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "    if not self.preembedded_hidden_size == self.config.hidden_size:\n",
        "      inputs_embeds = self.encoder(inputs_embeds)\n",
        "\n",
        "    outputs = self.transformer(input_ids,\n",
        "                               attention_mask=attention_mask,\n",
        "                               token_type_ids=token_type_ids,\n",
        "                               position_ids=position_ids,\n",
        "                               head_mask=head_mask,\n",
        "                               inputs_embeds=inputs_embeds,\n",
        "                               encoder_hidden_states=encoder_hidden_states,\n",
        "                               encoder_attention_mask=encoder_attention_mask,\n",
        "                               output_attentions=output_attentions,\n",
        "                               output_hidden_states=output_hidden_states,\n",
        "                               return_dict=return_dict,)\n",
        "    last_hidden_state = outputs['last_hidden_state']\n",
        "    \n",
        "    if labels_embeddings is not None:\n",
        "      if not self.preembedded_hidden_size == self.config.hidden_size:\n",
        "        last_hidden_state = self.decoder(last_hidden_state)\n",
        "\n",
        "      sentence_prediction_output = self.cls(last_hidden_state, labels_embeddings, labels_mask)\n",
        "\n",
        "      return DocumentModelingOutput(per_example_loss_distance=sentence_prediction_output[4],\n",
        "                                    per_example_loss_product=sentence_prediction_output[5],\n",
        "                                    loss_distance=sentence_prediction_output[6],\n",
        "                                    loss_product=sentence_prediction_output[7],\n",
        "                                    loss_variance_distance=sentence_prediction_output[8],\n",
        "                                    loss_variance_product=sentence_prediction_output[9],\n",
        "                                    logits=sentence_prediction_output[0],\n",
        "                                    hidden_states=outputs.hidden_states,\n",
        "                                    last_hidden_state=last_hidden_state,\n",
        "                                    attentions=outputs.attentions,)\n",
        "    \n",
        "    else:\n",
        "      return DocumentModelingOutput(hidden_states=outputs.hidden_states,\n",
        "                                    last_hidden_state=last_hidden_state,\n",
        "                                    attentions=outputs.attentions,)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgYFQGv17gez"
      },
      "source": [
        "class HATEDocumentModelCrossEncoderRanking(TransformerPreTrainedModel):\n",
        "\n",
        "  def __init__(self, config, preembedded_hidden_size=128, ranking_loss_margin=0.25):\n",
        "    super().__init__(config)\n",
        "\n",
        "    self.preembedded_hidden_size = preembedded_hidden_size\n",
        "    self.config = config\n",
        "    self.ranking_loss_margin = ranking_loss_margin\n",
        "\n",
        "    if not preembedded_hidden_size == config.hidden_size:\n",
        "      self.encoder = EncoderMLP(config, preembedded_hidden_size)\n",
        "\n",
        "    self.transformer = TransformerBase(config, add_pooling_layer=False)\n",
        "    self.cls = nn.Linear(config.hidden_size, 1)\n",
        "\n",
        "  def forward(self,\n",
        "              input_ids=None,\n",
        "              attention_mask=None,\n",
        "              token_type_ids=None,\n",
        "              position_ids=None,\n",
        "              head_mask=None,\n",
        "              inputs_embeds=None,\n",
        "              encoder_hidden_states=None,\n",
        "              encoder_attention_mask=None,\n",
        "              output_attentions=None,\n",
        "              output_hidden_states=None,\n",
        "              compute_loss=False,\n",
        "              labels_initial=None,\n",
        "              query_ids=None, # for the \"hardmode\" finetuning where we want to distinguish more negative samples from a first stage retrieval but we have to know what query all of them belong to\n",
        "              return_dict=None,):\n",
        "    \n",
        "    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "    if not self.preembedded_hidden_size == self.config.hidden_size:\n",
        "      inputs_embeds = self.encoder(inputs_embeds)\n",
        "\n",
        "    outputs = self.transformer(input_ids,\n",
        "                               attention_mask=attention_mask,\n",
        "                               token_type_ids=token_type_ids,\n",
        "                               position_ids=position_ids,\n",
        "                               head_mask=head_mask,\n",
        "                               inputs_embeds=inputs_embeds,\n",
        "                               encoder_hidden_states=encoder_hidden_states,\n",
        "                               encoder_attention_mask=encoder_attention_mask,\n",
        "                               output_attentions=output_attentions,\n",
        "                               output_hidden_states=output_hidden_states,\n",
        "                               return_dict=return_dict,)\n",
        "    \n",
        "\n",
        "    cls_embeddings = []\n",
        "    for query in outputs['last_hidden_state']:\n",
        "      cls_embeddings.append(query[0])\n",
        "    cls_embeddings = torch.stack(cls_embeddings)\n",
        "\n",
        "    predictions = torch.sigmoid(self.cls(cls_embeddings)) # Instead of linear layer use pooling layer (similar to SMITH)\n",
        "\n",
        "    if self.training:\n",
        "      if query_ids is not None and labels is not None:\n",
        "        print(\"TODO\")\n",
        "      else:\n",
        "        loss_fct = nn.MarginRankingLoss(margin=self.ranking_loss_margin, reduction='sum')\n",
        "\n",
        "        split_size = int((predictions.size()[0])/2)\n",
        "\n",
        "        predictions_split = torch.split(predictions, split_size)\n",
        "\n",
        "        x = torch.cat((predictions_split[0], predictions_split[1]))\n",
        "        y = torch.cat((predictions_split[1], predictions_split[0]))\n",
        "\n",
        "        labels_pos = torch.ones_like(predictions_split[0])\n",
        "        labels_neg = torch.neg(labels_pos)\n",
        "        labels = torch.cat((labels_pos, labels_neg))\n",
        "\n",
        "        loss = loss_fct(x, y, labels)\n",
        "\n",
        "      return cls_embeddings, predictions, loss\n",
        "    \n",
        "    elif not self.training and labels_initial is not None:\n",
        "      predictions = torch.flatten(predictions)\n",
        "      sorted_predictions, sorted_indices = torch.sort(predictions, descending=True)\n",
        "      labels_initial = torch.flatten(labels_initial)\n",
        "      labels_reranked = labels_initial[sorted_indices]\n",
        "\n",
        "      return cls_embeddings, predictions, sorted_predictions, sorted_indices, labels_initial, labels_reranked"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7GH8SayGwaL"
      },
      "source": [
        "class HATEDocumentModelCosineSimilarityRanking(TransformerPreTrainedModel):\n",
        "  def __init__(self, config, preembedded_hidden_size=128, ranking_loss_margin=0.25):\n",
        "    super().__init__(config)\n",
        "\n",
        "    self.preembedded_hidden_size = preembedded_hidden_size\n",
        "    self.config = config\n",
        "    self.ranking_loss_margin = ranking_loss_margin\n",
        "\n",
        "    if not preembedded_hidden_size == config.hidden_size:\n",
        "      self.encoder = EncoderMLP(config, preembedded_hidden_size)\n",
        "    \n",
        "    self.transformer = TransformerBase(config, add_pooling_layer=False)\n",
        "\n",
        "  def forward(self,\n",
        "              embedding_mode='cls',\n",
        "              input_ids=None,\n",
        "              attention_mask=None,\n",
        "              token_type_ids=None,\n",
        "              position_ids=None,\n",
        "              head_mask=None,\n",
        "              query_embeds=None,\n",
        "              inputs_embeds=None,\n",
        "              encoder_hidden_states=None,\n",
        "              encoder_attention_mask=None,\n",
        "              output_attentions=None,\n",
        "              output_hidden_states=None,\n",
        "              compute_loss=False,\n",
        "              labels_initial=None,\n",
        "              query_ids=None, # for the \"hardmode\" finetuning where we want to distinguish more negative samples from a first stage retrieval but we have to know what query all of them belong to\n",
        "              return_dict=None,):\n",
        "    \n",
        "    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "    if not self.preembedded_hidden_size == self.config.hidden_size:\n",
        "      query_embeds = self.encoder(query_embeds)\n",
        "      inputs_embeds = self.encoder(inputs_embeds)\n",
        "\n",
        "    outputs = self.transformer(input_ids,\n",
        "                               attention_mask=attention_mask,\n",
        "                               token_type_ids=token_type_ids,\n",
        "                               position_ids=position_ids,\n",
        "                               head_mask=head_mask,\n",
        "                               inputs_embeds=inputs_embeds,\n",
        "                               encoder_hidden_states=encoder_hidden_states,\n",
        "                               encoder_attention_mask=encoder_attention_mask,\n",
        "                               output_attentions=output_attentions,\n",
        "                               output_hidden_states=output_hidden_states,\n",
        "                               return_dict=return_dict,)\n",
        "    \n",
        "\n",
        "    doc_embeddings = []\n",
        "    \n",
        "    for i, doc in enumerate(outputs['last_hidden_state']):\n",
        "      if embedding_mode == 'cls':\n",
        "        doc_embeddings.append(doc[0])\n",
        "      elif embedding_mode == 'mean':\n",
        "        doc_not_as_view = torch.clone(doc)\n",
        "        doc_not_as_view[0] = 0\n",
        "        doc_not_as_view[~attention_mask[i].bool()] = 0\n",
        "        doc_embed = torch.mean(doc_not_as_view, dim=0)\n",
        "        doc_embeddings.append(doc_embed)\n",
        "      elif embedding_mode == 'sum':\n",
        "        doc_not_as_view = torch.clone(doc)\n",
        "        doc_not_as_view[0] = 0\n",
        "        doc_not_as_view[~attention_mask[i].bool()] = 0\n",
        "        doc_embed = torch.sum(doc_not_as_view, dim=0)\n",
        "        doc_embeddings.append(doc_embed)\n",
        "\n",
        "    doc_embeddings = torch.stack(doc_embeddings)  # Add pooling pooling layer for better doc reps (similar to SMITH)\n",
        "\n",
        "    if self.training:\n",
        "      loss_fct = nn.CosineEmbeddingLoss(margin=self.ranking_loss_margin, reduction='sum') # #0.25 worked somewhat TODO try torch.nn.HingeEmbeddingLoss()\n",
        "      device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "      shuffled_indexes = torch.randperm(doc_embeddings.shape[0])\n",
        "      doc_shuffled = doc_embeddings[shuffled_indexes]\n",
        "\n",
        "      queries_stacked = torch.cat((query_embeds, query_embeds))\n",
        "      doc_stacked = torch.cat((doc_embeddings, doc_shuffled))\n",
        "\n",
        "      labels_pos = torch.ones(query_embeds.size()[0], device=device)\n",
        "      labels_neg = torch.neg(labels_pos)\n",
        "      labels_stacked = torch.cat((labels_pos, labels_neg))\n",
        "\n",
        "      predictions = f.cosine_similarity(queries_stacked, doc_stacked)\n",
        "\n",
        "      loss = loss_fct(queries_stacked, doc_stacked, labels_stacked)\n",
        "\n",
        "      return doc_embeddings, predictions, loss\n",
        "    \n",
        "    elif not self.training and labels_initial is not None:\n",
        "      query_embeds = torch.squeeze(query_embeds, dim=1)\n",
        "      predictions = f.cosine_similarity(query_embeds, doc_embeddings)\n",
        "\n",
        "      sorted_predictions, sorted_indices = torch.sort(predictions, descending=True)\n",
        "      labels_reranked = labels_initial[0][sorted_indices]\n",
        "\n",
        "      return doc_embeddings, predictions, sorted_predictions, sorted_indices, labels_initial, labels_reranked"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HprI1YP3DaO"
      },
      "source": [
        "class NaiveCosineSimilarityRanking(TransformerPreTrainedModel):\n",
        "  def __init__(self, config, pretrained_transformer_weights=None, finetuned_model_weights=None):\n",
        "    super().__init__(config)\n",
        "\n",
        "  def forward(self,\n",
        "              doc_embedding,\n",
        "              attention_mask=None,\n",
        "              query_embeds=None,\n",
        "              inputs_embeds=None,\n",
        "              labels_initial=None):\n",
        "    \n",
        "    # doc_embedding can be either 'mean' or 'sum'\n",
        "\n",
        "    doc_embeds = []\n",
        "    for i, doc in enumerate(inputs_embeds):\n",
        "      doc[0] = 0\n",
        "      doc[~attention_mask[i].bool()] = 0\n",
        "      if doc_embedding == 'mean':\n",
        "        doc_embed = torch.mean(doc, dim=0)\n",
        "        doc_embeds.append(doc_embed)\n",
        "      elif doc_embedding == 'sum':\n",
        "        doc_embed = torch.sum(doc, dim=0)\n",
        "        doc_embeds.append(doc_embed)\n",
        "\n",
        "    query_embeds = torch.squeeze(query_embeds, dim=1)\n",
        "    doc_embeds = torch.stack(doc_embeds)\n",
        "    predictions = f.cosine_similarity(query_embeds, doc_embeds)\n",
        "\n",
        "    sorted_predictions, sorted_indices = torch.sort(predictions, descending=True)\n",
        "    labels_reranked = labels_initial[0][sorted_indices]\n",
        "\n",
        "    return predictions, sorted_predictions, sorted_indices, labels_initial, labels_reranked"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IVJdy-_qAPt"
      },
      "source": [
        "## 4.5 Hierachical Attention-Based Document Encoder (HATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3evzCMPzdeL"
      },
      "source": [
        "#### HATE Model Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0WDofhneV06"
      },
      "source": [
        "class HATEOutput(ModelOutput):\n",
        "  \"\"\"\n",
        "  Class for the whole model output\n",
        "  \"\"\"\n",
        "  sentence_model_hidden_states = None\n",
        "  document_model_hidden_states = None\n",
        "\n",
        "  sentence_model_last_hidden_states = None\n",
        "  document_model_last_hidden_states = None\n",
        "\n",
        "  sentence_model_attentions = None\n",
        "  document_model_attentions = None\n",
        "\n",
        "  sentence_model_logits = None\n",
        "  document_model_logits = None\n",
        "\n",
        "  sentence_model_loss = None\n",
        "  document_model_loss = None\n",
        "  document_model_per_example_loss = None\n",
        "  total_loss = None\n",
        "\n",
        "  loss_variance_distance = None\n",
        "  loss_variance_product = None\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSXwRXFRziOi"
      },
      "source": [
        "#### HATE Model Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VglcydbCqvL_"
      },
      "source": [
        "class HATEConfig ():\n",
        "  def __init__(self,\n",
        "               sentence_model_config,\n",
        "               document_model_config,\n",
        "               hidden_size=128,\n",
        "               is_pretraining=False,\n",
        "               pretrain_sentence_model=False,\n",
        "               sentence_model_checkpoint=None,\n",
        "               pretrain_document_model=True,\n",
        "               document_model_checkpoint=None,\n",
        "               use_product_loss=True):\n",
        "    \n",
        "    \"\"\"\n",
        "    Constructs ModelConfig.\n",
        "    Args:\n",
        "      Stuff\n",
        "    Returns:\n",
        "      Stuff\n",
        "    \"\"\"\n",
        "    self.sentence_model_config = sentence_model_config\n",
        "    self.document_model_config = document_model_config\n",
        "    self.hidden_size = hidden_size\n",
        "    self.is_pretraining = is_pretraining\n",
        "    self.pretrain_sentence_model = pretrain_sentence_model\n",
        "    self.sentence_model_checkpoint = sentence_model_checkpoint\n",
        "    self.pretrain_document_model = pretrain_document_model\n",
        "    self.document_model_checkpoint = document_model_checkpoint\n",
        "    self.use_product_loss = use_product_loss\n",
        "    \n",
        "  # @classmethod\n",
        "  # def from_dict(cls, json_object):\n",
        "  #   \"\"\"Constructs a `BertConfig` from a Python dictionary of parameters.\"\"\"\n",
        "  #   config = BertConfig(vocab_size=None)\n",
        "  #   for (key, value) in six.iteritems(json_object):\n",
        "  #     config.__dict__[key] = value\n",
        "  #   return config\n",
        "\n",
        "  # @classmethod\n",
        "  # def from_json_file(cls, json_file):\n",
        "  #   \"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"\n",
        "  #   with tf.gfile.GFile(json_file, \"r\") as reader:\n",
        "  #     text = reader.read()\n",
        "  #   return cls.from_dict(json.loads(text))\n",
        "\n",
        "  # def to_dict(self):\n",
        "  #   \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
        "  #   output = copy.deepcopy(self.__dict__)\n",
        "  #   return output\n",
        "\n",
        "  # def to_json_string(self):\n",
        "  #   \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
        "  #   return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\""
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Pc3_xs3zmp-"
      },
      "source": [
        "#### HATE Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxULKxFYz7uz"
      },
      "source": [
        "class HATEModel (torch.nn.Module):\n",
        "  def __init__(self, hate_config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.hate_config = hate_config\n",
        "\n",
        "    \n",
        "    self.sentence_model = HATESentenceModel(hate_config.sentence_model_config, \n",
        "                                            hate_config.pretrain_sentence_model)\n",
        "    self.document_model = HATEDocumentModel(hate_config.document_model_config, \n",
        "                                            hate_config.pretrain_document_model)\n",
        "    \n",
        "    \n",
        "    # Intermediate layers could be trained if hiden_size changes between models\n",
        "    # self.intermediate_dense = nn.Linear(in_features=hate_config.sentence_model_config.hidden_size,\n",
        "    #                                     out_features=hate_config.document_model_config.hidden_size)\n",
        "    # self.intermediary_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "    # self.intermediary_dropout =  nn.Dropout()\n",
        "\n",
        "\n",
        "  def forward(self, max_doc_length, doc_length_before_padding, batch, device): \n",
        "    \"\"\"\n",
        "    Args:\n",
        "      batch: List with 3 entries from our custom collate function\n",
        "        batch[0]: The length of the largest document in the batch, the other\n",
        "          documents in the batch will be padded to this length with 0-sentences \n",
        "        batch[1]: A list of the original unpadded size of every document in the\n",
        "          batch, will be used when the intermediary sentence representations are\n",
        "          padded with the appropriate attention mask and special token mask for\n",
        "          the document model.\n",
        "        batch[2]: Dictionary that contains the padded tensors.\n",
        "          ['input_ids'] (batch_size, max_doc_length, padding_strategy)\n",
        "          ['special_tokens_mask'] (batch_size, max_doc_length, padding_strategy)\n",
        "          ['attention_mask'] (batch_size, max_doc_length, padding_strategy)\n",
        "          ['token_type_ids'] (batch_size, max_doc_length, padding_strategy)\n",
        "    Returns:\n",
        "      A HATEModelOutput Object, entries are filled depending on whether model is\n",
        "      in pretraining mode or not. During pretraining every entry is filled. If\n",
        "      model is not in pretraining it won't return losses and logits.\n",
        "    \"\"\"\n",
        "\n",
        "    if self.hate_config.sentence_model_config.hidden_size != self.hate_config.sentence_model_config.hidden_size:\n",
        "      raise Exception(f\"Hidden sizes of Sentence and Document Model must be compatibe, got {self.hate_config.sentence_model_config.hidden_size} and {self.hate_config.sentence_model_config.hidden_size}\")\n",
        "\n",
        "    sentence_level_loss = 0\n",
        "    document_level_loss = 0\n",
        "\n",
        "    # Initiates lists that aggregate input for the document model\n",
        "    intermediary_embeddings = []\n",
        "    intermediary_attention_mask = []\n",
        "    intermediary_special_tokens_mask = []\n",
        "\n",
        "    # Initiates lists later used in HATEModelOutput\n",
        "    sentence_model_hidden_states = []\n",
        "    sentence_model_last_hidden_states = []\n",
        "    sentence_model_attentions = []\n",
        "    sentence_model_logits = []\n",
        "\n",
        "    # Iterate over documents and apply the Sentence Model per doc\n",
        "    for doc_counter, doc in enumerate(batch):\n",
        "      \n",
        "      # Initiate embedding tensor, attention mask and special tokens mask for one\n",
        "      # document. The individual sentence embeddings and their according attention\n",
        "      # mask label as well as special token label will be appended to this. We \n",
        "      # initiate them with a random tensor at position 0 to denote the CLS token.\n",
        "      sentence_embeddings_per_doc = [torch.randn(self.hate_config.hidden_size, device=device)]\n",
        "      attention_mask_per_doc = [1]\n",
        "      special_tokens_mask_per_doc = [1]\n",
        "\n",
        "      # Compute sentence loss only if (sentence) model is training\n",
        "      if self.hate_config.is_pretraining and self.hate_config.pretrain_sentence_model:\n",
        "        # Mask input IDs for one document\n",
        "        masking_output = mask_input_ids(doc['input_ids'], doc['special_tokens_mask'])\n",
        "        # Feed all the inputs to the Sentence Model for one document\n",
        "        sentence_model_output = self.sentence_model(input_ids=masking_output[0], \n",
        "                                                    attention_mask=doc['attention_mask'], \n",
        "                                                    token_type_ids=doc['token_type_ids'],\n",
        "                                                    labels=masking_output[1],\n",
        "                                                    output_attentions=True,\n",
        "                                                    output_hidden_states=True)\n",
        "        sentence_level_loss += sentence_model_output['loss']\n",
        "        \n",
        "      # If either of the pretrain settings is set to false we don't compute loss\n",
        "      elif self.hate_config.is_pretraining == False or self.hate_config.pretrain_sentence_model == False:\n",
        "        sentence_model_output = self.sentence_model(input_ids=doc['input_ids'], \n",
        "                                                    attention_mask=doc['attention_mask'], \n",
        "                                                    token_type_ids=doc['token_type_ids'],)\n",
        "      \n",
        "      # TODO fix this so we have a proper output\n",
        "      # Aggregate releveant sentence model outputs for HATEModelOutput\n",
        "      sentence_model_hidden_states.append(sentence_model_output['hidden_states'])\n",
        "      sentence_model_last_hidden_states.append(sentence_model_output['last_hidden_state'])\n",
        "      sentence_model_attentions.append(sentence_model_output['attentions'])\n",
        "      sentence_model_logits.append(sentence_model_output['logits'])\n",
        "\n",
        "      # Iterate over sequence embeddings returned by the model for one document\n",
        "      # sentence_model_output['last_hidden_state'] has size \n",
        "      # (doc_length, max_position_embeddings, hidden_size)\n",
        "      for sentence_counter, sentence in enumerate(sentence_model_output['last_hidden_state']):\n",
        "        # CLS embedding for a sentence at sentence_counter position in the document\n",
        "        # CLS is at position 0 out of max_position_embeddings (512 usually)\n",
        "        # Pad the embeddings and additional necessary inputs to the maximum document length in the batch\n",
        "        sentence_embeddings_per_doc.append(sentence[0])\n",
        "        # If the sentence representations are still from 'real' sentences then we\n",
        "        # add positive attention mask and negative special token mask\n",
        "        if sentence_counter < doc_length_before_padding[doc_counter]:\n",
        "          attention_mask_per_doc.append(1)\n",
        "          special_tokens_mask_per_doc.append(0)\n",
        "        # If the sentence representation is just a padding added by our custom\n",
        "        # collate function then we add the masks the other way around.\n",
        "        else:\n",
        "          attention_mask_per_doc.append(0)\n",
        "          special_tokens_mask_per_doc.append(1)\n",
        "\n",
        "\n",
        "      # TODO insert linear layer+layernorm+dropout in case sentence_model.hidden_size != document_model.hidden_size in order to transform the embeddings\n",
        "\n",
        "\n",
        "      # Appends the padded tensors to the list of document-wise sentence embeddings\n",
        "      intermediary_embeddings.append(torch.stack(sentence_embeddings_per_doc))\n",
        "      if torch.cuda.is_available():\n",
        "        intermediary_attention_mask.append(torch.cuda.FloatTensor(attention_mask_per_doc))\n",
        "        intermediary_special_tokens_mask.append(torch.cuda.FloatTensor(special_tokens_mask_per_doc))\n",
        "      else:\n",
        "        intermediary_attention_mask.append(torch.FloatTensor(attention_mask_per_doc))\n",
        "        intermediary_special_tokens_mask.append(torch.FloatTensor(special_tokens_mask_per_doc))\n",
        "\n",
        "    # Stacks the list into a Torch Tensor of fixed size\n",
        "    # Embedding tensor has size (batch_size, max_doc_length+1, hidden_size)\n",
        "    intermediary_embeddings_tensor = torch.stack(intermediary_embeddings)\n",
        "    intermediary_attention_mask_tensor = torch.stack(intermediary_attention_mask)\n",
        "    intermediary_special_tokens_mask_tensor = torch.stack(intermediary_special_tokens_mask)\n",
        "\n",
        "    # TODO add dense layer and normalization on intermediary_embeddings_tensor?\n",
        "\n",
        "    # Don't perform masking if pretraining=False:\n",
        "    if self.hate_config.is_pretraining:\n",
        "      masked_input_embeddings = mask_input_embeddings(intermediary_embeddings_tensor, \n",
        "                                                      intermediary_special_tokens_mask_tensor,\n",
        "                                                      device)\n",
        "\n",
        "      document_model_output = self.document_model(attention_mask=intermediary_attention_mask_tensor,\n",
        "                                                  inputs_embeds=masked_input_embeddings[1],\n",
        "                                                  labels_embeddings=masked_input_embeddings[2],\n",
        "                                                  labels_mask=masked_input_embeddings[3],)\n",
        "      \n",
        "\n",
        "      # Allow switching between loss functions in document model\n",
        "      if self.hate_config.use_product_loss:\n",
        "        document_level_loss += document_model_output['loss_product']\n",
        "        document_model_per_example_loss = document_model_output['per_example_loss_product']\n",
        "      else:\n",
        "        document_level_loss += document_model_output['loss_distance']\n",
        "        document_model_per_example_loss = document_model_output['per_example_loss_distance']\n",
        "\n",
        "      total_loss = sentence_level_loss + document_level_loss\n",
        "      if total_loss.item() == 0.0:\n",
        "        # TODO Seems to appear when only a singe document in the batch is masked\n",
        "        print(\"Weird loss\\nSentence loss: \", sentence_level_loss, \"\\nDocument loss: \", document_level_loss)\n",
        "        print(\"Intermediary embeddings: \", intermediary_embeddings_tensor)\n",
        "        print(\"Embeddings masking: \", masked_input_embeddings)\n",
        "\n",
        "      return HATEOutput(sentence_model_hidden_states=sentence_model_hidden_states,\n",
        "                        document_model_hidden_states=document_model_output['hidden_states'],\n",
        "                        sentence_model_last_hidden_states=sentence_model_last_hidden_states,\n",
        "                        document_model_last_hidden_states=document_model_output['last_hidden_state'],\n",
        "                        sentence_model_attentions=sentence_model_attentions,\n",
        "                        document_model_attentions=document_model_output['attentions'],\n",
        "                        sentence_model_logits=sentence_model_logits,\n",
        "                        document_model_logits=document_model_output['logits'],\n",
        "                        sentence_model_loss=sentence_level_loss,\n",
        "                        document_model_loss=document_level_loss,\n",
        "                        document_model_per_example_loss=document_model_per_example_loss,\n",
        "                        total_loss=total_loss,\n",
        "                        loss_variance_distance=document_model_output['loss_variance_distance'],\n",
        "                        loss_variance_product=document_model_output['loss_variance_product'])\n",
        "    else:\n",
        "      document_model_output = document_model(attention_mask=intermediary_attention_mask_tensor,\n",
        "                                             inputs_embeds=intermediary_embeddings_tensor)\n",
        "      \n",
        "      return HATEOutput(sentence_model_hidden_states=sentence_model_hidden_states,\n",
        "                        document_model_hidden_states=document_model_output['hidden_states'],\n",
        "                        sentence_model_last_hidden_states=sentence_model_last_hidden_states,\n",
        "                        document_model_last_hidden_states=document_model_output['last_hidden_state'],\n",
        "                        sentence_model_attentions=sentence_model_attentions,\n",
        "                        document_model_attentions=document_model_output['attentions'])\n",
        "\n",
        "    # TODO prune layers? transformers.modeling_utils.find_pruneable_heads_and_indices"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atWVsSmsdZLG"
      },
      "source": [
        "class HATEModelForDocumentRanking(torch.nn.Module):\n",
        "  def __init__():\n",
        "    print(\"TODO\")\n",
        "    \n",
        "  def forward(first_stage_retrievals=None, labels=None, is_training=False, use_cosine_simlarity=False, use_cross_encoder=False): \n",
        "    # TODO put is_training in config\n",
        "    # TODO as a matter of fact put as much as possible in the config, only leave data handover to the forward() arguments\n",
        "    \n",
        "    # Regular sentence model\n",
        "\n",
        "    # TODO both of the following should work for training and inference\n",
        "    if labels is None:\n",
        "      # Assume all samples are positives\n",
        "      # Feed into document model similar to prepare_for_msmarco_cross_encoder/_cosine_similarity\n",
        "      if use_cosine_similarity: # We know from __init__() which one we use, infer the mode from there instead of passing more arguments to forward\n",
        "        print(\"TODO\")\n",
        "        # Prepare data accordingly\n",
        "        # Call cosine sim model\n",
        "      elif use_cross_encoder:\n",
        "        print(\"TODO\")\n",
        "\n",
        "        # Call cross encoder model\n",
        "    else:\n",
        "      # Feed into document model with appropriate query types and labels\n",
        "      if use_cosine_similarity:\n",
        "        print(\"TODO\")\n",
        "        # Call cosine sim model\n",
        "      elif use_cross_encoder:\n",
        "        print(\"TODO\")\n",
        "        # Call cross encoder model"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5s5ahK_VSSYv"
      },
      "source": [
        "## 4.6 Weight Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESbpWkmFSNMf"
      },
      "source": [
        "def load_weights(model, weights_path, device):\n",
        "  pretrained_dict = torch.load(weights_path, map_location=device)['model_state_dict']\n",
        "  model_dict = model.state_dict()\n",
        "\n",
        "  # 1. filter out unnecessary keys\n",
        "  pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
        "  # 2. overwrite entries in the existing state dict\n",
        "  model_dict.update(pretrained_dict) \n",
        "  # 3. load the new state dict\n",
        "  model.load_state_dict(pretrained_dict, strict=False)\n",
        "\n",
        "  return model"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YleJunzjQSh-"
      },
      "source": [
        "# **5.** Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDtLRm7g6XZj",
        "outputId": "74944d34-a370-4f8c-d360-4d93771433af"
      },
      "source": [
        "CLS = torch.Tensor([-0.9981,  1.1354, -3.0456, -1.6170, -0.8531,  0.1014,  0.2313,  0.6844,\n",
        "                    -0.8162,  1.8423,  0.6286, -0.2557, -3.2614,  0.4459,  1.6963, -1.2250,\n",
        "                    -0.4966, -0.2596, -2.1223,  0.8322, -0.1752, -0.4665,  3.0415,  0.8801,\n",
        "                     0.3117,  0.1467,  0.4205,  0.5684,  0.0836, -0.0689, -1.4866, -1.3490,\n",
        "                    -1.1071,  0.3617,  1.7387, -1.0797,  0.3693, -0.8626, -1.4490, -1.6486,\n",
        "                     0.9362, -1.3971,  0.8101, -1.2994, -0.2535, -1.9793, -0.8475, -1.2452,\n",
        "                     0.1394, -0.0303, -0.7455,  1.4981, -0.0614, -0.3954,  1.4995, -1.3618,\n",
        "                     0.2045,  0.9455, -1.0810,  1.8007,  1.3504, -0.3771, -1.0584,  0.0370,\n",
        "                    -0.5903,  0.4348,  0.4075, -0.0932,  1.4073, -0.6776, -0.2490,  0.5176,\n",
        "                    -0.9007,  0.4315,  0.3015,  0.0353,  0.5067,  0.7153,  2.1073,  0.3022,\n",
        "                    -0.4577,  0.5652, -0.2365,  0.1794, -0.4954, -0.5183, -0.2060, -0.7410,\n",
        "                     2.8978, -2.0026,  1.5776,  0.1521, -1.3113,  1.8437,  1.0323,  0.9030,\n",
        "                     0.4231, -0.5792, -0.1886,  0.4181, -1.1298,  0.1378,  0.1590, -0.7804,\n",
        "                     1.0177, -1.4610, -0.3874,  0.7050,  0.8824, -1.5715,  0.6487,  0.6148,\n",
        "                    -0.1988, -0.2373,  0.4794,  1.6276, -0.3890, -0.9612,  1.2258,  0.3845,\n",
        "                    -0.3818,  1.0645,  1.4688, -0.8300, -1.6368, -0.9549, -0.9308, -1.2218])\n",
        "\n",
        "SEP = torch.Tensor([-2.1891e+00,  1.2711e+00, -8.9792e-01, -1.5016e+00, -5.9451e-01,\n",
        "                     4.3516e-01,  7.7865e-01,  1.9005e-01, -2.2061e-01,  1.1726e+00,\n",
        "                     3.0554e-01, -7.1097e-01, -3.7538e+00,  6.0648e-02,  2.0107e+00,\n",
        "                    -1.3236e+00, -8.7579e-01, -2.9496e-01, -1.8437e+00,  5.3592e-01,\n",
        "                    -4.6524e-01, -4.3795e-01,  2.1429e+00,  1.1876e+00,  4.3196e-01,\n",
        "                     2.0292e-01, -1.4503e-01,  6.9892e-01,  5.6384e-01,  4.6437e-01,\n",
        "                    -1.9509e+00, -3.2635e-02,  3.2464e-01,  6.8599e-01,  1.3163e+00,\n",
        "                    -1.7324e+00,  2.6175e-01, -7.4656e-01, -1.8667e+00, -1.5655e+00,\n",
        "                     1.2984e+00, -7.0660e-01,  1.9371e-01, -9.8461e-01, -2.6006e-02,\n",
        "                    -2.1590e+00, -8.7077e-01, -1.0568e+00, -1.8535e-02, -4.5890e-01,\n",
        "                    -2.0912e-01,  1.7237e+00,  1.4897e-01, -3.0492e-02,  1.0122e+00,\n",
        "                    -5.5029e-01,  1.6240e+00,  4.2045e-01, -1.2400e+00,  2.0293e+00,\n",
        "                     4.3488e-01, -7.9258e-01, -1.3834e+00,  6.4267e-01, -6.0309e-01,\n",
        "                    -7.2195e-04,  3.7046e-01,  5.0487e-01,  1.1453e+00,  8.6877e-02,\n",
        "                    -1.6964e-02,  1.0068e+00, -1.0256e+00, -9.3181e-02,  3.8434e-01,\n",
        "                     3.1522e-01, -1.7939e-01,  8.0017e-02,  9.9643e-01, -1.9349e-01,\n",
        "                    -5.2971e-01, -8.9503e-01, -3.0495e-01,  2.0245e-01,  7.9598e-02,\n",
        "                    -1.1440e+00, -4.3081e-02, -4.9467e-01,  2.0151e+00, -1.7306e+00,\n",
        "                     1.0992e+00,  8.5625e-01, -1.9254e+00,  1.8504e+00,  1.3958e+00,\n",
        "                     1.4260e+00,  3.1682e-01, -8.9479e-01,  6.8218e-01,  7.8508e-01,\n",
        "                    -1.2306e+00,  4.1371e-01, -1.1960e-01, -1.0888e+00,  1.0746e+00,\n",
        "                    -1.3507e+00, -2.8201e-01,  8.3991e-01,  3.8711e-01, -1.9244e+00,\n",
        "                     1.9884e-01,  1.1749e+00,  5.3496e-01,  5.5470e-01,  1.0846e+00,\n",
        "                     1.9696e+00, -8.7533e-01, -1.4858e+00,  6.4223e-01,  1.2655e+00,\n",
        "                    -1.9945e-01,  8.3856e-01,  5.2890e-01, -1.1276e+00, -1.7738e+00,\n",
        "                    -1.9558e+00, -1.1231e+00, -2.5221e-01])\n",
        "\n",
        "print(\"CLS: \", CLS.size(), \" SEP: \", SEP.size())"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CLS:  torch.Size([128])  SEP:  torch.Size([128])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DriQikaMnxv"
      },
      "source": [
        "## 5.1 Pretraining Data: Wikipedia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boIilmUBvAWw"
      },
      "source": [
        "**Evidence for the need for Pretraining**\n",
        "- HIBERT https://arxiv.org/pdf/1905.06566.pdf\n",
        "- Language Model Pre-training for Hierarchical Document Representations https://arxiv.org/pdf/1901.09128.pdf\n",
        "- Pre-training Tasks for Embedding-based Large-scale Retrieval https://arxiv.org/pdf/2002.03932.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kLltdEAWvd7"
      },
      "source": [
        "Describe Dataset used for pretraining, where to get it and how to load it\n",
        "https://dumps.wikimedia.org/enwiki/20200920/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwlLYYpKHAj2"
      },
      "source": [
        "#wikipedia = load_dataset('wikipedia', '20200501.en', split='train')"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7N_khVplx60e"
      },
      "source": [
        "# print(wikipedia.dataset_size)\n",
        "# print(\"size: \", wikipedia.dataset_size)\n",
        "# print(\"column names: \", wikipedia.column_names)\n",
        "# print(\"shape: \", wikipedia.shape)\n",
        "# print(\"format: \", wikipedia.format)\n",
        "# print(\"description: \", wikipedia.description)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTJuia_PGDMx"
      },
      "source": [
        "#wikipedia_small = wikipedia[:16384]"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCXBiH9uiNBO"
      },
      "source": [
        "Some theory on preprocessing: https://mlexplained.com/2019/11/06/a-deep-dive-into-the-wonderful-world-of-preprocessing-in-nlp/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBnJKkLtc7YE"
      },
      "source": [
        "### 5.1.1 Dataset Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-EnIHn7qwtb"
      },
      "source": [
        "The class is used as a relatively generic Dataset class for raw text input. If we use the full HATE model and/or want to train it from scratch this is the appropriate Dataset class to use. It requires the data to be stored in a list of documents similar to the Wikipedia dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mwz_T5I5v_zw"
      },
      "source": [
        "class PretrainingData(Dataset):\n",
        "  \n",
        "  def __init__(self, dataset):\n",
        "    self.tokenizer = transformers.BertTokenizerFast.from_pretrained('google/bert_uncased_L-2_H-128_A-2')\n",
        "    self.wiki_dump = dataset # dataset #load_dataset('wikipedia', '20200501.en', split='train')\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.wiki_dump['text'])\n",
        "\n",
        "  # TODO which of batch_encode_plus/encode_plus/prepare_for_model is better?\n",
        "  def __getitem__(self, id):\n",
        "    # Split document at dataset[id] into sentences\n",
        "    doc_split = sent_tokenize(self.wiki_dump['text'][id])\n",
        "    # Batch-encodes a whole document at dataset[id] at once\n",
        "    doc_tokenized =  self.tokenizer.batch_encode_plus(doc_split,\n",
        "                                                      padding='longest',\n",
        "                                                      truncation='longest_first',\n",
        "                                                      return_tensors='pt',\n",
        "                                                      return_special_tokens_mask=True)\n",
        "    return doc_tokenized"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cywrPEdxDAfa"
      },
      "source": [
        "A smaller Dataset class that loads pre-encoded TensorDataset objects in a pickle from disk. A TensorDataset contains the pre-encoded sentence embeddings for one document and their attention mask als well as special tokens mask. A pickle file contains 16384 such TensorDatasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocxgMlB_9mT3"
      },
      "source": [
        "class PretrainingDataPreEncoded(Dataset):\n",
        "  def __init__(self, filepath):\n",
        "    with open(filepath, 'rb') as p:\n",
        "      self.dataset = pickle.load(p)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.dataset[idx]"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Tj1ZTCxp-Tq"
      },
      "source": [
        "### 5.1.2 Pre-encode Batches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prLMkJwYqJj9"
      },
      "source": [
        "In order to speed up training time we are encoding large chunks of our Pretraining data with a loaded BERT Module. Since we are fixing the weights of the sentence model anyway this would free up GPU RAM significantly as we will only load the document model during training and feed the uncontextualized sentence vectors to it from storage.\n",
        "\n",
        "This serves as a helper routine in order to cope with our hardware limitations. Given the appropriate compute power we would train our full model via our adaptive pretrain_full() function that scales to distributed computations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQVcd5QWqFkz"
      },
      "source": [
        "def pre_encode_wikipedia(model,\n",
        "                         save_path,\n",
        "                         pretrained_sentence_model='google/bert_uncased_L-2_H-128_A-2'):\n",
        "  \n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  tokenizer = transformers.BertTokenizerFast.from_pretrained(pretrained_sentence_model)\n",
        "  document_data_list = []\n",
        "\n",
        "  for iteration, document in enumerate(wikipedia_small['text']):\n",
        "    sentence_embeds_per_doc = [torch.randn(128, device=device)]\n",
        "    attention_mask_per_doc = [1]\n",
        "    special_tokens_per_doc = [1]\n",
        "    doc_split = sent_tokenize(document)\n",
        "    doc_tokenized = tokenizer.batch_encode_plus(doc_split,\n",
        "                                                padding='longest',\n",
        "                                                truncation=True,\n",
        "                                                max_length=512,\n",
        "                                                return_tensors='pt')\n",
        "    for key, value in doc_tokenized.items():\n",
        "      doc_tokenized[key] = doc_tokenized[key].to(device)\n",
        "    doc_encoded = model(**doc_tokenized)\n",
        "    for sentence in doc_encoded['last_hidden_state']:\n",
        "      sentence_embeds_per_doc.append(sentence[0])\n",
        "      attention_mask_per_doc.append(1)\n",
        "      special_tokens_per_doc.append(0)\n",
        "\n",
        "    sentence_embeds = torch.stack(sentence_embeds_per_doc)\n",
        "    if torch.cuda.is_available():\n",
        "      attention_mask = torch.cuda.FloatTensor(attention_mask_per_doc)\n",
        "      special_tokens_mask = torch.cuda.FloatTensor(special_tokens_per_doc)\n",
        "    else:\n",
        "      attention_mask = torch.FloatTensor(attention_mask_per_doc)\n",
        "      special_tokens_mask = torch.FloatTensor(special_tokens_per_doc)\n",
        "    \n",
        "    sentence_embeds.to('cpu')\n",
        "    attention_mask.to('cpu')\n",
        "    special_tokens_mask.to('cpu')\n",
        "    torch.cuda.empty_cache()\n",
        "    document_data = torch.utils.data.TensorDataset(*[sentence_embeds, attention_mask, special_tokens_mask])\n",
        "    document_data_list.append(document_data)\n",
        "    print(f\"Document at {iteration} encoded and appended.\")\n",
        "  \n",
        "  with open(f'{save_path}{time.strftime(\"%Y%m%d-%H%M%S\")}_16384.pkl', 'wb') as p:\n",
        "    pickle.dump(document_data_list, p) \n",
        "    \n",
        "  print(f\"Batch saved in pickle file.\")"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnPhMcLt4Gw_"
      },
      "source": [
        "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# bert_tiny = BertModel.from_pretrained('google/bert_uncased_L-2_H-128_A-2')\n",
        "# bert_tiny.to(device)\n",
        "# pytorch_total_params = sum(p.numel() for p in bert_tiny.parameters())\n",
        "# print(pytorch_total_params)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlKqwEHhuPQh"
      },
      "source": [
        "# pre_encode_wikipedia(model=bert_tiny, save_path='/content/drive/MyDrive/Bachelor/Thesis/datasets/wikipedia_preembedded_128/')"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IO_CxT4c1Kb"
      },
      "source": [
        "### 5.1.3 Collate Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Fi5CWVIrL4U"
      },
      "source": [
        "The custom collate function receives a batch of documents that were encoded by the tokenizer. Each document was pre-split into sentences and then batch-encoded by a pretrained tokenizer. The encoded documents contain the input IDs, the attention mask, the special tokens mask and the token type IDs. Since the batch has to have a fixed size overall to be fed into the model we have to pad shorter documents sentence wise. [Insert graphics]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLhuaZogTHn_"
      },
      "source": [
        "def pad_input_tokens(batch):\n",
        "  \"\"\"\n",
        "  Receives a batch of documents and pads them to the longest document in the \n",
        "  batch so that each batch has documents with the same length but different\n",
        "  batches can have documents with different length.\n",
        "  NOTE: This does not affect the padding of sentences *within* documents! The\n",
        "        individual sentences of a document get padded to the length of the\n",
        "        document's longest sentence by the tokenizer.\n",
        "  Args:\n",
        "    batch: A list of length [batch_size] specified within the DataLoader that\n",
        "      contains documents and their model-specific information as a Dictionary\n",
        "      which itself contains torch.Tensors of size (doc_length, longest_seq_in_doc)\n",
        "      where longest_seq_in_doc is computed by the Tokenizer during retrieval \n",
        "      from the Dataset.\n",
        "  Returns:\n",
        "    (longest_document, longest_sentence, batch): \n",
        "      A triple where the first entry is the longest doc in the batch before padding. \n",
        "      It's passed to the model as a utility because the model uses that size for \n",
        "      intermediary padding.\n",
        "      The second entry is the length of longest sentence as a list with length\n",
        "      [batch_size] that holds the length of the longest sentence of each document.\n",
        "      That number was used by the Tokenizer to pad sentences document-wise.\n",
        "      The third entry is the appropriately padded batch ready to be passed to\n",
        "      the model.\n",
        "  \"\"\"\n",
        "  longest_document = max(len(doc['input_ids']) for doc in batch)\n",
        "  doc_length_before_padding = []\n",
        "  \n",
        "  for doc in batch:\n",
        "    doc_length_before_padding.append(len(doc['input_ids']))\n",
        "    if len(doc['input_ids']) < longest_document:\n",
        "      doc['input_ids'] = torch.Tensor.long(torch.cat((doc['input_ids'], torch.zeros(longest_document - len(doc['input_ids']), doc['input_ids'].size()[1])), dim=0))\n",
        "      doc['token_type_ids'] = torch.Tensor.long(torch.cat((doc['token_type_ids'], torch.zeros(longest_document - len(doc['token_type_ids']), doc['token_type_ids'].size()[1])), dim=0))\n",
        "      doc['attention_mask'] = torch.Tensor.long(torch.cat((doc['attention_mask'], torch.zeros(longest_document - len(doc['attention_mask']), doc['attention_mask'].size()[1])), dim=0))\n",
        "      doc['special_tokens_mask'] = torch.Tensor.long(torch.cat((doc['special_tokens_mask'], torch.zeros(longest_document - len(doc['special_tokens_mask']), doc['special_tokens_mask'].size()[1])), dim=0))\n",
        "\n",
        "  return (longest_document, doc_length_before_padding, batch)\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAtaQlSHDuBW"
      },
      "source": [
        "A different collate function is needed for the fast version of our pretraining loop. Since the DataLoader is fetching TensorDataset objects of pre-encoded documents from disk we need to \"unpack\" these TensorDatasets and also pad them accordingly to the largest document length in the batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1HXEMMrEHof"
      },
      "source": [
        "def pad_input_embeds(batch):\n",
        "\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  CLS_128 = torch.unsqueeze(CLS, dim=0)\n",
        "\n",
        "  longest_document = max(len(doc.tensors[0]) for doc in batch)\n",
        "  if longest_document > 512:\n",
        "    longest_document = 512\n",
        "\n",
        "  input_embeds_list = []\n",
        "  attention_mask_list = []\n",
        "  special_tokens_mask_list = []\n",
        "\n",
        "  for doc in batch:\n",
        "    if len(doc.tensors[0]) < longest_document:\n",
        "      embedding = doc.tensors[0].to(device)\n",
        "      padded_embeds = (torch.cuda.FloatTensor(torch.cat((CLS_128.to(device), embedding, torch.randn(longest_document - len(embedding), embedding.size()[1], device=device)), dim=0)))\n",
        "      input_embeds_list.append(padded_embeds)\n",
        "\n",
        "      attention = doc.tensors[1].to(device)\n",
        "      padded_attention = (torch.Tensor.long(torch.cat((torch.ones(1, device=device), attention, torch.zeros(longest_document - len(attention), device=device)), dim=0)))\n",
        "      attention_mask_list.append(padded_attention)\n",
        "\n",
        "      masks = doc.tensors[2].to(device)\n",
        "      padded_special_tokens = (torch.Tensor.long(torch.cat((torch.ones(1, device=device), masks, torch.ones(longest_document - len(masks), device=device)), dim=0)))\n",
        "      special_tokens_mask_list.append(padded_special_tokens)\n",
        "\n",
        "    elif len(doc.tensors[0]) == longest_document:\n",
        "      embedding = doc.tensors[0].to(device)\n",
        "      embedding = torch.cuda.FloatTensor(torch.cat((CLS_128.to(device), embedding), dim=0))\n",
        "      input_embeds_list.append(embedding)\n",
        "\n",
        "      attention = doc.tensors[1].to(device)\n",
        "      attention = torch.cuda.FloatTensor(torch.cat((torch.ones(1, device=device), attention), dim=0))\n",
        "      attention_mask_list.append(attention)\n",
        "\n",
        "      masks = doc.tensors[2].to(device)\n",
        "      masks = torch.cuda.FloatTensor(torch.cat((torch.ones(1, device=device), masks), dim=0))\n",
        "      special_tokens_mask_list.append(masks)\n",
        "\n",
        "  input_embeds = torch.stack(input_embeds_list)\n",
        "  attention_mask = torch.stack(attention_mask_list)\n",
        "  special_tokens_mask = torch.stack(special_tokens_mask_list)\n",
        "\n",
        "  return input_embeds, attention_mask, special_tokens_mask"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZjMUHenK7Lt"
      },
      "source": [
        "## 5.2 Learning to Rank Dataset: MS MARCO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbazGzRqa-8i"
      },
      "source": [
        "### 5.2.1 Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vibHoFjvkujV"
      },
      "source": [
        "class MSMarcoDocumentRankingDataset(Dataset):\n",
        "  def __init__(self, filepath):\n",
        "    with open(filepath, 'rb') as p:\n",
        "      self.dataset = pickle.load(p)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.dataset[idx]"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1uP0qLcbCYa"
      },
      "source": [
        "### 5.2.2 Collate Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icXGFw3kbKlY"
      },
      "source": [
        "#### Cross-Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwZvk-p0lCat"
      },
      "source": [
        "def prepare_msmarco_for_cross_encoder(batch):\n",
        "\n",
        "  hidden_size = len(batch[0].tensors[0][0])\n",
        "\n",
        "  CLS_128 = torch.unsqueeze(CLS, dim=0)\n",
        "  SEP_128 = torch.unsqueeze(SEP, dim=0)\n",
        "  \n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  longest_document = max(len(pair.tensors[1][0]) for pair in batch)\n",
        "  if longest_document > 509:\n",
        "    longest_document = 509\n",
        "\n",
        "  just_padded_queries =  []\n",
        "  just_padded_documents = []\n",
        "  pairs_embeddings_negative_list = []\n",
        "\n",
        "  query_document_embeddings = []\n",
        "  query_document_attention_mask = []\n",
        "  query_document_token_types = []\n",
        "  #query_document_special_tokens = []\n",
        "\n",
        "\n",
        "  for i, query_doc_pair in enumerate(batch):\n",
        "    if len(query_doc_pair.tensors[1][0]) <= longest_document:\n",
        "      CLS_128 = CLS_128.to(device)\n",
        "      query = query_doc_pair.tensors[0].to(device)\n",
        "      SEP_128 = SEP_128.to(device)\n",
        "      document = query_doc_pair.tensors[1][0].to(device)\n",
        "      doc_length = len(document)\n",
        "      padding = torch.randn(longest_document - doc_length, hidden_size, device=device)\n",
        "\n",
        "      full_embedding = torch.cat((CLS_128, query, SEP_128, document, padding), 0)\n",
        "      query_document_embeddings.append(full_embedding)\n",
        "      just_padded_queries.append(torch.cat((CLS_128, query, SEP_128), 0))\n",
        "      just_padded_documents.append(torch.cat((document, padding), 0))\n",
        "\n",
        "      attention = torch.cat((torch.ones(3 + doc_length, dtype=torch.long), torch.zeros(longest_document - doc_length, dtype=torch.long))).to(device)\n",
        "      query_document_attention_mask.append(attention)\n",
        "\n",
        "      token_types = torch.cat((torch.zeros(3, dtype=torch.long), torch.ones(longest_document, dtype=torch.long))).to(device)\n",
        "      query_document_token_types.append(token_types)\n",
        "\n",
        "      #special_tokens = torch.cat((torch.Tensor([1,0,1]), torch.zeros(doc_length), torch.ones(longest_document - doc_length)))\n",
        "      #query_document_special_tokens.append(special_tokens)\n",
        "\n",
        "  pairs_embeddings_positive = torch.stack(query_document_embeddings)\n",
        "  pairs_attention_positive = torch.stack(query_document_attention_mask)\n",
        "  pairs_token_types_positive = torch.stack(query_document_token_types)\n",
        "  #pairs_special_tokens_positive = torch.stack(query_document_special_tokens)\n",
        "\n",
        "  queries_unshuffled = torch.stack(just_padded_queries)\n",
        "  docs_unshuffled = torch.stack(just_padded_documents)\n",
        "  \n",
        "  indexes = torch.randperm(docs_unshuffled.shape[0])\n",
        "  docs_shuffled = docs_unshuffled[indexes]\n",
        "  for i, random_doc in enumerate(docs_shuffled):\n",
        "    negative_pair = torch.cat((queries_unshuffled[i], random_doc))\n",
        "    pairs_embeddings_negative_list.append(negative_pair)\n",
        "\n",
        "  pairs_embeddings_negative = torch.stack(pairs_embeddings_negative_list)\n",
        "  pairs_attention_negative = pairs_attention_positive[indexes]\n",
        "  pairs_token_types_negative = pairs_token_types_positive[indexes]\n",
        "  #pairs_special_tokens_negative = pairs_special_tokens_positive[indexes]\n",
        "\n",
        "  embeddings = torch.cat((pairs_embeddings_positive, pairs_embeddings_negative))\n",
        "  attention_mask = torch.cuda.LongTensor(torch.cat((pairs_attention_positive, pairs_attention_negative)))\n",
        "  token_types = torch.cuda.LongTensor(torch.cat((pairs_token_types_positive, pairs_token_types_negative)))\n",
        "  #special_tokens = torch.cuda.LongTensor(torch.cat((pairs_special_tokens_positive, pairs_special_tokens_negative)))\n",
        "\n",
        "  return embeddings, attention_mask, token_types#, special_tokens"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4w6Ha7A-tzPz"
      },
      "source": [
        "def prepare_msmarco_topx_for_cross_encoder(batch):\n",
        "  \"\"\"\n",
        "  Collate function that can load datasets with x number of initial retrievals\n",
        "  per query. The retrievals are stored in a list of lists where the first\n",
        "  dimension marks distinct query IDs and the corresponding retrievals are stored\n",
        "  in the second dimension.\n",
        "  Can be used in DataLoaders for both \"hard-mode\" training with in-query negatives\n",
        "  or for evaluation.\n",
        "  Note that for evaluation the batch size should be set to 1 in the DataLoader.\n",
        "\n",
        "  Args:\n",
        "    batch: list of lists consisting of batch_size number of query IDs and their\n",
        "      corresponding retrievals\n",
        "  \"\"\"\n",
        "\n",
        "  hidden_size = len(batch[0][0].tensors[1][0])\n",
        "\n",
        "  CLS_128 = torch.unsqueeze(CLS, dim=0)\n",
        "  SEP_128 = torch.unsqueeze(SEP, dim=0)\n",
        "\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  longest_document = 0\n",
        "  for query in batch:\n",
        "    max_per_query = max(len(retrieval.tensors[2][0]) for retrieval in query)\n",
        "    if max_per_query > longest_document:\n",
        "      longest_document = max_per_query\n",
        "\n",
        "  labels = []\n",
        "  query_document_embeddings = []\n",
        "  query_document_attention_mask = []\n",
        "  query_document_token_types = []\n",
        "  \n",
        "  for query in batch:\n",
        "    labels_per_query = []\n",
        "    for retrieval in query:\n",
        "      if len(retrieval.tensors[2][0]) <= longest_document:\n",
        "\n",
        "        CLS_128 = CLS_128.to(device)\n",
        "        query = retrieval.tensors[1].to(device)\n",
        "        SEP_128 = SEP_128.to(device)\n",
        "        document = retrieval.tensors[2][0].to(device)\n",
        "        doc_length = len(document)\n",
        "        padding = torch.randn(longest_document - doc_length, hidden_size, device=device)\n",
        "\n",
        "        full_embedding = torch.cat((CLS_128, query, SEP_128, document, padding), 0)\n",
        "        query_document_embeddings.append(full_embedding)\n",
        "        \n",
        "        attention = torch.cat((torch.ones(3 + doc_length, dtype=torch.long), torch.zeros(longest_document - doc_length, dtype=torch.long))).to(device)\n",
        "        query_document_attention_mask.append(attention)\n",
        "\n",
        "        token_types = torch.cat((torch.zeros(3, dtype=torch.long), torch.ones(longest_document, dtype=torch.long))).to(device)\n",
        "        query_document_token_types.append(token_types)\n",
        "\n",
        "        labels_per_query.append(retrieval.tensors[0][0][0].item())\n",
        "\n",
        "    labels.append(torch.LongTensor(labels_per_query))\n",
        "\n",
        "  labels = torch.stack(labels).to(device)\n",
        "\n",
        "  embeddings = torch.stack(query_document_embeddings)\n",
        "  pairs_attention_positive = torch.stack(query_document_attention_mask)\n",
        "  pairs_token_types_positive = torch.stack(query_document_token_types)\n",
        "  #pairs_special_tokens_positive = torch.stack(query_document_special_tokens)\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "    attention_mask = torch.cuda.LongTensor(pairs_attention_positive)\n",
        "    token_types = torch.cuda.LongTensor(pairs_token_types_positive)\n",
        "  else:\n",
        "    attention_mask = torch.LongTensor(pairs_attention_positive)\n",
        "    token_types = torch.LongTensor(pairs_token_types_positive)\n",
        "  #special_tokens = torch.cuda.LongTensor(torch.cat((pairs_special_tokens_positive, pairs_special_tokens_negative)))\n",
        "\n",
        "  return embeddings, attention_mask, token_types, labels#, query_ids,"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2sUNjbWbN_8"
      },
      "source": [
        "#### Cosine Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFjr3UaylJdp"
      },
      "source": [
        "def prepare_msmarco_for_cosine_similarity(batch):\n",
        "\n",
        "  hidden_size = len(batch[0].tensors[0][0])\n",
        "  CLS_128 = torch.unsqueeze(CLS, dim=0)\n",
        "  \n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  longest_document = max(len(pair.tensors[1][0]) for pair in batch)\n",
        "  if longest_document > 509:\n",
        "    longest_document = 509\n",
        "\n",
        "  query_embeddings_list = []\n",
        "  docs_padded_list = []\n",
        "  attention_list = []\n",
        "\n",
        "  for i, query_doc_pair in enumerate(batch):\n",
        "    if len(query_doc_pair.tensors[1][0]) <= longest_document:\n",
        "\n",
        "      query = query_doc_pair.tensors[0][0].to(device)\n",
        "      query_embeddings_list.append(query)\n",
        "\n",
        "      CLS_128 = CLS_128.to(device)\n",
        "      document = query_doc_pair.tensors[1][0].to(device)\n",
        "      doc_length = len(document)\n",
        "      padding = torch.randn(longest_document - doc_length, hidden_size, device=device)\n",
        "\n",
        "      doc_padded = torch.cat((CLS_128, document, padding), 0)\n",
        "      docs_padded_list.append(doc_padded)\n",
        "\n",
        "      attention = torch.cat((torch.ones(1 + doc_length, dtype=torch.long), torch.zeros(longest_document - doc_length, dtype=torch.long))).to(device)\n",
        "      attention_list.append(attention)\n",
        "\n",
        "  query_embeddings = torch.stack(query_embeddings_list)\n",
        "  docs_embeddings = torch.stack(docs_padded_list)\n",
        "  attention_mask = torch.stack(attention_list)\n",
        "\n",
        "  return query_embeddings, docs_embeddings, attention_mask"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJ1bWoDxkv4N"
      },
      "source": [
        "def prepare_msmarco_topx_for_cosine_similarity(batch):\n",
        "\n",
        "  hidden_size = len(batch[0][0].tensors[1][0])\n",
        "  CLS_128 = torch.unsqueeze(CLS, dim=0)\n",
        "\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  longest_document = 0\n",
        "  for query in batch:\n",
        "    max_per_query = max(len(retrieval.tensors[2][0]) for retrieval in query)\n",
        "    if max_per_query > longest_document:\n",
        "      longest_document = max_per_query\n",
        "\n",
        "  labels = []\n",
        "\n",
        "  query_embeddings_list = []\n",
        "  docs_padded_list = []\n",
        "  attention_list = []\n",
        "\n",
        "  for query in batch:\n",
        "    labels_per_query = []\n",
        "    for retrieval in query:\n",
        "      if len(retrieval.tensors[2][0]) <= longest_document:\n",
        "        query = retrieval.tensors[1].to(device)\n",
        "        query_embeddings_list.append(query)\n",
        "\n",
        "        CLS_128 = CLS_128.to(device)\n",
        "        document = retrieval.tensors[2][0].to(device)\n",
        "        doc_length = len(document)\n",
        "        padding = torch.randn(longest_document - doc_length, hidden_size, device=device)\n",
        "\n",
        "        doc_padded = torch.cat((CLS_128, document, padding), 0)\n",
        "        docs_padded_list.append(doc_padded)\n",
        "\n",
        "        attention = torch.cat((torch.ones(1 + doc_length, dtype=torch.long), torch.zeros(longest_document - doc_length, dtype=torch.long))).to(device)\n",
        "        attention_list.append(attention)\n",
        "\n",
        "        labels_per_query.append(retrieval.tensors[0][0][0].item())\n",
        "\n",
        "    labels.append(torch.LongTensor(labels_per_query))\n",
        "\n",
        "  labels = torch.stack(labels).to(device)\n",
        "\n",
        "\n",
        "  query_embeddings = torch.stack(query_embeddings_list)\n",
        "  docs_embeddings = torch.stack(docs_padded_list)\n",
        "  attention_mask = torch.stack(attention_list)\n",
        "\n",
        "  return query_embeddings, docs_embeddings, attention_mask, labels"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UiwsiplLGEN"
      },
      "source": [
        "## 5.3 Document Matching Dataset: GWikiMatch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALE7RCZ0y3B2"
      },
      "source": [
        "#**6.** Pretraining"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BDd9NdYtigI"
      },
      "source": [
        "### 6.1 Full Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQWBgEbxblTN"
      },
      "source": [
        "# https://github.com/cs230-stanford/cs230-code-examples/blob/master/pytorch/nlp/train.py\n",
        "# https://github.com/henryre/pytorch-fitmodule/blob/master/pytorch_fitmodule/fit_module.py\n",
        "# https://stackoverflow.com/questions/59584457/pytorch-is-there-a-definitive-training-loop-similar-to-keras-fit\n",
        "\n",
        "\n",
        "def pretrain_full(hate_config,\n",
        "                  dataset,\n",
        "                  use_gpu=False,\n",
        "                  train_parallel=False,\n",
        "                  use_gradient_clipping=False,\n",
        "                  avarage_gpu_losses=False,\n",
        "                  accumulate_gradients=False,\n",
        "                  config_filepath=None,\n",
        "                  load_checkpoint_filepath=None,\n",
        "                  save_checkpoint_filepath=None,\n",
        "                  log_filepath=None,\n",
        "                  save_every_n_iters=10,\n",
        "                  batch_size=2,\n",
        "                  epochs=5):\n",
        "  \n",
        "  dataset = PretrainingData(dataset)\n",
        "  split = [int(0.9*dataset.__len__()),int(0.1*(dataset.__len__()))]\n",
        "  train_dataset, validation_dataset = torch.utils.data.dataset.random_split(dataset, lengths=split)\n",
        "\n",
        "  train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_input_tokens)\n",
        "  validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_input_tokens)\n",
        "\n",
        "  # TODO read config from dict or json\n",
        "  #config = HATEModelConfig()\n",
        "  model = HATEModel(hate_config)\n",
        "\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() and use_gpu else \"cpu\")\n",
        "  if train_parallel and (torch.cuda.device_count() > 1):\n",
        "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "    # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
        "    model = nn.DataParallel(model)\n",
        "  model.to(device)\n",
        "  print(\"Running on \", device, \"\\n \\n\")\n",
        "\n",
        "  if log_filepath is not None:\n",
        "    writer = SummaryWriter(log_dir=log_filepath)\n",
        "    input = next(iter(train_dataloader))\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters())\n",
        "  scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10) # TODO what's a good T_0?\n",
        "\n",
        "  if load_checkpoint_filepath is not None:\n",
        "    checkpoint = torch.load(load_checkpoint_filepath)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epoch = checkpoint['epoch']\n",
        "    loss = checkpoint['loss']\n",
        "  \n",
        "\n",
        "  # TODO add logging\n",
        "  for epoch in range(epochs):\n",
        "    # Activates dropout layers\n",
        "    model.train()\n",
        "    # TODO add option to perform scheduler.step() every n iterations\n",
        "    scheduler.step()\n",
        "    for iteration, batch in enumerate(train_dataloader):\n",
        "      print(f\"Computing iteration {iteration} at epoch {epoch}\")\n",
        "      \n",
        "      # Move all inputs to the given device\n",
        "      for doc in batch[2]:\n",
        "        for key, value in doc.items():\n",
        "          doc[key] = doc[key].to(device)\n",
        "\n",
        "      # inputs = {'max_doc_length' : batch[0], 'doc_length_before_padding' : batch[1], 'batch' : batch[2], 'device' : device} --> **inputs\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(batch[0], batch[1], batch[2], device)\n",
        "      print(f\"Per Example Loss at epoch {epoch}, iteration {iteration}: \", outputs['document_model_per_example_loss'])\n",
        "      print(f\"Total Loss at epoch {epoch}, iteration {iteration}: \", outputs['total_loss'])\n",
        "      print(\"\\n\")\n",
        "      loss = outputs['total_loss']\n",
        "      loss.backward()       \n",
        "      optimizer.step()\n",
        "      # TODO Avarage GPU-losses? See medium post below\n",
        "      # TODO Add gradients accumulation option over multiple passes? See https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255\n",
        "\n",
        "      # Adds loss to log file for visualization\n",
        "      if log_filepath is not None:\n",
        "        writer.add_scalar(\"Loss/train\", loss, iteration)\n",
        "\n",
        "      # TODO specify the last dataset ID where we left of so we don't restart from the first document\n",
        "      if (iteration % save_every_n_iters == 0) and save_checkpoint_filepath is not None:\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'loss': loss,\n",
        "            }, save_checkpoint_filepath + \"checkpoint.tar\")\n",
        "        \n",
        "    writer.flush()\n",
        "\n",
        "    # Deactives stochastic properties\n",
        "    model.eval()\n",
        "    for iteration, batch in enumerate(validation_dataloader):\n",
        "      \n",
        "      # Moves data to given device\n",
        "      for doc in batch[2]:\n",
        "        for key, value in doc.items():\n",
        "          doc[key] = doc[key].to(device)\n",
        "      # inputs = {'max_doc_length' : batch[0], 'doc_length_before_padding' : batch[1], 'batch' : batch[2], 'device' : device} --> **inputs\n",
        "\n",
        "      outputs = model(batch[0], batch[1], batch[2], device)\n",
        "      total_loss = outputs['total_loss']\n",
        "      per_example_loss = outputs['document_model_per_example_loss']\n",
        "      sentence_loss = outputs['sentence_model_loss']\n",
        "      document_loss = outputs['document_model_loss']\n",
        "      if (iteration % 10 == 0):\n",
        "        # TODO print statistics\n",
        "        print(\"Still haven't added metrics\")\n",
        "  # TODO wrap this in a with statement\n",
        "  writer.close()"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPa7xk_Dtod6"
      },
      "source": [
        "### 6.2 Document Model Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJrMjoe-JUEK"
      },
      "source": [
        "def pretrain_doc_model(model, config, dataset_directory, batch_size, epochs, save_name, load_checkpoint_path = None):\n",
        "  \n",
        "  pickle_batches = [] # A list of the individual Pickle filenames\n",
        "\n",
        "  for filename in os.listdir(dataset_directory):\n",
        "    if filename.endswith(\".pkl\"):\n",
        "        pickle_batches.append(dataset_directory + filename)\n",
        "\n",
        "  \n",
        "  dataset_length = len(pickle_batches)*32768\n",
        "\n",
        "  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "  save_checkpoint_path='/content/drive/MyDrive/Bachelor/Thesis/checkpoints/'\n",
        "  \n",
        "  if load_checkpoint_path is None: # if a model with these parameters has already been trained before (exists in checkpoints/) then append a random number to save_name\n",
        "    save_name = save_name # + random number\n",
        "\n",
        "  writer = SummaryWriter(log_dir=f'/content/drive/MyDrive/Bachelor/Thesis/logs/{save_name}')\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters())\n",
        "  scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10)\n",
        "\n",
        "  step = 1\n",
        "\n",
        "  if load_checkpoint_path is not None:\n",
        "    print(device)\n",
        "    checkpoint = torch.load(load_checkpoint_path, map_location=torch.device(device))\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    elapsed_epochs = checkpoint['epoch']\n",
        "    loss_history = checkpoint['loss_history']\n",
        "    step = checkpoint['step']\n",
        "  else:\n",
        "    elapsed_epochs = 0\n",
        "    loss_history = []\n",
        "    step = 1\n",
        "\n",
        "  if not os.path.exists(save_checkpoint_path + save_name):\n",
        "    os.mkdir(save_checkpoint_path + save_name)\n",
        "\n",
        "  with open(f'{save_checkpoint_path}{save_name}/{save_name}.pkl', 'wb') as c:\n",
        "    pickle.dump(config, c)\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  for epoch in range(epochs - elapsed_epochs):\n",
        "    for megabatch in pickle_batches:\n",
        "      dataset = PretrainingDataPreEncoded(megabatch)\n",
        "      dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_input_embeds)\n",
        "      scheduler.step()\n",
        "      for batch in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        for doc in batch:\n",
        "          doc[0] = doc[0].to(device)\n",
        "          doc[1] = doc[1].to(device)\n",
        "          doc[2] = doc[2].to(device)\n",
        "        masked_input_data = mask_input_embeddings(batch[0], batch[2], device)\n",
        "        print(batch[1].size())\n",
        "        print(masked_input_data[2].size())\n",
        "        print(cer)\n",
        "        model_outputs = model(attention_mask=batch[1],\n",
        "                              inputs_embeds=masked_input_data[1],\n",
        "                              labels_embeddings=masked_input_data[2],\n",
        "                              labels_mask=masked_input_data[3])\n",
        "        loss = model_outputs['loss_product']\n",
        "        loss.backward()       \n",
        "        optimizer.step()\n",
        "        loss_history.append({'loss_product' : model_outputs['loss_product'], 'loss_variance_product' : model_outputs['loss_variance_product'] , 'loss_distance' : model_outputs['loss_distance'], 'loss_variance_distance' : model_outputs['loss_variance_product']})\n",
        "        writer.add_scalar(\"Product loss\", loss, step)\n",
        "        writer.add_scalar(\"Per example loss variance\", model_outputs['loss_product'], step)\n",
        "        print(f\"STEP {step}: loss_product: {model_outputs['loss_product']}, variance {model_outputs['loss_variance_product']} // loss_distance: {model_outputs['loss_distance']}, variance {model_outputs['loss_variance_product']}\")\n",
        "        if step % 1000 == 0:\n",
        "          torch.save({'epoch': epoch,\n",
        "                      'model_state_dict': model.state_dict(),\n",
        "                      'optimizer_state_dict': optimizer.state_dict(),\n",
        "                      'scheduler_state_dict': scheduler.state_dict(),\n",
        "                      'loss_history': loss_history,\n",
        "                      'step': step\n",
        "                      }, f'{save_checkpoint_path}{save_name}/S-{step}.tar')\n",
        "        step += 1\n",
        "    writer.flush()\n",
        "  writer.close()"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wm_2yiWg0hGO"
      },
      "source": [
        "# **7.** Finetuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zapMTCO-BAcn"
      },
      "source": [
        "## 7.1 Finetuning Goals and Principles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3s9HHqxQlUz"
      },
      "source": [
        "## 7.2 MS MARCO Document Ranking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcvB0xjtQtNE"
      },
      "source": [
        "### 7.2.1 Learning to Rank"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-vDGYbWdz3l"
      },
      "source": [
        "- https://microsoft.github.io/msmarco (has list of other document reranking models)\n",
        "\n",
        "- MS MARCO: A Human Generated MAchine Reading COmprehension Dataset https://arxiv.org/pdf/1611.09268.pdf\n",
        "\n",
        "- https://microsoft.github.io/TREC-2020-Deep-Learning/\n",
        "\n",
        "\n",
        "\n",
        "also relevant for finetuning:\n",
        "\n",
        "- RepBERT: Contextualized Text Embeddings for First-Stage Retrieval https://arxiv.org/pdf/2006.15498.pdf\n",
        "- TwinBERT: Distilling Knowledge to Twin-Structured BERT Models for Efficient Retrieval (1) https://arxiv.org/pdf/2002.06275.pdf (2) https://github.com/deepampatel/TwinBert/blob/master/TwinBert.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyOv4ZUquCdd"
      },
      "source": [
        "Alternative Tasks:\n",
        "- https://research.google/tools/datasets/ Wikipedia and arXiv similarity triplets\n",
        "- https://github.com/LiqunW/Long-document-dataset with this paper: Long Document Classification From Local Word Glimpses via Recurrent Attention Learning https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8675939\n",
        "- https://datasets.quantumstat.com/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlY-Oj9GX-v8"
      },
      "source": [
        "### 7.2.3 Document Ranking Task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22wOTsScwy40"
      },
      "source": [
        "- https://github.com/microsoft/MSMARCO-Document-Ranking\n",
        "\n",
        "- Baseline: Longformer: The Long-Document Transformer (1) https://arxiv.org/pdf/2004.05150.pdf (2) https://github.com/isekulic/longformer-marco/blob/master/src/TransformerMarco.py\n",
        "Baseline: Conformer-Kernel with Query Term Independence for Document Retrieval (1) https://arxiv.org/pdf/2007.10434.pdf "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnjX9LYNahuY"
      },
      "source": [
        "### 7.2.4 Training Loop "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71j18VNfmIUI"
      },
      "source": [
        "- loss function for ranking task -> cosine contrastive loss? check this TwinBERT implementation https://github.com/deepampatel/TwinBert/blob/master/TwinBert.ipynb\n",
        "- what training objective is actually used in the finetuning here? is it already a ranking task? could the loss function be the same as in pretraining? \n",
        "- ranking algorithm for the document representations\n",
        "- Maybe train a cross-encoder like in the sentence transformer library?\n",
        "- A batch consits of n Query-Document pairs (stored in dictonaries)\n",
        "- The query as well as the documents have been passed through the sentence model as a preprocessing step (or not? would be nice to train the model end-to-end)\n",
        "- All the documents in the batch get passed through the document model just as during pretraining\n",
        "- We do not use any sentence masking though, afer \"contextualizing\" all the sentence embeddings via the document model we compute elementwise similarities between the query and all sentence embeddings of a document PLUS a randomly sampled second document from the batch, that's considered our negative pair per query\n",
        "- We slide an RBF Kernel over both rows per one query, the positive document similarity scores and the negative ones\n",
        "- We compute a loss (contrastive or whatever) per triple (query, positive sample, negative sample)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugSalCWClg_9"
      },
      "source": [
        "def finetune_msmarco_cross_encoder(model, config, dataset_directory, batch_size, epochs, save_name, load_checkpoint_path=None):\n",
        "  pickle_batches = [] # A list of the individual Pickle filenames\n",
        "\n",
        "  for filename in os.listdir(dataset_directory):\n",
        "    if filename.endswith(\".pkl\"):\n",
        "        pickle_batches.append(dataset_directory + filename)\n",
        "\n",
        "  \n",
        "  dataset_length = len(pickle_batches)*32768\n",
        "\n",
        "  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "  print(device)\n",
        "\n",
        "  save_checkpoint_path='/content/drive/MyDrive/Bachelor/Thesis/checkpoints/'\n",
        "\n",
        "  writer = SummaryWriter(log_dir=f'/content/drive/MyDrive/Bachelor/Thesis/logs/{save_name}')\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters())\n",
        "  scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10)\n",
        "\n",
        "  eval_step = 1\n",
        "\n",
        "  if load_checkpoint_path is not None:\n",
        "    print(device)\n",
        "    checkpoint = torch.load(load_checkpoint_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    elapsed_epochs = checkpoint['epoch']\n",
        "    loss_history = checkpoint['loss_history']\n",
        "    mrr_eval_history = checkpoint['mrr_eval_history']\n",
        "    step = checkpoint['step']\n",
        "  else:\n",
        "    elapsed_epochs = 0\n",
        "    loss_history = []\n",
        "    mrr_eval_history = []\n",
        "    step = 1\n",
        "\n",
        "  # if not os.path.exists(save_checkpoint_path + save_name):\n",
        "  #   os.mkdir(save_checkpoint_path + save_name)\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  # with open(f'{save_checkpoint_path}{save_name}/{save_name}.pkl', 'wb') as c:\n",
        "  #   pickle.dump(config, c)\n",
        "\n",
        "  for epoch in range(epochs - elapsed_epochs):\n",
        "    for megabatch in pickle_batches:\n",
        "      dataset = MSMarcoDocumentRankingDataset(megabatch)\n",
        "      dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=prepare_msmarco_for_cross_encoder)\n",
        "      scheduler.step()\n",
        "      for batch in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        model_outputs = model(attention_mask=batch[1], token_type_ids=batch[2], inputs_embeds=batch[0], compute_loss=True)\n",
        "        loss = model_outputs[2]\n",
        "        loss.backward()       \n",
        "        optimizer.step()\n",
        "        loss_history.append(model_outputs[2])\n",
        "        \n",
        "        writer.add_scalar(\"Ranking loss\", loss, step)\n",
        "        print(f\"STEP {step}: ranking_loss: {model_outputs[2]}\")          \n",
        "        #if step % 1000 == 0:\n",
        "          # mrr_eval = evaluate_ranking_model(model, 'cross_encoder', keep_training=True)\n",
        "          # mrr_eval_history.append(mrr_eval)\n",
        "          # writer.add_scalar(\"MRR@20 Eval\", mrr_eval, eval_step)\n",
        "          # eval_step += 1\n",
        "          # torch.save({'epoch': epoch,\n",
        "          #             'model_state_dict': model.state_dict(),\n",
        "          #             'optimizer_state_dict': optimizer.state_dict(),\n",
        "          #             'scheduler_state_dict': scheduler.state_dict(),\n",
        "          #             'loss_history': loss_history,\n",
        "          #             'mrr_eval_history': mrr_eval_history,\n",
        "          #             'step': step\n",
        "          #             }, f'{save_checkpoint_path}{save_name}/S-{step}.tar')\n",
        "        step += 1\n",
        "    writer.flush()\n",
        "  writer.close()"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUEpq2KxWm-b"
      },
      "source": [
        "def finetune_msmarco_cosine_similarity(model, config, dataset_directory, batch_size, epochs, save_name, load_checkpoint_path=None):\n",
        "  \n",
        "  pickle_batches = [] # A list of the individual Pickle filenames\n",
        "\n",
        "  for filename in os.listdir(dataset_directory):\n",
        "    if filename.endswith(\".pkl\"):\n",
        "        pickle_batches.append(dataset_directory + filename)\n",
        "\n",
        "  \n",
        "  dataset_length = len(pickle_batches)*32768\n",
        "\n",
        "  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "  print(device)\n",
        "\n",
        "  save_checkpoint_path='/content/drive/MyDrive/Bachelor/Thesis/checkpoints/'\n",
        "\n",
        "  writer = SummaryWriter(log_dir=f'/content/drive/MyDrive/Bachelor/Thesis/logs/{save_name}')\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters())\n",
        "  scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10)\n",
        "\n",
        "  eval_step = 1\n",
        "\n",
        "  if load_checkpoint_path is not None:\n",
        "    print(device)\n",
        "    checkpoint = torch.load(load_checkpoint_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    elapsed_epochs = checkpoint['epoch']\n",
        "    loss_history = checkpoint['loss_history']\n",
        "    mrr_eval_history = checkpoint['mrr_eval_history']\n",
        "    step = checkpoint['step']\n",
        "  else:\n",
        "    elapsed_epochs = 0\n",
        "    loss_history = []\n",
        "    mrr_eval_history = []\n",
        "    step = 1\n",
        "\n",
        "  if not os.path.exists(save_checkpoint_path + save_name):\n",
        "    os.mkdir(save_checkpoint_path + save_name)\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  with open(f'{save_checkpoint_path}{save_name}/{save_name}.pkl', 'wb') as c:\n",
        "    pickle.dump(config, c)\n",
        "\n",
        "  for epoch in range(epochs - elapsed_epochs):\n",
        "    for megabatch in pickle_batches:\n",
        "      dataset = MSMarcoDocumentRankingDataset(megabatch)\n",
        "      dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=prepare_msmarco_for_cosine_similarity)\n",
        "      scheduler.step()\n",
        "      for batch in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        model_outputs = model(attention_mask=batch[2], query_embeds=batch[0], inputs_embeds=batch[1], compute_loss=True)\n",
        "        loss = model_outputs[2]\n",
        "        loss.backward()       \n",
        "        optimizer.step()\n",
        "        loss_history.append(model_outputs[2])\n",
        "        writer.add_scalar(\"Cosine Embedding Loss\", loss, step)\n",
        "        print(f\"STEP {step}: Running Ranking Loss (last 10): {sum(loss_history[-10:])/10} // Current Loss: {model_outputs[2]}\")          \n",
        "        if step % 1000 == 0:\n",
        "          # mrr_eval = evaluate_ranking_model(model, 'cosine_similarity', keep_training=True)\n",
        "          # mrr_eval_history.append(mrr_eval)\n",
        "          # writer.add_scalar(\"MRR@20 Eval\", mrr_eval, eval_step)\n",
        "          # eval_step += 1\n",
        "          torch.save({'epoch': epoch,\n",
        "                      'model_state_dict': model.state_dict(),\n",
        "                      'optimizer_state_dict': optimizer.state_dict(),\n",
        "                      'scheduler_state_dict': scheduler.state_dict(),\n",
        "                      'loss_history': loss_history,\n",
        "                      'mrr_eval_history': mrr_eval_history,\n",
        "                      'step': step\n",
        "                      }, f'{save_checkpoint_path}{save_name}/S-{step}.tar')\n",
        "        step += 1\n",
        "    writer.flush()\n",
        "  writer.close()"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qRaLtACAi-S"
      },
      "source": [
        "### 7.2.5 MRR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92nzWNB3mCgW"
      },
      "source": [
        "def mean_reciprocal_rank(rs):\n",
        "  \"\"\"Score is reciprocal of the rank of the first relevant item\n",
        "  First element is 'rank 1'.  Relevance is binary (nonzero is relevant).\n",
        "  Example from http://en.wikipedia.org/wiki/Mean_reciprocal_rank\n",
        "  >>> rs = [[0, 0, 1], [0, 1, 0], [1, 0, 0]]\n",
        "  >>> mean_reciprocal_rank(rs)\n",
        "  0.61111111111111105\n",
        "  >>> rs = np.array([[0, 0, 0], [0, 1, 0], [1, 0, 0]])\n",
        "  >>> mean_reciprocal_rank(rs)\n",
        "  0.5\n",
        "  >>> rs = [[0, 0, 0, 1], [1, 0, 0], [1, 0, 0]]\n",
        "  >>> mean_reciprocal_rank(rs)\n",
        "  0.75\n",
        "  Args:\n",
        "    rs: Iterator of relevance scores (list or numpy) in rank order\n",
        "        (first element is the first item)\n",
        "  Returns:\n",
        "    Mean reciprocal rank\n",
        "  \"\"\"\n",
        "  rs = (np.asarray(r).nonzero()[0] for r in rs)\n",
        "  return np.mean([1. / (r[0] + 1) if r.size else 0. for r in rs])"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JS7apa8EIjeJ"
      },
      "source": [
        "def evaluate_ranking_model(model, mode, embedding_mode=None, dataset_filepath='/content/drive/MyDrive/Bachelor/Thesis/datasets/ms_marco/docdev_top20/docdev_top20_eval_small.pkl', keep_training=False):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    embedding_mode: If mode='cosine_similarity' is chosen then set embedding_mode\n",
        "      either to 'cls', 'mean' or 'sum', else 'None'\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  model.eval()\n",
        "  reranked_labels = []\n",
        "  \n",
        "  if mode == 'cross_encoder':\n",
        "    #print(\"Chose cross-encoder mode\")\n",
        "    if os.path.isfile(dataset_filepath):\n",
        "      dataset = MSMarcoDocumentRankingDataset(dataset_filepath)\n",
        "      #print(\"Initialized Dataset\")\n",
        "      dataloader = DataLoader(dataset=dataset, batch_size=1, collate_fn=prepare_msmarco_topx_for_cross_encoder)\n",
        "      #print(\"Initialized DataLoader\")\n",
        "      for i, batch in enumerate(dataloader):\n",
        "        #if i % 100 == 0:\n",
        "          #print(f\"Computing batch {i}\")\n",
        "        model_outputs = model(attention_mask=batch[1], token_type_ids=batch[2], inputs_embeds=batch[0], labels_initial=batch[3]) # cls_embeddings, predictions, sorted_predictions, sorted_indices, labels_initial, labels_reranked\n",
        "        labels = model_outputs[5].tolist()\n",
        "        labels = [0 if x==-1.0 else x for x in labels]\n",
        "        reranked_labels.append(labels)\n",
        "\n",
        "    elif os.path.isdir(dataset_filepath):\n",
        "      pickle_batches = [] # A list of the individual Pickle filenames\n",
        "      for filename in os.listdir(dataset_filepath):\n",
        "        if filename.endswith(\".pkl\"):\n",
        "            pickle_batches.append(dataset_filepath + filename)\n",
        "\n",
        "      for i, megabatch in enumerate(pickle_batches):\n",
        "        dataset = PretrainingDataPreEncoded(megabatch)\n",
        "        dataloader = DataLoader(dataset, batch_size=1, collate_fn=prepare_msmarco_topx_for_cross_encoder)\n",
        "        for j, batch in enumerate(dataloader):\n",
        "          #if j % 100 == 0:\n",
        "            #print(f\"Computing batch {i*100 + j}\")\n",
        "          model_outputs = model(attention_mask=batch[1], token_type_ids=batch[2], inputs_embeds=batch[0], labels_initial=batch[3]) # cls_embeddings, predictions, sorted_predictions, sorted_indices, labels_initial, labels_reranked\n",
        "          labels = model_outputs[5].tolist()\n",
        "          labels = [0 if x==-1.0 else x for x in labels]\n",
        "          reranked_labels.append(labels)\n",
        "\n",
        "\n",
        "\n",
        "  elif mode == 'cosine_similarity':\n",
        "    if os.path.isfile(dataset_filepath):\n",
        "      #print(\"Chose cosine similarity mode\")\n",
        "      dataset = MSMarcoDocumentRankingDataset(dataset_filepath)\n",
        "      #print(\"Initialized Dataset\")\n",
        "      dataloader = DataLoader(dataset=dataset, batch_size=1, collate_fn=prepare_msmarco_topx_for_cosine_similarity)\n",
        "      #print(\"Initialized DataLoader\")\n",
        "      for i, batch in enumerate(dataloader):\n",
        "        #if i % 100 == 0:\n",
        "          #print(f\"Computing batch {i}\")\n",
        "        if model.__class__.__name__ == 'NaiveCosineSimilarityRanking':\n",
        "          model_outputs = model(embedding_mode, attention_mask=batch[2], query_embeds=batch[0], inputs_embeds=batch[1], labels_initial=batch[3]) # cls_embeddings, predictions, sorted_predictions, sorted_indices, labels_initial, labels_reranked\n",
        "          labels = model_outputs[4].tolist()\n",
        "        else:\n",
        "          model_outputs = model(embedding_mode, attention_mask=batch[2], query_embeds=batch[0], inputs_embeds=batch[1], labels_initial=batch[3]) # cls_embeddings, predictions, sorted_predictions, sorted_indices, labels_initial, labels_reranked\n",
        "          labels = model_outputs[5].tolist()\n",
        "        labels = [0 if x==-1.0 else x for x in labels]\n",
        "        reranked_labels.append(labels)\n",
        "    \n",
        "    elif os.path.isdir(dataset_filepath):\n",
        "      pickle_batches = [] # A list of the individual Pickle filenames\n",
        "      for filename in os.listdir(dataset_filepath):\n",
        "        if filename.endswith(\".pkl\"):\n",
        "            pickle_batches.append(dataset_filepath + filename)\n",
        "\n",
        "      for i, megabatch in enumerate(pickle_batches):\n",
        "        dataset = PretrainingDataPreEncoded(megabatch)\n",
        "        dataloader = DataLoader(dataset, batch_size=1, collate_fn=prepare_msmarco_topx_for_cosine_similarity)\n",
        "        for j, batch in enumerate(dataloader):\n",
        "          #if j % 100 == 0:\n",
        "            #print(f\"Computing batch {i*100 + j}\")\n",
        "          if model.__class__.__name__ == 'NaiveCosineSimilarityRanking':\n",
        "            model_outputs = model(embedding_mode, attention_mask=batch[2], query_embeds=batch[0], inputs_embeds=batch[1], labels_initial=batch[3]) # cls_embeddings, predictions, sorted_predictions, sorted_indices, labels_initial, labels_reranked\n",
        "            labels = model_outputs[4].tolist()\n",
        "          else:\n",
        "            model_outputs = model(embedding_mode, attention_mask=batch[2], query_embeds=batch[0], inputs_embeds=batch[1], labels_initial=batch[3]) # cls_embeddings, predictions, sorted_predictions, sorted_indices, labels_initial, labels_reranked\n",
        "            labels = model_outputs[5].tolist()\n",
        "          labels = [0 if x==-1.0 else x for x in labels]\n",
        "          reranked_labels.append(labels)\n",
        "\n",
        "\n",
        "\n",
        "  elif mode == 'baseline':\n",
        "    #print(\"Chose baseline mode\")\n",
        "    if os.path.isfile(dataset_filepath):\n",
        "      dataset = MSMarcoDocumentRankingDataset(dataset_filepath)\n",
        "      #print(\"Initialized Dataset\")\n",
        "      dataloader = DataLoader(dataset=dataset, batch_size=1, collate_fn=prepare_msmarco_topx_for_cosine_similarity)\n",
        "      #print(\"Initialized DataLoader\")\n",
        "      for i, batch in enumerate(dataloader):\n",
        "        #if i % 100 == 0:\n",
        "          #print(f\"Computing batch {i}\")\n",
        "        labels = batch[3][0].tolist()\n",
        "        labels = [0 if x==-1.0 else x for x in labels]\n",
        "        reranked_labels.append(labels)\n",
        "\n",
        "    elif os.path.isdir(dataset_filepath):\n",
        "      pickle_batches = [] # A list of the individual Pickle filenames\n",
        "      for filename in os.listdir(dataset_filepath):\n",
        "        if filename.endswith(\".pkl\"):\n",
        "            pickle_batches.append(dataset_filepath + filename)\n",
        "\n",
        "      for i, megabatch in enumerate(pickle_batches):\n",
        "        dataset = PretrainingDataPreEncoded(megabatch)\n",
        "        dataloader = DataLoader(dataset, batch_size=1, collate_fn=prepare_msmarco_topx_for_cosine_similarity)\n",
        "        for j, batch in enumerate(dataloader):\n",
        "          #if j % 100 == 0:\n",
        "            #print(f\"Computing batch {i*100 + j}\")\n",
        "          labels = batch[3][0].tolist()\n",
        "          labels = [0 if x==-1.0 else x for x in labels]\n",
        "          reranked_labels.append(labels)\n",
        "      \n",
        "  else:\n",
        "    print(\"Enter a valid mode: 'cross_encoder', 'cosine_similarity', or 'baseline'\")\n",
        "\n",
        "  mrr = mean_reciprocal_rank(reranked_labels)\n",
        "\n",
        "  if keep_training:\n",
        "    model.train()\n",
        "\n",
        "  return mrr"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDJxvVKGx5_e"
      },
      "source": [
        "When we evaluate the cross-encoder we could visualize the attention weights to see whether the model actually paid attention to the correct passage with regards to the query embedding!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTiCkRtNAmte"
      },
      "source": [
        "- MRR@100\n",
        "- NDCG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4vLsZn8fp9m"
      },
      "source": [
        "### 7.2.6 Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Uez2pMCfz5z"
      },
      "source": [
        "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# print(device)\n",
        "\n",
        "# ranking_config = HATEDocumentModelConfig(hidden_size=128, num_hidden_layers=2, num_attention_heads=2, intermediate_size=512)\n",
        "# ranking_model = HATEDocumentModelCrossEncoderRanking(ranking_config)\n",
        "\n",
        "# folder = 'FT_MS_XE_DM_L-2_H-128_A-2_B-64_E-20_D-262144_V-1'\n",
        "# model_version = 'FT_MS_XE_DM_L-2_H-128_A-2_B-64_E-20_D-262144_V-1_S-16384.tar'\n",
        "# ranking_model = load_weights(ranking_model, f'/content/drive/MyDrive/Bachelor/Thesis/checkpoints/{folder}/{model_version}', device)\n",
        "\n",
        "# pytorch_total_params = sum(p.numel() for p in ranking_model.parameters())\n",
        "# print(pytorch_total_params)\n",
        "# ranking_model.to(device)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6OFg2LDf9Ru"
      },
      "source": [
        "# mrr = evaluate_ranking_model(ranking_model, 'cross_encoder', 'mean')#, dataset_filepath='/content/drive/MyDrive/Bachelor/Thesis/datasets/ms_marco/docdev_top100_1000_queries/') \n",
        "# print(mrr)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_m2Yy_kgAh2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07185fa5-c4e8-4e15-ebb3-31865927954d"
      },
      "source": [
        "results = pd.DataFrame(columns=['Model name', 'Pretraining', 'Finetuning', 'Document Embedding Mode', 'MRR@20', 'MRR@100*'])\n",
        "\n",
        "results = results.append({'Model name': 'BM25', 'Pretraining': np.nan, 'Finetuning': np.nan, 'Document Embedding Mode': np.nan, 'MRR@20': 0.292, 'MRR@100*': 0.237}, ignore_index=True)\n",
        "results = results.append({'Model name': 'HATE SM 4M', 'Pretraining': 'Knowledge Distiallation', 'Finetuning': np.nan, 'Document Embedding Mode': 'Mean', 'MRR@20': 0.260, 'MRR@100*': 0.117}, ignore_index=True)\n",
        "results = results.append({'Model name': 'HATE DM 4M', 'Pretraining': np.nan, 'Finetuning': np.nan, 'Document Embedding Mode': 'CLS', 'MRR@20': 0.126, 'MRR@100*': 0.035}, ignore_index=True)\n",
        "\n",
        "results = results.append({'Model name': 'HATE DM 4M', 'Pretraining': 'Wiki600k', 'Finetuning': np.nan, 'Document Embedding Mode': 'CLS', 'MRR@20': np.nan, 'MRR@100*': 0.037}, ignore_index=True)\n",
        "results = results.append({'Model name': 'HATE DM 4M', 'Pretraining': 'Wiki600k', 'Finetuning': np.nan, 'Document Embedding Mode': 'Mean', 'MRR@20': np.nan, 'MRR@100*': 0.053}, ignore_index=True)\n",
        "\n",
        "\n",
        "results = results.append({'Model name': 'HATE DM 4M', 'Pretraining': \"Wiki300k\", 'Finetuning': \"CS MS130k\", 'Document Embedding Mode': 'CLS', 'MRR@20': 0.387, 'MRR@100*': np.nan}, ignore_index=True)\n",
        "results = results.append({'Model name': 'HATE DM 4M', 'Pretraining': \"Wiki600k\", 'Finetuning': \"CS MS260k\", 'Document Embedding Mode': 'CLS', 'MRR@20': np.nan, 'MRR@100*': 0.207}, ignore_index=True)\n",
        "results = results.append({'Model name': 'HATE DM 4M', 'Pretraining': np.nan, 'Finetuning': \"CS MS260k\", 'Document Embedding Mode': 'CLS', 'MRR@20': np.nan, 'MRR@100*': 0.220}, ignore_index=True)\n",
        "\n",
        "\n",
        "results = results.append({'Model name': 'HATE DM 4M', 'Pretraining': np.nan, 'Finetuning': \"XE MS130k\", 'Document Embedding Mode': 'CLS', 'MRR@20': 0.314, 'MRR@100*': np.nan}, ignore_index=True)\n",
        "results = results.append({'Model name': 'HATE DM 4M', 'Pretraining': np.nan, 'Finetuning': \"XE MS260k\", 'Document Embedding Mode': 'CLS', 'MRR@20': 0.177, 'MRR@100*': 0.037}, ignore_index=True)\n",
        "\n",
        "results.to_csv('/content/drive/MyDrive/Bachelor/Thesis/metrics/results.tsv', sep='\\t')\n",
        "\n",
        "# MRR@100*\n",
        "# Baseline: 0.237\n",
        "\n",
        "# Randomly initialized CS model: 0.035\n",
        "# Randomly initialized XE model: 0.039\n",
        "\n",
        "# Naive CS mean: 0.117\n",
        "# Naive CS sum: 0.117\n",
        "\n",
        "# PT600k cls: 0.037 (30k steps) 0.031 (50k steps)\n",
        "# PT600k mean: 0.057 (30k steps) 0.053 (50k steps)\n",
        "# PT600k sum: 0.058 (30k steps) 0.052 (50k steps)\n",
        "\n",
        "# CS260k: 0.220 (16k steps)\n",
        "# CS260k: 0.214 (45k steps)\n",
        "\n",
        "# XE260k: 0.037 (16k steps)\n",
        "\n",
        "# PT600k + CS260k: 0.207\n",
        "# PT600k + XE260k: \n",
        "\n",
        "print(results)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Model name              Pretraining  ... MRR@20 MRR@100*\n",
            "0        BM25                      NaN  ...  0.292    0.237\n",
            "1  HATE SM 4M  Knowledge Distiallation  ...  0.260    0.117\n",
            "2  HATE DM 4M                      NaN  ...  0.126    0.035\n",
            "3  HATE DM 4M                 Wiki600k  ...    NaN    0.037\n",
            "4  HATE DM 4M                 Wiki600k  ...    NaN    0.053\n",
            "5  HATE DM 4M                 Wiki300k  ...  0.387      NaN\n",
            "6  HATE DM 4M                 Wiki600k  ...    NaN    0.207\n",
            "7  HATE DM 4M                      NaN  ...    NaN    0.220\n",
            "8  HATE DM 4M                      NaN  ...  0.314      NaN\n",
            "9  HATE DM 4M                      NaN  ...  0.177    0.037\n",
            "\n",
            "[10 rows x 6 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "anV8MLha89qY",
        "outputId": "89877b99-9dba-46ea-a446-9ea87aa520a1"
      },
      "source": [
        "baseline = [(0, 0.237), (50, 0.237)]\n",
        "\n",
        "random = [(0, 0.035), (50, 0.035)]\n",
        "\n",
        "sm_4m_mean_pt = [(0, 0.117), (50, 0.117)]\n",
        "\n",
        "dm_4m_cls_xe = [(0, 0.039), (16, 0.037)]\n",
        "\n",
        "dm_4m_cls_pt = [(0, 0.035), (30, 0.031), (50, 0.037)]\n",
        "dm_4m_mean_pt = [(0, 0.035), (30, 0.053), (50, 0.057)]\n",
        "\n",
        "dm_4m_cls_cs = [(0, 0.034), (4, 0.166), (8, 0.218), (12, 0.215), (16, 0.214), (20, 0.224), (24, 0.219), (28, 0.217), (32, 0.220), (36, 0.222), (40, 0.220)]\n",
        "\n",
        "\n",
        "\n",
        "plt.title('HATE Eval')\n",
        "plt.xlabel('1k Training Steps')\n",
        "plt.ylabel('MRR@100*')\n",
        "\n",
        "plt.plot(*zip(*baseline), 'r', label='Baseline')\n",
        "\n",
        "plt.plot(*zip(*random), 'k', label='Random weights')\n",
        "\n",
        "plt.plot(*zip(*sm_4m_mean_pt), 'b', label='SM 4M')\n",
        "\n",
        "# plt.plot(*zip(*dm_4m_cls_xe), 'y-o', label='DM 4M XE290k cls')\n",
        "\n",
        "plt.plot(*zip(*dm_4m_cls_pt), 'g-o', label='DM 4M PT600k cls')\n",
        "plt.plot(*zip(*dm_4m_mean_pt), 'c-o', label='DM 4M PT600k mean')\n",
        "\n",
        "plt.plot(*zip(*dm_4m_cls_cs), 'm-o', label='DM 4M FT2600k cls')\n",
        "\n",
        "\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iccbrZzuM4aR"
      },
      "source": [
        "## 7.3 Document Matching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9qJXR9fju4q"
      },
      "source": [
        "# **8.** End-to-End Pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NS1cnQeFn4_"
      },
      "source": [
        "## 8.1 Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeWkboqFF2C0"
      },
      "source": [
        "def fit_doc_model(training_objective='pretrain', ranking_mode='cs', preembedded_hidden_size=128, hidden_size=128, hidden_layers=2, attention_heads=2, intermediate_size=512, embedding_mode='cls', ranking_loss_margin=0.25, load_weights_path=None, restart_from_checkpoint=False, dataset_version='large', epochs=20):\n",
        "  \"\"\"\n",
        "  End-to-end function that receives the given parameters and trains the corresponding\n",
        "  model given the training objective and the other parameters.\n",
        "\n",
        "  Args:\n",
        "    training_objective:\n",
        "      'pretrain' or 'ranking'\n",
        "    ranking_mode:\n",
        "      'cs' for Cosine Similarity Model\n",
        "      'ce' for Cross Encoder Model\n",
        "    preembedded_hidden_size:\n",
        "    hidden_size:\n",
        "    hidden_layers: Set to 2 by default, number of encoder modules in the Transformer\n",
        "    attention_heads: Set to 2 by default, must divide the intermediary size\n",
        "    intermediary_size: Usually set to 4 times the size of hidden_size, must be divisible by the number of attention heads\n",
        "    embedding_mode: Set to 'cls' by default, can also be 'mean' or 'sum'\n",
        "    ranking_loss_margin: Set to 0.25 by default\n",
        "    load_weights_path:\n",
        "    restart_from_checkpoint: Only set to True if training was interrupted and you \n",
        "      want to pick up from a checkpoint, if you just want to load pretrained weights\n",
        "      leave unchanged and only provide load_weights_path\n",
        "    dataset_version:\n",
        "    epochs:\n",
        "  \"\"\"\n",
        "\n",
        "  save_name = f\"DM_L-{hidden_layers}_H-{hidden_size}_A-{attention_heads}_E-{epochs}\"\n",
        "\n",
        "  config = HATEDocumentModelConfig(hidden_size=hidden_size, num_hidden_layers=hidden_layers, num_attention_heads=attention_heads, intermediate_size=intermediate_size)\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  if training_objective == 'pretrain':\n",
        "    if dataset_version == 'large':\n",
        "      dataset_path = f'/content/drive/MyDrive/Bachelor/Thesis/datasets/wikipedia_preembedded_{preembedded_hidden_size}_600k/'\n",
        "      pretrain_prefix = \"PT_W-L_\"\n",
        "      save_name = pretrain_prefix + save_name\n",
        "    elif dataset_version == 'small':\n",
        "      dataset_path = f'/content/drive/MyDrive/Bachelor/Thesis/datasets/wikipedia_preembedded_{preembedded_hidden_size}_300k/'\n",
        "      pretrain_prefix = \"PT_W-S_\"\n",
        "      save_name = pretrain_prefix + save_name\n",
        "\n",
        "    model = HATEDocumentModel(config, preembedded_hidden_size)\n",
        "\n",
        "    if load_weights_path is not None:\n",
        "      model = load_weights(model, load_weights_path, device)\n",
        "    model.to(device)\n",
        "\n",
        "    if restart_from_checkpoint:\n",
        "      pretrain_doc_model(model, config, dataset_path, 64, epochs, save_name, load_weights_path)\n",
        "    else:\n",
        "      pretrain_doc_model(model, config, dataset_path, 64, epochs, save_name)\n",
        "\n",
        "\n",
        "  elif training_objective == 'ranking':\n",
        "    save_name = save_name + f\"_M-{ranking_loss_margin}_{embedding_mode.upper()}\"\n",
        "    if dataset_version == 'large':\n",
        "      dataset_path = f'/content/drive/MyDrive/Bachelor/Thesis/datasets/ms_marco/doctrain260k/'\n",
        "      prefix = \"FT_MS-L_\"\n",
        "    elif dataset_version == 'small':\n",
        "      dataset_path = f'/content/drive/MyDrive/Bachelor/Thesis/datasets/ms_marco/doctrain130k/'\n",
        "      prefix = \"FT_MS-S_\"\n",
        "    \n",
        "    if ranking_mode == 'cs':\n",
        "      model = HATEDocumentModelCosineSimilarityRanking(config, preembedded_hidden_size, ranking_loss_margin)\n",
        "      prefix = prefix + \"CS_\"\n",
        "    elif ranking_mode == 'ce':\n",
        "      model = HATEDocumentModelCrossEncoderRanking(config, preembedded_hidden_size, ranking_loss_margin)\n",
        "      prefix = prefix + \"CE_\"\n",
        "\n",
        "\n",
        "    if load_weights_path is not None:\n",
        "      model = load_weights(model, load_weights_path, device)\n",
        "    model.to(device)\n",
        "\n",
        "    if load_weights_path is not None and \"PT_W-L\" in load_weights_path:\n",
        "      prefix = \"PT_W-L_\" + prefix\n",
        "    elif load_weights_path is not None and \"PT_W-S\" in load_weights_path:\n",
        "      prefix = \"PT_W-S_\" + prefix\n",
        "\n",
        "    save_name = prefix + save_name\n",
        "\n",
        "    if restart_from_checkpoint:\n",
        "      if ranking_mode == 'cs':\n",
        "        finetune_msmarco_cosine_similarity(model, config, dataset_path, 32, epochs, save_name, load_weights_path)\n",
        "      elif ranking_mode == 'ce':\n",
        "        finetune_msmarco_cross_encoder(model, config, dataset_path, 64, epochs, save_name, load_weights_path)\n",
        "    else:\n",
        "      if ranking_mode == 'cs':\n",
        "        finetune_msmarco_cosine_similarity(model, config, dataset_path, 32, epochs, save_name)\n",
        "      elif ranking_mode == 'ce':\n",
        "        finetune_msmarco_cross_encoder(model, config, dataset_path, 64, epochs, save_name)\n",
        "\n",
        "\n",
        "  elif training_objective == 'matching':\n",
        "    print(\"Implement Matching Task\")\n",
        "\n",
        "  total_params = sum(p.numel() for p in model.parameters())"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RtbFbHdFq18"
      },
      "source": [
        "## 8.2 Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVJ-Kqp2j0Ro"
      },
      "source": [
        "Write all the trained modules in such a compact way that we can just hand it a document or sentence and it will infer its representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DDuJgAmiuZx"
      },
      "source": [
        "def count_parameters(model):\n",
        "  return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nw1qyXh4rMm"
      },
      "source": [
        "def evaluate_doc_model(model_name, embedding_mode='cls', target_metric='MRR@20'):\n",
        "  \n",
        "  model_type = ''\n",
        "\n",
        "  config_path = f'/content/drive/MyDrive/Bachelor/Thesis/checkpoints/{model_name}/{model_name}.pkl'\n",
        "  directory = f'/content/drive/MyDrive/Bachelor/Thesis/checkpoints/{model_name}'\n",
        "\n",
        "  with open(config_path, \"rb\") as config_file:\n",
        "    config = pickle.load(config_file)\n",
        "\n",
        "  if 'CE' in model_name:\n",
        "    model_type = 'cross_encoder'\n",
        "    model = HATEDocumentModelCrossEncoderRanking(config)\n",
        "  else:\n",
        "    model_type = 'cosine_similarity'\n",
        "    model = HATEDocumentModelCosineSimilarityRanking(config)\n",
        "\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  model.eval()\n",
        "  model.to(device)\n",
        "\n",
        "  eval_scores = []\n",
        "\n",
        "  for i, checkpoint in enumerate(os.listdir(directory)):\n",
        "    if checkpoint.endswith('.tar'):\n",
        "      model = load_weights(model, f'{directory}/{checkpoint}', device)\n",
        "      model.eval()\n",
        "      model.to(device)\n",
        "      if target_metric == 'MRR@20':\n",
        "        mrr = evaluate_ranking_model(model, model_type, embedding_mode, dataset_filepath='/content/drive/MyDrive/Bachelor/Thesis/datasets/ms_marco/docdev_top20/docdev_top20_eval_small.pkl')\n",
        "      elif target_metric == 'MRR@100':\n",
        "        mrr = evaluate_ranking_model(model, model_type, embedding_mode, dataset_filepath='/content/drive/MyDrive/Bachelor/Thesis/datasets/ms_marco/docdev_top100_1000_queries/')\n",
        "      print(f\"{target_metric} at step {checkpoint}: \", mrr)\n",
        "      eval_scores.append(mrr)\n",
        "\n",
        "  with open(f'/content/drive/MyDrive/Bachelor/Thesis/checkpoints/{model_name}/{target_metric}-{embedding_mode}.pkl', 'wb') as e:\n",
        "    pickle.dump(eval_scores, e)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6Dl2lL1Ery9"
      },
      "source": [
        "def get_best_checkpoint(print_all_scores=False, test_on_mrr100=False, embedding_mode='cls'):\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  checkpoint_directory = '/content/drive/MyDrive/Bachelor/Thesis/checkpoints/'\n",
        "  mrr100_filepath='/content/drive/MyDrive/Bachelor/Thesis/datasets/ms_marco/docdev_top100_1000_queries/'\n",
        "  ranking_model_type = ''\n",
        "\n",
        "  for model_name in os.listdir(checkpoint_directory):\n",
        "    if 'V-1' not in model_name:\n",
        "      if 'MRR@20.pkl' in os.listdir(f'{checkpoint_directory}{model_name}/'):\n",
        "        with open(f'{checkpoint_directory}{model_name}/MRR@20.pkl', 'rb') as mrr_at_20:\n",
        "          scores = pickle.load(mrr_at_20)\n",
        "          print(f\"{model_name}: Best MRR@20 {max(scores)} in S-{(scores.index(max(scores))+1)*1000}.tar after {len(scores)*1000} total training steps\")\n",
        "          if print_all_scores:\n",
        "            print(scores, '\\n')\n",
        "          if test_on_mrr100:\n",
        "            with open(f'{checkpoint_directory}{model_name}/{model_name}.pkl', 'rb') as c:\n",
        "              ranking_config = pickle.load(c)\n",
        "              if 'CE' in model_name:\n",
        "                embedding_mode='cls'\n",
        "                ranking_model = HATEDocumentModelCrossEncoderRanking(ranking_config)\n",
        "                ranking_model_type = 'cross_encoder'\n",
        "              else:\n",
        "                ranking_model = HATEDocumentModelCosineSimilarityRanking(ranking_config)\n",
        "                ranking_model_type = 'cosine_similarity'\n",
        "                \n",
        "              ranking_model = load_weights(ranking_model, f'{checkpoint_directory}{model_name}/S-{(scores.index(max(scores))+1)*1000}.tar', device)\n",
        "              ranking_model.to(device)\n",
        "              mrr100 = evaluate_ranking_model(ranking_model, ranking_model_type, embedding_mode, mrr100_filepath)\n",
        "              print(f\"Score on MRR@100*: {mrr100}\\n\")"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMVSuQBXxApK"
      },
      "source": [
        "## 8.3 Visualize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0i2m2ZlzxFTq"
      },
      "source": [
        "def plot_loss_eval_per_model(model_name, color, axis_1, axis_2, label, smoothing_factor=300):\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  checkpoint_directory = '/content/drive/MyDrive/Bachelor/Thesis/checkpoints/'\n",
        "  mrr100_filepath='/content/drive/MyDrive/Bachelor/Thesis/datasets/ms_marco/docdev_top100_1000_queries/'\n",
        "  model_name = model_name\n",
        "  model_path = checkpoint_directory + model_name\n",
        "  last_checkpoint = os.listdir(f'{model_path}')[-3:-2][0]\n",
        "  eval_scores = os.listdir(f'{model_path}')[-1:][0]\n",
        "\n",
        "  with open(f'{model_path}/{last_checkpoint}', 'rb') as cp:\n",
        "    checkpoint = torch.load(cp, map_location=device)\n",
        "\n",
        "    loss = []\n",
        "    for loss_checkpoint in checkpoint['loss_history']:\n",
        "      if not torch.is_tensor(loss_checkpoint) and 'loss_product' in loss_checkpoint.keys():\n",
        "        loss.append(loss_checkpoint['loss_product'].item())\n",
        "      else:\n",
        "        loss.append(loss_checkpoint.item())\n",
        "\n",
        "    \n",
        "    smoothed_loss = []\n",
        "    for i, value in enumerate(loss):\n",
        "      if len(loss[:i]) > smoothing_factor:\n",
        "        smoothed_loss.append(sum(loss[:i][-smoothing_factor:])/smoothing_factor)\n",
        "      else:\n",
        "        smoothed_loss.append(value)\n",
        "    axis_1.plot(smoothed_loss, color, label=label)\n",
        "\n",
        "  with open(f'{model_path}/{eval_scores}', 'rb') as e:\n",
        "    eval_scores_list = pickle.load(e)\n",
        "    scale = list(range(len(eval_scores_list)))\n",
        "    for i in scale:\n",
        "      scale[i] = scale[i]*1000\n",
        "    axis_2.plot(scale, eval_scores_list, color, label=label)\n"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKLiiI3p53zc"
      },
      "source": [
        "def plot_loss_eval_per_hidden_size(hidden_size=128):\n",
        "\n",
        "  colors = ['gray', 'indianred', 'cadetblue', 'burlywood', 'mediumslateblue']\n",
        "\n",
        "  fig, (ax1,ax2) = plt.subplots(1,2)\n",
        "  fig.set_figwidth(15)\n",
        "  #ax2 = ax1.twinx()\n",
        "\n",
        "  ax1.set_xlabel('Training Steps (1k)')\n",
        "  ax1.set_ylabel('Loss', color='black')\n",
        "  ax1.set_yscale('log')\n",
        "  ax1.legend(loc='upper right')\n",
        "  ax2.set_ylabel('Eval MRR@20', color='black')\n",
        "  ax2.axhline(y=0.126, color='black', linestyle=':')\n",
        "  ax2.axhline(y=0.292, color='black', linestyle='--')\n",
        "  ax2.legend(loc='lower right')\n",
        "\n",
        "  model_names = os.listdir('/content/drive/MyDrive/Bachelor/Thesis/checkpoints/')\n",
        "\n",
        "  model_names = [name for name in model_names if f'{hidden_size}' in name and 'L-4' not in name]\n",
        "\n",
        "  for i, model in enumerate(model_names):\n",
        "    if 'PT' in model and 'FT' not in model:\n",
        "      color = colors[0]\n",
        "      label = \"PT\"\n",
        "    elif 'PT' in model and 'CS' in model:\n",
        "      color = colors[1]\n",
        "      label = \"PT+CS\"\n",
        "    elif 'PT' in model and 'CE' in model:\n",
        "      color = colors[2]\n",
        "      label = \"PT + XE\"\n",
        "    elif 'PT' not in model and 'CS' in model:\n",
        "      color = colors[3]\n",
        "      label = \"CS\"\n",
        "    elif 'PT' not in model and 'CE' in model:\n",
        "      color = colors[4]\n",
        "      label = \"XE\"\n",
        "    plot_loss_eval_per_model(model, color, ax1, ax2, label)\n",
        "\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "  "
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVcYhugRehlh"
      },
      "source": [
        "def display_pca_scatterplot_3D(first_text_pt, first_text_list, second_text_pt, second_text_list, single_sentences_pt=None, single_sentences_list=None):\n",
        "  \n",
        "  first_text_np = first_text_pt.cpu().detach().numpy()\n",
        "  second_text_np = second_text_pt.cpu().detach().numpy()\n",
        "\n",
        "\n",
        "  first_text_three_dim = PCA(random_state=0).fit_transform(first_text_np)[:,:3]\n",
        "  second_text_three_dim = PCA(random_state=0).fit_transform(second_text_np)[:,:3]\n",
        "\n",
        "  if single_sentences_pt is not None:\n",
        "    single_sentences_np = single_sentences_pt.cpu().detach().numpy()\n",
        "    sentences_three_dim = PCA(random_state=0).fit_transform(single_sentences_np)[:,:3]\n",
        "    print(second_text_three_dim)\n",
        "\n",
        "  data = []\n",
        "  count = 0\n",
        "\n",
        "  # Compute Coordinates for first text\n",
        "  trace_input = go.Scatter3d(\n",
        "                    x = first_text_three_dim[count:,0], \n",
        "                    y = first_text_three_dim[count:,1],  \n",
        "                    z = first_text_three_dim[count:,2],\n",
        "                    text = first_text_list[count:],\n",
        "                    name = 'Potato Chips Contextualized',\n",
        "                    textposition = \"top center\",\n",
        "                    textfont_size = 20,\n",
        "                    mode = 'markers+text',\n",
        "                    marker = {\n",
        "                        'size': 10,\n",
        "                        'opacity': 1,\n",
        "                        'color': 'orange'\n",
        "                    }\n",
        "                    )\n",
        "  \n",
        "  data.append(trace_input)\n",
        "\n",
        "\n",
        "  # Compute Coordinates for second text\n",
        "  trace_input = go.Scatter3d(\n",
        "                    x = second_text_three_dim[0:,0], \n",
        "                    y = second_text_three_dim[0:,1],  \n",
        "                    z = second_text_three_dim[0:,2],\n",
        "                    text = second_text_list[count:],\n",
        "                    name = 'CPU Chips Contextualized',\n",
        "                    textposition = \"top center\",\n",
        "                    textfont_size = 20,\n",
        "                    mode = 'markers+text',\n",
        "                    marker = {\n",
        "                        'size': 10,\n",
        "                        'opacity': 1,\n",
        "                        'color': 'grey'\n",
        "                    }\n",
        "                    )\n",
        "  \n",
        "  data.append(trace_input)\n",
        "\n",
        "\n",
        "  # Compute Coordinates for single sentences\n",
        "  if single_sentences_pt is not None:\n",
        "    trace_input = go.Scatter3d(\n",
        "                    x = sentences_three_dim[0:,0], \n",
        "                    y = sentences_three_dim[0:,1],  \n",
        "                    z = sentences_three_dim[0:,2],\n",
        "                    text = single_sentences_list[count:],\n",
        "                    name = 'Uncontextualized sentence',\n",
        "                    textposition = \"top center\",\n",
        "                    textfont_size = 20,\n",
        "                    mode = 'markers+text',\n",
        "                    marker = {\n",
        "                        'size': 10,\n",
        "                        'opacity': 1,\n",
        "                        'color': 'green'\n",
        "                    }\n",
        "                    )\n",
        "    data.append(trace_input)\n",
        "\n",
        "\n",
        "  # Configure the layout\n",
        "  layout = go.Layout(\n",
        "        margin = {'l': 0, 'r': 0, 'b': 0, 't': 0},\n",
        "        showlegend=True,\n",
        "        legend=dict(\n",
        "        x=1,\n",
        "        y=0.5,\n",
        "        font=dict(\n",
        "            family=\"Courier New\",\n",
        "            size=25,\n",
        "            color=\"black\"\n",
        "        )),\n",
        "        font = dict(\n",
        "            family = \" Courier New \",\n",
        "            size = 15),\n",
        "        autosize = False,\n",
        "        width = 1000,\n",
        "        height = 1000\n",
        "        )\n",
        "\n",
        "\n",
        "  plot_figure = go.Figure(data = data, layout = layout)\n",
        "  plot_figure.show()"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5mJmVBJ7V_q"
      },
      "source": [
        "## 8.4 Execute"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqZnXYu17hQ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "899d5b08-1cd2-4232-8438-e83b6d1bf8d6"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Apr 14 17:42:57 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P0    37W / 250W |   3713MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8bIxrG_GMYt"
      },
      "source": [
        "fit_doc_model(training_objective='ranking', ranking_mode='ce')#, load_weights_path='/content/drive/MyDrive/Bachelor/Thesis/checkpoints/PT_W-L_DM_L-4_H-128_A-4_E-20/S-36000.tar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qde8eBAnyhtr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9d9d9e7-d14c-427c-9551-8e62b413ae45"
      },
      "source": [
        "evaluate_doc_model(model_name='PT_W-L_FT_MS-L_CS_DM_L-4_H-128_A-4_E-20_M-0.25_CLS', embedding_mode='cls', target_metric='MRR@100')"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MRR@100 at step S-1000.tar:  0.15749410290997087\n",
            "MRR@100 at step S-2000.tar:  0.18312653651102692\n",
            "MRR@100 at step S-3000.tar:  0.19268099553389967\n",
            "MRR@100 at step S-4000.tar:  0.18881303494758206\n",
            "MRR@100 at step S-5000.tar:  0.22235497914735897\n",
            "MRR@100 at step S-6000.tar:  0.2247564088150094\n",
            "MRR@100 at step S-7000.tar:  0.22957394526816888\n",
            "MRR@100 at step S-8000.tar:  0.22872143300557524\n",
            "MRR@100 at step S-9000.tar:  0.22544187255509607\n",
            "MRR@100 at step S-10000.tar:  0.20346507440482867\n",
            "MRR@100 at step S-11000.tar:  0.1982552455241597\n",
            "MRR@100 at step S-12000.tar:  0.203421667690005\n",
            "MRR@100 at step S-13000.tar:  0.22623414879364373\n",
            "MRR@100 at step S-14000.tar:  0.23008568207592348\n",
            "MRR@100 at step S-15000.tar:  0.23807715991511977\n",
            "MRR@100 at step S-16000.tar:  0.23393173545030968\n",
            "MRR@100 at step S-17000.tar:  0.2204412709056336\n",
            "MRR@100 at step S-18000.tar:  0.2240966827241066\n",
            "MRR@100 at step S-19000.tar:  0.222599765304642\n",
            "MRR@100 at step S-20000.tar:  0.205839459397016\n",
            "MRR@100 at step S-21000.tar:  0.22209205399263\n",
            "MRR@100 at step S-22000.tar:  0.2299517229997544\n",
            "MRR@100 at step S-23000.tar:  0.22853764240625582\n",
            "MRR@100 at step S-24000.tar:  0.23392747368539904\n",
            "MRR@100 at step S-25000.tar:  0.21763130359282554\n",
            "MRR@100 at step S-26000.tar:  0.21782291430229406\n",
            "MRR@100 at step S-27000.tar:  0.22204782359233843\n",
            "MRR@100 at step S-28000.tar:  0.21711812921260126\n",
            "MRR@100 at step S-29000.tar:  0.23611452819034545\n",
            "MRR@100 at step S-30000.tar:  0.22683445454950685\n",
            "MRR@100 at step S-31000.tar:  0.2323688902013987\n",
            "MRR@100 at step S-32000.tar:  0.23805555087738745\n",
            "MRR@100 at step S-33000.tar:  0.22018784854211007\n",
            "MRR@100 at step S-34000.tar:  0.21840687694710936\n",
            "MRR@100 at step S-35000.tar:  0.21802475782108652\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OS1tTh41Babz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bfbfd29-c2dc-488d-ac51-101656e46f4c"
      },
      "source": [
        "get_best_checkpoint(test_on_mrr100=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PT_W-L_DM_L-2_H-128_A-2_E-20: Best MRR@20 0.2275225647417598 in S-7000.tar after 187000 total training steps\n",
            "PT_W-L_DM_L-2_H-64_A-2_E-20: Best MRR@20 0.24425609018604377 in S-9000.tar after 60000 total training steps\n",
            "PT_W-L_DM_L-2_H-16_A-2_E-20: Best MRR@20 0.1958400849064936 in S-3000.tar after 37000 total training steps\n",
            "PT_W-L_DM_L-2_H-32_A-2_E-20: Best MRR@20 0.22100434924552573 in S-9000.tar after 38000 total training steps\n",
            "FT_MS-L_CS_DM_L-2_H-16_A-2_E-20: Best MRR@20 0.3137527460413684 in S-50000.tar after 136000 total training steps\n",
            "FT_MS-L_CS_DM_L-2_H-32_A-2_E-20: Best MRR@20 0.3097533154066436 in S-33000.tar after 42000 total training steps\n",
            "FT_MS-L_CS_DM_L-2_H-64_A-2_E-20: Best MRR@20 0.3140990579939496 in S-48000.tar after 49000 total training steps\n",
            "FT_MS-L_CS_DM_L-2_H-128_A-2_E-20: Best MRR@20 0.398018890165562 in S-79000.tar after 126000 total training steps\n",
            "PT_W-L_FT_MS-L_CS_DM_L-2_H-128_A-2_E-20: Best MRR@20 0.3962868231355848 in S-128000.tar after 151000 total training steps\n",
            "PT_W-L_FT_MS-L_CS_DM_L-2_H-32_A-2_E-20: Best MRR@20 0.3346862413128667 in S-106000.tar after 136000 total training steps\n",
            "PT_W-L_FT_MS-L_CS_DM_L-2_H-64_A-2_E-20: Best MRR@20 0.33082836971250285 in S-71000.tar after 101000 total training steps\n",
            "PT_W-L_FT_MS-L_CS_DM_L-2_H-16_A-2_E-20: Best MRR@20 0.30975906634645023 in S-20000.tar after 127000 total training steps\n",
            "FT_MS-L_CE_DM_L-2_H-16_A-2_E-20_M-0.1_CLS: Best MRR@20 0.27209955658376717 in S-24000.tar after 51000 total training steps\n",
            "FT_MS-L_CE_DM_L-2_H-64_A-2_E-20_M-0.05_CLS: Best MRR@20 0.30495983274551386 in S-12000.tar after 12000 total training steps\n",
            "FT_MS-L_CE_DM_L-2_H-128_A-2_E-20_M-0.15_CLS: Best MRR@20 0.33529682076609324 in S-7000.tar after 10000 total training steps\n",
            "FT_MS-L_CE_DM_L-2_H-32_A-2_E-20_M-0.25_CLS: Best MRR@20 0.33508936339718076 in S-61000.tar after 70000 total training steps\n",
            "PT_W-L_FT_MS-L_CE_DM_L-2_H-128_A-2_E-20_M-0.25_CLS: Best MRR@20 0.36619316389664686 in S-10000.tar after 42000 total training steps\n",
            "PT_W-L_FT_MS-L_CE_DM_L-2_H-64_A-2_E-20_M-0.25_CLS: Best MRR@20 0.35320578095794813 in S-26000.tar after 45000 total training steps\n",
            "PT_W-L_FT_MS-L_CE_DM_L-2_H-32_A-2_E-20_M-0.25_CLS: Best MRR@20 0.32605867696974505 in S-17000.tar after 42000 total training steps\n",
            "PT_W-L_FT_MS-L_CE_DM_L-2_H-16_A-2_E-20_M-0.25_CLS: Best MRR@20 0.3161199906904241 in S-31000.tar after 41000 total training steps\n",
            "PT_W-L_DM_L-4_H-128_A-4_E-20: Best MRR@20 0.2312418382614977 in S-36000.tar after 37000 total training steps\n",
            "PT_W-L_FT_MS-L_CS_DM_L-4_H-128_A-4_E-20_M-0.25_CLS: Best MRR@20 0.39377162876683003 in S-16000.tar after 35000 total training steps\n",
            "PT_W-L_FT_MS-L_CE_DM_L-4_H-128_A-4_E-20_M-0.25_CLS: Best MRR@20 0.40402396760446296 in S-36000.tar after 38000 total training steps\n",
            "FT_MS-L_CS_DM_L-4_H-128_A-4_E-20_M-0.25_CLS: Best MRR@20 0.4011715964032527 in S-13000.tar after 35000 total training steps\n",
            "Score on MRR@100*: 0.2333224560150317\n",
            "\n",
            "FT_MS-L_CE_DM_L-4_H-128_A-4_E-20_M-0.25_CLSbad: Best MRR@20 0.37435384908349306 in S-11000.tar after 20000 total training steps\n",
            "Score on MRR@100*: 0.014790012549381943\n",
            "\n",
            "FT_MS-L_CE_DM_L-4_H-128_A-4_E-20_M-0.25_CLS: Best MRR@20 0.4240676830393548 in S-14000.tar after 39000 total training steps\n",
            "Score on MRR@100*: 0.22163398824397354\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJQgh23E6urS"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "ranking_config = HATEDocumentModelConfig()\n",
        "ranking_model = HATEDocumentModelCosineSimilarityRanking(ranking_config)\n",
        "#ranking_model = load_weights(ranking_model, '/content/drive/MyDrive/Bachelor/Thesis/checkpoints/FT_MS-L_DM_L-2_H-64_A-2_E-20/S-49000.tar', device)\n",
        "ranking_model.to(device)\n",
        "\n",
        "mrr = evaluate_ranking_model(ranking_model, 'cosine_similarity', 'cls', dataset_filepath='/content/drive/MyDrive/Bachelor/Thesis/datasets/ms_marco/docdev_top100_1000_queries/') \n",
        "print(mrr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "oy9vO5spNoZF",
        "outputId": "2d4168f9-d310-4510-98b8-7586420691f9"
      },
      "source": [
        "plot_loss_eval_per_model('PT_W-L_DM_L-2_H-32_A-2_E-20')"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6AMApcrsJ_2"
      },
      "source": [
        "import matplotlib"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "EFyryiiXPYcf",
        "outputId": "cfc4047f-8cc0-4d3b-d510-6ad2af1a204f"
      },
      "source": [
        "points = [0,1,2,4,8,16,32,64,128,256,512,768,896,960,992,1008,1016,1020,1022,1021]\n",
        "points_scaled = [i*2 for i in points]\n",
        "points_scaled.reverse()\n",
        "\n",
        "scale = list(range(20))\n",
        "for i in scale:\n",
        "  scale[i] = scale[i]*100\n",
        "\n",
        "comparison = list(range(200))\n",
        "\n",
        "fig, ax1 = plt.subplots()\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax1.plot(scale, points, 'g-')\n",
        "ax2.plot(scale, points_scaled, 'b-')\n",
        "\n",
        "ax1.set_xlabel('X data')\n",
        "ax1.set_ylabel('Y1 data', color='g')\n",
        "ax2.set_ylabel('Y2 data', color='b')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# plt.plot(comparison)\n",
        "# plt.plot(scale, points)\n",
        "# plt.plot(scale, points_scaled)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcirSK4Cx3P4"
      },
      "source": [
        "###############\n",
        "# TENSORBOARD #\n",
        "###############\n",
        "\n",
        "%load_ext tensorboard\n",
        "import datetime, os\n",
        "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "print(logdir)\n",
        "\n",
        "%tensorboard --logdir '/content/drive/MyDrive/Bachelor/Thesis/logs/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBc__W4vyL5U"
      },
      "source": [
        "# **9.** Outlook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-KmTKws5rqj"
      },
      "source": [
        "- Deeper look at attention: transformers are graph neural networks https://thegradient.pub/transformers-are-graph-neural-networks/\n",
        "- Generalization to Hopfield Nets: (1) https://ml-jku.github.io/hopfield-layers/ (2) http://franksworld.com/2020/08/10/explaining-the-paper-hopfield-networks-is-all-you-need/ (3) https://analyticsindiamag.com/modern-hopfield-network-transformers-attention/ (4) https://towardsdatascience.com/hopfield-networks-are-useless-heres-why-you-should-learn-them-f0930ebeadcd\n",
        "- Implications of these findings for language representation?\n",
        "- Check http://nlp.seas.harvard.edu/code/ for more ideas\n",
        "- Reducing time-complexity of Attention"
      ]
    }
  ]
}