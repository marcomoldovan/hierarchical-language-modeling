{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hierarchical-transformer-based-document-encoder.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "kpwbDUkHtLu9",
        "-B019eaBuLMx",
        "oItvz3k-uFSp",
        "gQzW6fxmtWEN",
        "gmX65TyjwxM-",
        "wO88nttwP0hi",
        "TZLIP6O_KRQP",
        "Ll4GDtfJKg76",
        "ZYYMKT3ILK5j",
        "YmWFH0wZQMeF",
        "Pw55fnznsNp5",
        "UmWmUTykN22p",
        "65pD0jHzthMl",
        "2Vx6Jhu7oRW_",
        "W8MVxIXFoYEI",
        "xYWwncTdp0sg",
        "Qgh7LvX2RWME",
        "kmJT1yF3i9k8",
        "LvdSZfthl-QN",
        "aFQ_HrliRfml",
        "ue_TauSD2gWp",
        "17qoRvtU2oEL",
        "GGoLVfY2i1P8",
        "oyPDLz_kN7uE",
        "pq3B99GAzqT3",
        "Ika4rqTyztVJ",
        "bPOCt2ECzw9Q",
        "ah6qLiazN-xP",
        "aVbDO-544Hb6",
        "ZJqgfSbh4OYN",
        "-1834mgo4S5k",
        "7IVJdy-_qAPt",
        "F3evzCMPzdeL",
        "PSXwRXFRziOi",
        "6Pc3_xs3zmp-",
        "aTTNBPu6MMsX",
        "1-smV5mIMXzj",
        "_DriQikaMnxv",
        "wBnJKkLtc7YE",
        "5IO_CxT4c1Kb",
        "UXwYKZCOUCuY"
      ],
      "mount_file_id": "1b4pp9mpqeumjRnUEMkhywInYeic480uk",
      "authorship_tag": "ABX9TyPNN4yIMqJuD0rD5Y18EMxO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcomoldovan/hierarchical-text-encoder/blob/master/hierarchical_transformer_based_document_encoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz5UHkyUWWrx"
      },
      "source": [
        "# **Hierarchical Transformer-based Text Encoder** \n",
        "\n",
        "We present a multi-purpose hierarchical text encoder (HATE) based on stacking sentence-level and document-level transformers on top of each other. By doing this we try to capture as many semantic facets of a given, arbitrarily long document. The model implicitly learns contextualized word and sentence representations as well a robust document representation. All encodings live in the same representation space and therefore similarity metrics such as cosine similarity can be applied on all levels of representations interchangeably. This has the purpose that only one model would have to be trained in order to encode both a query and a candidate document as well as all its contextualized sentences in one representation space where similarity measures could be applied in order to retrieve most relevant documents as well as scoring answer passages candidates by their likelihood of relevancy. Due to the robust sentence embeddings that are also produced, the model could be used for document segmentation as well: since intuitively there would be a semantic break between paragraphs this would also be reflected in the representations of the sentences. One could apply an algorithms that separates unstructured texts into paragraphs at points where the likelihood of a semantic break, and therefore the start of a separate paragraph is high."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6LstRZsO_YI"
      },
      "source": [
        "# **1.** Introduction\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpwbDUkHtLu9"
      },
      "source": [
        "# **2.** Background"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-B019eaBuLMx"
      },
      "source": [
        "## 2.1 Statistical foundations of machine learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRjb0HdQRGPW"
      },
      "source": [
        "(1) https://towardsdatascience.com/the-statistical-foundations-of-machine-learning-973c356a95f\n",
        "\n",
        "More on regression, classification, etc. in a probabilistic context. Show that these problems are essentially MAP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oItvz3k-uFSp"
      },
      "source": [
        "## 2.2 Mathematics of optimization for deep learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACsq9qRNRK7V"
      },
      "source": [
        "(1) https://towardsdatascience.com/the-mathematics-of-optimization-for-deep-learning-11af2b1fda30\n",
        "\n",
        "- Optimization: (1) visualizing loss landscape https://arxiv.org/pdf/1712.09913.pdf\n",
        "- Momentum based optimizers\n",
        "- Dropout (1) https://arxiv.org/pdf/1207.0580.pdf\n",
        "- Batch normalization (1) https://arxiv.org/abs/1502.03167 (2) https://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf\n",
        "- Weight initialization\n",
        "- Reguralization (1) NLP specific: https://mlexplained.com/2018/03/02/regularization-techniques-for-natural-language-processing-with-code-examples/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQzW6fxmtWEN"
      },
      "source": [
        "## 2.3 Representation Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejoRhg9wWkOK"
      },
      "source": [
        "Touch briefly on the theory of representation learning, independent of language. Main focus on the Bengio paper: https://arxiv.org/pdf/1206.5538.pdf Also refer to representation learning slides from DL&AI course from last semester as an appropriate introduction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmX65TyjwxM-"
      },
      "source": [
        "## 2.4 Language Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxwbJYXIQUpq"
      },
      "source": [
        "- Language modeling via auto-encoding or auto-regressive methods in general\n",
        "\n",
        "\n",
        "- Embeddings in Language Models (1) https://jalammar.github.io/skipgram-recommender-talk/Text (2) https://dspace.mit.edu/handle/1721.1/118079\n",
        "-  Word embeddings (1) https://ruder.io/word-embeddings-1/index.html (2) https://ruder.io/word-embeddings-softmax/index.html (3) https://ruder.io/secret-word2vec/index.html (4) https://ruder.io/word-embeddings-2017/index.html (5) https://jalammar.github.io/illustrated-word2vec/ (6) Glove https://mlexplained.com/2018/04/29/paper-dissected-glove-global-vectors-for-word-representation-explained/ (7) ELMo https://mlexplained.com/2018/06/15/paper-dissected-deep-contextualized-word-representations-explained/ (8) https://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html (9) https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html#loss-functions\n",
        "- Sentence embeddings (1) https://supernlp.github.io/2018/11/26/sentreps/ (2) https://mlexplained.com/2017/12/28/an-overview-of-sentence-embedding-methods/ (3) https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a\n",
        "- Document Embeddings (1) https://towardsdatascience.com/document-embedding-techniques-fed3e7a6a25d#1409 (2) https://graphaware.com/nlp/2018/09/03/advanced-document-representation.html\n",
        "\n",
        "- General early language models that are based on DL: RNNs/LSTM (touch shortly) (1) https://arxiv.org/pdf/1312.6026.pdf (2) https://distill.pub/2019/memorization-in-rnns/\n",
        "- On the diffictuly of training recurrent neural networks https://arxiv.org/pdf/1211.5063.pdf\n",
        "- Transition to attention mechanism, at first in RNNs\n",
        "- Why these large DL-based models are so important for transfer learning in NLP (1) https://ruder.io/transfer-learning/ (2) https://thegradient.pub/nlp-imagenet/ (3) very linguistic study of word embeddings for transfer tasks https://arxiv.org/pdf/1903.08855.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wO88nttwP0hi"
      },
      "source": [
        "# **3.** Related Work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZLIP6O_KRQP"
      },
      "source": [
        "## 3.1 Attention and why it's all you need"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACcPMJQLOvQU"
      },
      "source": [
        "- Attention is all you need (1) https://mlexplained.com/2017/12/29/attention-is-all-you-need-explained/ (2) https://arxiv.org/pdf/1706.03762.pdf (3) https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec (4) https://blog.floydhub.com/attention-mechanism/ (5) https://jalammar.github.io/illustrated-transformer/ (6) https://distill.pub/2016/augmented-rnns/ (7) https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html\n",
        "- BERT (1) https://mlexplained.com/2019/01/07/paper-dissected-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-explained/ (2) https://arxiv.org/pdf/1810.04805.pdf (3) https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/ (4) https://jalammar.github.io/illustrated-bert/\n",
        "- What Does BERT Look At? An Analysis of BERTâ€™s Attention \n",
        "https://arxiv.org/pdf/1906.04341.pdf https://arxiv.org/abs/1909.10430v2\n",
        "- BERT raw embeddings https://arxiv.org/abs/1909.00512v1 https://towardsdatascience.com/examining-berts-raw-embeddings-fd905cb22df7\n",
        "- Sentence embeddings revisited: BERT methods -> transition to intuition for my document representation model\n",
        "- Call-back to section 2.4 where I touch on classical embeddings and compare to deep, high-parameter, attention based models such as BERT\n",
        "- Why these large pretrained models are so important for transfer learning in NLP (1) https://ruder.io/transfer-learning/ (2) https://thegradient.pub/nlp-imagenet/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ll4GDtfJKg76"
      },
      "source": [
        "## 3.2 Hierarchical Attention-Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R44qFjnbj6o5"
      },
      "source": [
        "- SMITH (1) https://arxiv.org/pdf/2004.12297v1.pdf (2) https://github.com/dmolony3/SMITH\n",
        "- HIBERT (1) https://arxiv.org/pdf/1905.06566.pdf (2) https://github.com/liangsi03/hibert_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYYMKT3ILK5j"
      },
      "source": [
        "## 3.3 Information Retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhoR7CDFbCS2"
      },
      "source": [
        "- TANDA: Transfer and Adapt Pre-Trained Transformer Modelsfor Answer Sentence Selection https://arxiv.org/pdf/1911.04118.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmWFH0wZQMeF"
      },
      "source": [
        "# **4.** The Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pw55fnznsNp5"
      },
      "source": [
        "#### Basic imports and installs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBIOm4PPUx39"
      },
      "source": [
        "Introduce all necessary top-level imports and install the transformer module from huggingface"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hv0fZAmgRgTt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aba48bc5-468a-4ca0-af9b-2b4539fb7438"
      },
      "source": [
        "!pip install transformers #installs transformer module from huggingface\n",
        "!pip install datasets #installs dataset module from huggingface\n",
        "!pip install tokenizers #installs tokenizer module from huggingface\n",
        "\n",
        "import re\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import nltk\n",
        "from nltk import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as f\n",
        "import torchtext.datasets\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import transformers\n",
        "from transformers import BertConfig, BertModel, BertForMaskedLM\n",
        "from transformers.models.bert.modeling_bert import BertPreTrainedModel\n",
        "from transformers import configuration_utils\n",
        "from transformers.activations import ACT2FN \n",
        "from transformers.file_utils import ModelOutput\n",
        "from transformers.modeling_utils import (PreTrainedModel, \n",
        "                                         apply_chunking_to_forward, \n",
        "                                         find_pruneable_heads_and_indices,\n",
        "                                         prune_linear_layer,)\n",
        "from transformers.configuration_utils import PretrainedConfig\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "from pprint import pprint\n",
        "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.1.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.6/dist-packages (1.1.3)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.6/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from datasets) (1.19.4)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.0.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from datasets) (0.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.6/dist-packages (from datasets) (2.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.6/dist-packages (0.9.4)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmWmUTykN22p"
      },
      "source": [
        "## 4.1 Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65pD0jHzthMl"
      },
      "source": [
        "#### Internal Embedding Lookup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqn5_6P2ezri"
      },
      "source": [
        "class EmbeddingsLookup(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    # Initialize the lookup matrix for input IDs, positional embeddings and token types\n",
        "    self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
        "    self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "    self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
        "    \n",
        "    # Adds to Layer Normalization and Dropout on inital word embeddings\n",
        "    self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "    self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n",
        "    self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
        "    self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
        "\n",
        "  def forward(self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None):\n",
        "    if input_ids is not None:\n",
        "      input_shape = input_ids.size()\n",
        "    else:\n",
        "      input_shape = inputs_embeds.size()[:-1]\n",
        "\n",
        "    seq_length = input_shape[1]\n",
        "\n",
        "    if position_ids is None:\n",
        "      position_ids = self.position_ids[:, :seq_length]\n",
        "\n",
        "    if token_type_ids is None:\n",
        "      token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n",
        "\n",
        "    if inputs_embeds is None:\n",
        "      inputs_embeds = self.word_embeddings(input_ids)\n",
        "    token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "\n",
        "    embeddings = inputs_embeds + token_type_embeddings\n",
        "    if self.position_embedding_type == \"absolute\":\n",
        "      position_embeddings = self.position_embeddings(position_ids)\n",
        "      embeddings += position_embeddings\n",
        "    embeddings = self.LayerNorm(embeddings)\n",
        "    embeddings = self.dropout(embeddings)\n",
        "    return embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Vx6Jhu7oRW_"
      },
      "source": [
        "#### Encoder Stack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VOKcXtf4gml"
      },
      "source": [
        "class BaseModelOutputWithCrossAttentions(ModelOutput):\r\n",
        "    \"\"\"\r\n",
        "    Base class for model's outputs, with potential hidden states and attentions.\r\n",
        "    Args:\r\n",
        "        last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`):\r\n",
        "            Sequence of hidden-states at the output of the last layer of the model.\r\n",
        "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\r\n",
        "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\r\n",
        "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\r\n",
        "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\r\n",
        "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\r\n",
        "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\r\n",
        "            sequence_length, sequence_length)`.\r\n",
        "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\r\n",
        "            heads.\r\n",
        "        cross_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` and ``config.add_cross_attention=True`` is passed or when ``config.output_attentions=True``):\r\n",
        "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\r\n",
        "            sequence_length, sequence_length)`.\r\n",
        "            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\r\n",
        "            weighted average in the cross-attention heads.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    last_hidden_state: torch.FloatTensor = None\r\n",
        "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\r\n",
        "    attentions: Optional[Tuple[torch.FloatTensor]] = None\r\n",
        "    cross_attentions: Optional[Tuple[torch.FloatTensor]] = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AICiXa9JfZ59"
      },
      "source": [
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
        "      raise ValueError(\n",
        "        \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "        \"heads (%d)\" % (config.hidden_size, config.num_attention_heads)\n",
        "      )\n",
        "\n",
        "    self.num_attention_heads = config.num_attention_heads\n",
        "    self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "    self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "    self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "    self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "    self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "    self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "    self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
        "    if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
        "      self.max_position_embeddings = config.max_position_embeddings\n",
        "      self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n",
        "\n",
        "  def transpose_for_scores(self, x):\n",
        "    new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "    x = x.view(*new_x_shape)\n",
        "    return x.permute(0, 2, 1, 3)\n",
        "\n",
        "  def forward(self,\n",
        "              hidden_states,\n",
        "              attention_mask=None,\n",
        "              head_mask=None,\n",
        "              encoder_hidden_states=None,\n",
        "              encoder_attention_mask=None,\n",
        "              output_attentions=False):\n",
        "    \n",
        "    mixed_query_layer = self.query(hidden_states)\n",
        "\n",
        "    # If this is instantiated as a cross-attention module, the keys\n",
        "    # and values come from an encoder; the attention mask needs to be\n",
        "    # such that the encoder's padding tokens are not attended to.\n",
        "    if encoder_hidden_states is not None:\n",
        "        mixed_key_layer = self.key(encoder_hidden_states)\n",
        "        mixed_value_layer = self.value(encoder_hidden_states)\n",
        "        attention_mask = encoder_attention_mask\n",
        "    else:\n",
        "        mixed_key_layer = self.key(hidden_states)\n",
        "        mixed_value_layer = self.value(hidden_states)\n",
        "\n",
        "    query_layer = self.transpose_for_scores(mixed_query_layer)    \n",
        "    key_layer = self.transpose_for_scores(mixed_key_layer)\n",
        "    value_layer = self.transpose_for_scores(mixed_value_layer)\n",
        "\n",
        "    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "\n",
        "    if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
        "      seq_length = hidden_states.size()[1]\n",
        "      position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n",
        "      position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n",
        "      distance = position_ids_l - position_ids_r\n",
        "      positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n",
        "      positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n",
        "\n",
        "      if self.position_embedding_type == \"relative_key\":\n",
        "        relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
        "        attention_scores = attention_scores + relative_position_scores\n",
        "      elif self.position_embedding_type == \"relative_key_query\":\n",
        "        relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
        "        relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n",
        "        attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n",
        "\n",
        "    attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "    if attention_mask is not None:\n",
        "      # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
        "      attention_scores = attention_scores + attention_mask\n",
        "\n",
        "    # Normalize the attention scores to probabilities.\n",
        "    attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "\n",
        "    # This is actually dropping out entire tokens to attend to, which might\n",
        "    # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "    attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "    # Mask heads if we want to\n",
        "    if head_mask is not None:\n",
        "      attention_probs = attention_probs * head_mask\n",
        "\n",
        "    context_layer = torch.matmul(attention_probs, value_layer)\n",
        "\n",
        "    context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "    new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "    context_layer = context_layer.view(*new_context_layer_shape)\n",
        "\n",
        "    outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
        "    return outputs\n",
        "  \n",
        "\n",
        "class SelfOutput(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "    self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "    self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "  def forward(self, hidden_states, input_tensor):\n",
        "    hidden_states = self.dense(hidden_states)\n",
        "    hidden_states = self.dropout(hidden_states)\n",
        "    hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "    return hidden_states\n",
        "\n",
        "\n",
        "class AttentionModule(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.self = SelfAttention(config)\n",
        "    self.output = SelfOutput(config)\n",
        "    self.pruned_heads = set()\n",
        "\n",
        "  def prune_head(self, heads):\n",
        "    if len(heads) == 0:\n",
        "      return\n",
        "    heads, index = find_pruneable_heads_and_indices(                            \n",
        "      heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n",
        "    )\n",
        "    # Prune linear layers\n",
        "    self.self.query = prune_linear_layer(self.self.query, index)\n",
        "    self.self.key = prune_linear_layer(self.self.key, index)\n",
        "    self.self.value = prune_linear_layer(self.self.value, index)\n",
        "    self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n",
        "\n",
        "    # Update hyper params and store pruned heads\n",
        "    self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n",
        "    self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n",
        "    self.pruned_heads = self.pruned_heads.union(heads)\n",
        "\n",
        "  def forward(self,\n",
        "              hidden_states,\n",
        "              attention_mask=None,\n",
        "              head_mask=None,\n",
        "              encoder_hidden_states=None,\n",
        "              encoder_attention_mask=None,\n",
        "              output_attentions=False):\n",
        "    self_outputs = self.self(\n",
        "            hidden_states,\n",
        "            attention_mask,\n",
        "            head_mask,\n",
        "            encoder_hidden_states,\n",
        "            encoder_attention_mask,\n",
        "            output_attentions)\n",
        "    attention_output = self.output(self_outputs[0], hidden_states)\n",
        "    outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
        "    return outputs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "    if isinstance(config.hidden_act, str):\n",
        "      self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
        "    else:\n",
        "      self.intermediate_act_fn = config.hidden_act\n",
        "\n",
        "  def forward(self, hidden_states):\n",
        "    hidden_states = self.dense(hidden_states)\n",
        "    hidden_states = self.intermediate_act_fn(hidden_states)\n",
        "    return hidden_states\n",
        "\n",
        "\n",
        "class Output(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "    self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "    self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "  def forward(self, hidden_states, input_tensor):\n",
        "    hidden_states = self.dense(hidden_states)\n",
        "    hidden_states = self.dropout(hidden_states)\n",
        "    hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "    return hidden_states\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class EncoderLayer (nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
        "    self.seq_len_dim = 1\n",
        "    self.attention = AttentionModule(config)\n",
        "    self.is_decoder = config.is_decoder\n",
        "    self.add_cross_attention = config.add_cross_attention\n",
        "    if self.add_cross_attention:\n",
        "      assert self.is_decoder, f\"{self} should be used as a decoder model if cross attention is added\"\n",
        "      self.crossattention = AttentionModule(config)\n",
        "    self.intermediate = FeedForward(config)\n",
        "    self.output = Output(config)\n",
        "\n",
        "  def forward(self,\n",
        "              hidden_states,\n",
        "              attention_mask=None,\n",
        "              head_mask=None,\n",
        "              encoder_hidden_states=None,\n",
        "              encoder_attention_mask=None,\n",
        "              output_attentions=False):\n",
        "    \n",
        "    self_attention_outputs = self.attention(hidden_states,\n",
        "                                            attention_mask,\n",
        "                                            head_mask,\n",
        "                                            output_attentions=output_attentions)\n",
        "    attention_output = self_attention_outputs[0]\n",
        "    outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
        "    if self.is_decoder and encoder_hidden_states is not None:\n",
        "      assert hasattr(self, \"crossattention\"), f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers by setting `config.add_cross_attention=True`\"\n",
        "      cross_attention_outputs = self.crossattention(attention_output,\n",
        "                                                    attention_mask,\n",
        "                                                    head_mask,\n",
        "                                                    encoder_hidden_states,\n",
        "                                                    encoder_attention_mask,\n",
        "                                                    output_attentions)\n",
        "      attention_output = cross_attention_outputs[0]\n",
        "      outputs = outputs + cross_attention_outputs[1:]  # add cross attentions if we output attention weights\n",
        "\n",
        "    layer_output = apply_chunking_to_forward(self.feed_forward_chunk,           # don't forget this!!\n",
        "                                             self.chunk_size_feed_forward, \n",
        "                                             self.seq_len_dim, attention_output)\n",
        "      \n",
        "    outputs = (layer_output,) + outputs\n",
        "    return outputs\n",
        "\n",
        "  def feed_forward_chunk(self, attention_output):\n",
        "    intermediate_output = self.intermediate(attention_output)\n",
        "    layer_output = self.output(intermediate_output, attention_output)\n",
        "    return layer_output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class EncoderStack(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.layer = nn.ModuleList([EncoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "  def forward(self,\n",
        "              hidden_states,\n",
        "              attention_mask=None,\n",
        "              head_mask=None,\n",
        "              encoder_hidden_states=None,\n",
        "              encoder_attention_mask=None,\n",
        "              output_attentions=False,\n",
        "              output_hidden_states=False,\n",
        "              return_dict=True):\n",
        "    all_hidden_states = () if output_hidden_states else None\n",
        "    all_self_attentions = () if output_attentions else None\n",
        "    all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
        "    for i, layer_module in enumerate(self.layer):\n",
        "      if output_hidden_states:\n",
        "        all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "      \n",
        "      layer_head_mask = head_mask[i] if head_mask is not None else None\n",
        "\n",
        "      if getattr(self.config, \"gradient_checkpointing\", False):\n",
        "        def create_custom_forward(module):\n",
        "          def custom_forward(*inputs):\n",
        "            return module(*inputs, output_attentions)\n",
        "          return custom_forward\n",
        "        layer_outputs = torch.utils.checkpoint.checkpoint(create_custom_forward(layer_module),\n",
        "                                                          hidden_states,\n",
        "                                                          attention_mask,\n",
        "                                                          layer_head_mask,\n",
        "                                                          encoder_hidden_states,\n",
        "                                                          encoder_attention_mask,)\n",
        "      else:\n",
        "        layer_outputs = layer_module(hidden_states,\n",
        "                                     attention_mask,\n",
        "                                     layer_head_mask,\n",
        "                                     encoder_hidden_states,\n",
        "                                     encoder_attention_mask,\n",
        "                                     output_attentions,)\n",
        "      \n",
        "      hidden_states = layer_outputs[0]\n",
        "      if output_attentions:\n",
        "        all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
        "        if self.config.add_cross_attention:\n",
        "          all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
        "\n",
        "    if output_hidden_states:\n",
        "      all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "    if not return_dict:\n",
        "      return tuple(v\n",
        "                   for v in [hidden_states, all_hidden_states, all_self_attentions, all_cross_attentions]\n",
        "                   if v is not None)\n",
        "    \n",
        "    return BaseModelOutputWithCrossAttentions(last_hidden_state=hidden_states,\n",
        "                                              hidden_states=all_hidden_states,\n",
        "                                              attentions=all_self_attentions,\n",
        "                                              cross_attentions=all_cross_attentions,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7REE2kpgFBV"
      },
      "source": [
        "class Pooler(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "    self.activation = nn.Tanh()\n",
        "\n",
        "  def forward(self, hidden_states):\n",
        "    # We \"pool\" the model by simply taking the hidden state corresponding\n",
        "    # to the first token.\n",
        "    first_token_tensor = hidden_states[:, 0]\n",
        "    pooled_output = self.dense(first_token_tensor)\n",
        "    pooled_output = self.activation(pooled_output)\n",
        "    return pooled_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8MVxIXFoYEI"
      },
      "source": [
        "#### Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8AsEjOE9tOo"
      },
      "source": [
        "class BaseModelOutputWithPoolingAndCrossAttentions(ModelOutput):\r\n",
        "  \"\"\"\r\n",
        "  Base class for model's outputs that also contains a pooling of the last hidden states.\r\n",
        "  Args:\r\n",
        "    last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`):\r\n",
        "      Sequence of hidden-states at the output of the last layer of the model.\r\n",
        "    pooler_output (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, hidden_size)`):\r\n",
        "      Last layer hidden-state of the first token of the sequence (classification token) further processed by a\r\n",
        "      Linear layer and a Tanh activation function. The Linear layer weights are trained from the next sentence\r\n",
        "      prediction (classification) objective during pretraining.\r\n",
        "    hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\r\n",
        "      Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\r\n",
        "      of shape :obj:`(batch_size, sequence_length, hidden_size)`.\r\n",
        "      Hidden-states of the model at the output of each layer plus the initial embedding outputs.\r\n",
        "    attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\r\n",
        "      Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\r\n",
        "      sequence_length, sequence_length)`.\r\n",
        "      Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\r\n",
        "      heads.\r\n",
        "    cross_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` and ``config.add_cross_attention=True`` is passed or when ``config.output_attentions=True``):\r\n",
        "      Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\r\n",
        "      sequence_length, sequence_length)`.\r\n",
        "      Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\r\n",
        "      weighted average in the cross-attention heads.\r\n",
        "  \"\"\"\r\n",
        "  last_hidden_state: torch.FloatTensor = None\r\n",
        "  pooler_output: torch.FloatTensor = None\r\n",
        "  hidden_states: Optional[Tuple[torch.FloatTensor]] = None\r\n",
        "  attentions: Optional[Tuple[torch.FloatTensor]] = None\r\n",
        "  cross_attentions: Optional[Tuple[torch.FloatTensor]] = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXxk31lYWr0Y"
      },
      "source": [
        "class TransformerPreTrainedModel(PreTrainedModel):\r\n",
        "  \"\"\"\r\n",
        "  An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\r\n",
        "  models.\r\n",
        "  \"\"\"\r\n",
        "  config_class = BertConfig\r\n",
        "  base_model_prefix = \"bert\"\r\n",
        "  _keys_to_ignore_on_load_missing = [r\"position_ids\"]\r\n",
        "\r\n",
        "  def _init_weights(self, module):\r\n",
        "    \"\"\" Initialize the weights \"\"\"\r\n",
        "    if isinstance(module, (nn.Linear, nn.Embedding)):\r\n",
        "      module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\r\n",
        "    elif isinstance(module, nn.LayerNorm):\r\n",
        "      module.bias.data.zero_()\r\n",
        "      module.weight.data.fill_(1.0)\r\n",
        "    if isinstance(module, nn.Linear) and module.bias is not None:\r\n",
        "      module.bias.data.zero_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lcr5I4EZcWdu"
      },
      "source": [
        "class TransformerConfig(PretrainedConfig):\r\n",
        "    \"\"\"\r\n",
        "    This is the configuration class to store the configuration of a TransformerModel. \r\n",
        "    It is used to instantiate a BERT model according to the specified arguments,\r\n",
        "    defining the model architecture. Instantiating a configuration with the defaults \r\n",
        "    will yield a similar configuration to that of the BERT `bert-base-uncased \r\n",
        "    <https://huggingface.co/bert-base-uncased>`__ architecture. Configuration objects \r\n",
        "    inherit from :class:`~transformers.PretrainedConfig` and can be used to control \r\n",
        "    the model outputs. Read the documentation from :class:`~transformers.PretrainedConfig` \r\n",
        "    for more information.\r\n",
        "    Args:\r\n",
        "        vocab_size (:obj:`int`, `optional`, defaults to 30522):\r\n",
        "            Vocabulary size of the BERT model. Defines the number of different tokens that can be represented by the\r\n",
        "            :obj:`inputs_ids` passed when calling :class:`~transformers.BertModel` or\r\n",
        "            :class:`~transformers.TFBertModel`.\r\n",
        "        hidden_size (:obj:`int`, `optional`, defaults to 768):\r\n",
        "            Dimensionality of the encoder layers and the pooler layer.\r\n",
        "        num_hidden_layers (:obj:`int`, `optional`, defaults to 12):\r\n",
        "            Number of hidden layers in the Transformer encoder.\r\n",
        "        num_attention_heads (:obj:`int`, `optional`, defaults to 12):\r\n",
        "            Number of attention heads for each attention layer in the Transformer encoder.\r\n",
        "        intermediate_size (:obj:`int`, `optional`, defaults to 3072):\r\n",
        "            Dimensionality of the \"intermediate\" (often named feed-forward) layer in the Transformer encoder.\r\n",
        "        hidden_act (:obj:`str` or :obj:`Callable`, `optional`, defaults to :obj:`\"gelu\"`):\r\n",
        "            The non-linear activation function (function or string) in the encoder and pooler. If string,\r\n",
        "            :obj:`\"gelu\"`, :obj:`\"relu\"`, :obj:`\"silu\"` and :obj:`\"gelu_new\"` are supported.\r\n",
        "        hidden_dropout_prob (:obj:`float`, `optional`, defaults to 0.1):\r\n",
        "            The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\r\n",
        "        attention_probs_dropout_prob (:obj:`float`, `optional`, defaults to 0.1):\r\n",
        "            The dropout ratio for the attention probabilities.\r\n",
        "        max_position_embeddings (:obj:`int`, `optional`, defaults to 512):\r\n",
        "            The maximum sequence length that this model might ever be used with. Typically set this to something large\r\n",
        "            just in case (e.g., 512 or 1024 or 2048).\r\n",
        "        type_vocab_size (:obj:`int`, `optional`, defaults to 2):\r\n",
        "            The vocabulary size of the :obj:`token_type_ids` passed when calling :class:`~transformers.BertModel` or\r\n",
        "            :class:`~transformers.TFBertModel`.\r\n",
        "        initializer_range (:obj:`float`, `optional`, defaults to 0.02):\r\n",
        "            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\r\n",
        "        layer_norm_eps (:obj:`float`, `optional`, defaults to 1e-12):\r\n",
        "            The epsilon used by the layer normalization layers.\r\n",
        "        gradient_checkpointing (:obj:`bool`, `optional`, defaults to :obj:`False`):\r\n",
        "            If True, use gradient checkpointing to save memory at the expense of slower backward pass.\r\n",
        "        position_embedding_type (:obj:`str`, `optional`, defaults to :obj:`\"absolute\"`):\r\n",
        "            Type of position embedding. Choose one of :obj:`\"absolute\"`, :obj:`\"relative_key\"`,\r\n",
        "            :obj:`\"relative_key_query\"`. For positional embeddings use :obj:`\"absolute\"`. For more information on\r\n",
        "            :obj:`\"relative_key\"`, please refer to `Self-Attention with Relative Position Representations (Shaw et al.)\r\n",
        "            <https://arxiv.org/abs/1803.02155>`__. For more information on :obj:`\"relative_key_query\"`, please refer to\r\n",
        "            `Method 4` in `Improve Transformer Models with Better Relative Position Embeddings (Huang et al.)\r\n",
        "            <https://arxiv.org/abs/2009.13658>`__.\r\n",
        "    Examples::\r\n",
        "        >>> from transformers import BertModel, BertConfig\r\n",
        "        >>> # Initializing a BERT bert-base-uncased style configuration\r\n",
        "        >>> configuration = BertConfig()\r\n",
        "        >>> # Initializing a model from the bert-base-uncased style configuration\r\n",
        "        >>> model = BertModel(configuration)\r\n",
        "        >>> # Accessing the model configuration\r\n",
        "        >>> configuration = model.config\r\n",
        "    \"\"\"\r\n",
        "    model_type = \"bert\"\r\n",
        "\r\n",
        "    def __init__(\r\n",
        "        self,\r\n",
        "        vocab_size=30522,\r\n",
        "        hidden_size=768,\r\n",
        "        num_hidden_layers=12,\r\n",
        "        num_attention_heads=12,\r\n",
        "        intermediate_size=3072,\r\n",
        "        hidden_act=\"gelu\",\r\n",
        "        hidden_dropout_prob=0.1,\r\n",
        "        attention_probs_dropout_prob=0.1,\r\n",
        "        max_position_embeddings=512,\r\n",
        "        type_vocab_size=2,\r\n",
        "        initializer_range=0.02,\r\n",
        "        layer_norm_eps=1e-12,\r\n",
        "        pad_token_id=0,\r\n",
        "        gradient_checkpointing=False,\r\n",
        "        position_embedding_type=\"absolute\",\r\n",
        "        **kwargs\r\n",
        "    ):\r\n",
        "        super().__init__(pad_token_id=pad_token_id, **kwargs)\r\n",
        "\r\n",
        "        self.vocab_size = vocab_size\r\n",
        "        self.hidden_size = hidden_size\r\n",
        "        self.num_hidden_layers = num_hidden_layers\r\n",
        "        self.num_attention_heads = num_attention_heads\r\n",
        "        self.hidden_act = hidden_act\r\n",
        "        self.intermediate_size = intermediate_size\r\n",
        "        self.hidden_dropout_prob = hidden_dropout_prob\r\n",
        "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\r\n",
        "        self.max_position_embeddings = max_position_embeddings\r\n",
        "        self.type_vocab_size = type_vocab_size\r\n",
        "        self.initializer_range = initializer_range\r\n",
        "        self.layer_norm_eps = layer_norm_eps\r\n",
        "        self.gradient_checkpointing = gradient_checkpointing\r\n",
        "        self.position_embedding_type = position_embedding_type"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6A_DB9Tf1aP4"
      },
      "source": [
        "class TransformerBase(BertPreTrainedModel):\n",
        "  \"\"\"\n",
        "  The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n",
        "  cross-attention is added between the self-attention layers, following the architecture described in `Attention is\n",
        "  all you need <https://arxiv.org/abs/1706.03762>`__ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\n",
        "  Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.\n",
        "  To behave as an decoder the model needs to be initialized with the :obj:`is_decoder` argument of the configuration\n",
        "  set to :obj:`True`. To be used in a Seq2Seq model, the model needs to initialized with both :obj:`is_decoder`\n",
        "  argument and :obj:`add_cross_attention` set to :obj:`True`; an :obj:`encoder_hidden_states` is then expected as an\n",
        "  input to the forward pass.\n",
        "  \"\"\"\n",
        "  def __init__(self, config, add_pooling_layer=True):\n",
        "    super().__init__(config)\n",
        "    self.config = config\n",
        "\n",
        "    self.embeddings = EmbeddingsLookup(config)\n",
        "    self.encoder = EncoderStack(config)\n",
        "\n",
        "    self.pooler = Pooler(config) if add_pooling_layer else None\n",
        "\n",
        "    self.init_weights() # Don't forget this\n",
        "\n",
        "  def get_input_embeddings(self):\n",
        "    return self.embeddings.word_embeddings\n",
        "\n",
        "  def set_input_embeddings(self, value):\n",
        "    self.embeddings.word_embeddings = value\n",
        "\n",
        "  def _prune_heads(self, heads_to_prune):\n",
        "    \"\"\"\n",
        "    Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n",
        "    class PreTrainedModel\n",
        "    \"\"\"\n",
        "    for layer, heads in heads_to_prune.items():\n",
        "      self.encoder.layer[layer].attention.prune_heads(heads)\n",
        "\n",
        "  def forward(self,\n",
        "              input_ids=None,\n",
        "              attention_mask=None,\n",
        "              token_type_ids=None,\n",
        "              position_ids=None,\n",
        "              head_mask=None,\n",
        "              inputs_embeds=None,\n",
        "              encoder_hidden_states=None,\n",
        "              encoder_attention_mask=None,\n",
        "              output_attentions=None,\n",
        "              output_hidden_states=None,\n",
        "              return_dict=None,):\n",
        "    \"\"\"\n",
        "    encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
        "      Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
        "      the model is configured as a decoder.\n",
        "    encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "      Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
        "      the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n",
        "      - 1 for tokens that are **not masked**,\n",
        "      - 0 for tokens that are **masked**.\n",
        "    \"\"\"\n",
        "    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "    output_hidden_states = output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "    if input_ids is not None and inputs_embeds is not None:\n",
        "      raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
        "    elif input_ids is not None:\n",
        "      input_shape = input_ids.size()\n",
        "    elif inputs_embeds is not None:\n",
        "      input_shape = inputs_embeds.size()[:-1]\n",
        "    else:\n",
        "      raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
        "\n",
        "    device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
        "\n",
        "    if attention_mask is None:\n",
        "      attention_mask = torch.ones(input_shape, device=device)\n",
        "    if token_type_ids is None:\n",
        "      token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
        "\n",
        "    # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
        "    # ourselves in which case we just need to make it broadcastable to all heads.\n",
        "    extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device) # can we throw this away?\n",
        "\n",
        "    # If a 2D or 3D attention mask is provided for the cross-attention\n",
        "    # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
        "    if self.config.is_decoder and encoder_hidden_states is not None:\n",
        "      encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
        "      encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
        "      if encoder_attention_mask is None:\n",
        "        encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
        "      encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
        "    else:\n",
        "      encoder_extended_attention_mask = None\n",
        "\n",
        "    # Prepare head mask if needed\n",
        "    # 1.0 in head_mask indicate we keep the head\n",
        "    # attention_probs has shape bsz x n_heads x N x N\n",
        "    # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
        "    # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
        "    head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
        "\n",
        "    embedding_output = self.embeddings(input_ids=input_ids,\n",
        "                                       position_ids=position_ids,\n",
        "                                       token_type_ids=token_type_ids,\n",
        "                                       inputs_embeds=inputs_embeds)\n",
        "    encoder_outputs = self.encoder(embedding_output,\n",
        "                                   attention_mask=extended_attention_mask,\n",
        "                                   head_mask=head_mask,\n",
        "                                   encoder_hidden_states=encoder_hidden_states,\n",
        "                                   encoder_attention_mask=encoder_extended_attention_mask,\n",
        "                                   output_attentions=output_attentions,\n",
        "                                   output_hidden_states=output_hidden_states,\n",
        "                                   return_dict=return_dict,)\n",
        "    sequence_output = encoder_outputs[0]\n",
        "    pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
        "\n",
        "    if not return_dict:\n",
        "      return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
        "\n",
        "    return BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=sequence_output,\n",
        "                                                        pooler_output=pooled_output,\n",
        "                                                        hidden_states=encoder_outputs.hidden_states,\n",
        "                                                        attentions=encoder_outputs.attentions,\n",
        "                                                        cross_attentions=encoder_outputs.cross_attentions,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYWwncTdp0sg"
      },
      "source": [
        "## 4.2 Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qgh7LvX2RWME"
      },
      "source": [
        "### 4.2.1 Input Masking Algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmJT1yF3i9k8"
      },
      "source": [
        "#### Token Masking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3pECwdHBae9"
      },
      "source": [
        "# word_mask_probability = 0.15\n",
        "# replace_with_mask_probability = 0.8\n",
        "# replace_randomly_probability = 0.1\n",
        "# keep_token_probability = 0.1\n",
        "\n",
        "def mask_input_ids(inputs: torch.tensor,\n",
        "                   #tokenizer: transformers.BertTokenizerFast,\n",
        "                   special_tokens_mask: Optional[torch.Tensor] = None,\n",
        "                   word_mask_probability = 0.15,\n",
        "                   replace_with_mask_probability = 0.8,\n",
        "                   replace_randomly_probability = 0.1,\n",
        "                   keep_token_probability = 0.1\n",
        "                   ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "  \"\"\"\n",
        "  We specifiy the probability with which to mask token for the language modeling\n",
        "  task. Generally 15% of tokens are considered for masking. If we just mask \n",
        "  naively then a problem arises: some masked token will never actually have been \n",
        "  seen at fine-tuning. The solution is to not replace the token with [MASK] 100%\n",
        "  of the time, instead:\n",
        "  - 80% of the time, replace the token with [MASK]\n",
        "    went to the store -> went to the [MASK]\n",
        "  - 10% of the time, replace random token\n",
        "    went to the store -> went to the running\n",
        "  - 10% of the time, keep same\n",
        "    went to the store -> went to the store\n",
        "  The same principle is also appilicable with masked sentence prediction, only\n",
        "  that we have to establish a sentence vocabulary beforehand\n",
        "\n",
        "  Args:\n",
        "    inputs: tensor, containing all the token IDs\n",
        "    special_tokens_mask: tensor, denotes whether a token is a word [0] or a \n",
        "      special token [1], [CLS] tokens and padding tokens are all counted as \n",
        "      special tokens. This will be used to create a mask so that only actual\n",
        "      words are considered for random masking\n",
        "\n",
        "  Returns:\n",
        "    masked_inputs:\n",
        "    labels:\n",
        "  \"\"\"\n",
        "  labels = inputs.clone()\n",
        "  # Tensor that hold the probability values for the Bernoulli function\n",
        "  probability_matrix = torch.full(inputs.shape, word_mask_probability)\n",
        "\n",
        "  tokenizer = transformers.BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "\n",
        "  # Get special token indices in order to exclude special tokens from masking\n",
        "  if special_tokens_mask is None:\n",
        "    special_tokens_mask = [\n",
        "      tokenizer.get_special_tokens_mask(entry, already_has_special_tokens=True) for entry in labels.tolist()\n",
        "    ]\n",
        "    special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
        "  else:\n",
        "    special_tokens_mask = special_tokens_mask.bool()\n",
        "\n",
        "  # Fill the probability matrix with 0.0 values where there are special tokens\n",
        "  probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
        "  # Draws from a bernoulli distribution where probability_matrix holds the \n",
        "  # probablitites for drawing the binary random number. The probablity matrix\n",
        "  # was previously filled with 0.0 values where special tokens are present so\n",
        "  # that only tokens containing words/sentences are considered\n",
        "  masked_indices = torch.bernoulli(probability_matrix).bool()\n",
        "  # In order to compute the loss only on the masked indices all the unmasked\n",
        "  # tokens in the label tensor are set to -100\n",
        "  labels[~masked_indices] = -100\n",
        "\n",
        "  # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
        "  indices_replaced = torch.bernoulli(torch.full(labels.shape, replace_with_mask_probability)).bool() & masked_indices\n",
        "  # Since we're dealing with tensors with numerical values we convert the [MASK]\n",
        "  # token right back to its token_id representation\n",
        "  inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
        "\n",
        "  # 10% of the time, we replace masked input tokens with random word\n",
        "  indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
        "  random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n",
        "  inputs[indices_random] = random_words[indices_random]\n",
        "\n",
        "  return (inputs, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvdSZfthl-QN"
      },
      "source": [
        "#### Embedding Masking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FELb70M2Ibnf"
      },
      "source": [
        "def mask_input_embeddings(input_embeddings: torch.tensor,\n",
        "                          special_embeddings_mask: torch.tensor,\n",
        "                          sentence_mask_probability = 0.15):\n",
        "  \"\"\"\n",
        "  Randomly masks sentences with a probability of 15%. The masked sentence\n",
        "  embeddings are replaced with a random tensor and the original embedding will\n",
        "  be stored in a labels tensor that has the same size as the input tensor. The\n",
        "  ground truth embedding will sit at the same position as is did in the input\n",
        "  tensor to make it easier to identify the correct ground truth for loss\n",
        "  computing.\n",
        "\n",
        "  Args:\n",
        "    input_embeddings: A torch.tensor containing all sentence embeddings computed\n",
        "      by the Sentence Model for a given batch. The size of the tensor is\n",
        "      [batch_size, max_doc_length, embedding_size]. Note that the documents are\n",
        "      already padded to the length of the longest document in the batch.\n",
        "    special_embeddings_mask: A torch.tensor of the same size as input_embeddings\n",
        "      [batch_size, max_doc_length] which hold 0s where there is a real sentence \n",
        "      present and 1s where there is a special token embedding, that includes \n",
        "      CLS, SEP and PAD tokens.\n",
        "  Returns:\n",
        "    masked_input_embeddings: Same shape as input embeddings, only that it holds\n",
        "      a random tensor wherever a sentence embedding was masked.\n",
        "    label_embeddings: Same shape as the masked_input_embeddings but all entries \n",
        "      are filled with 0s except where there is a masked sentence embedding. That\n",
        "      entry will be filled with the original input embedding.\n",
        "    label_mask: torch.BoolTensor\n",
        "  \"\"\"\n",
        "  masked_input_embeddings = input_embeddings.clone()\n",
        "  label_embeddings = torch.zeros_like(input_embeddings)\n",
        "  label_mask = torch.zeros_like(special_embeddings_mask)\n",
        "\n",
        "  probability_matrix = torch.full(special_embeddings_mask.shape, sentence_mask_probability)\n",
        "\n",
        "  probability_matrix.masked_fill_(special_embeddings_mask, value=0.0)\n",
        "\n",
        "  masked_indices = torch.bernoulli(probability_matrix).bool()\n",
        "\n",
        "  document_counter = 0\n",
        "  sentence_counter = 0\n",
        "\n",
        "  for document in input_embeddings:\n",
        "    sentence_counter = 0\n",
        "    for sentence in document:\n",
        "      if masked_indices[document_counter][sentence_counter]:\n",
        "        label_embeddings[document_counter][sentence_counter] = input_embeddings[document_counter][sentence_counter]\n",
        "        label_mask[document_counter][sentence_counter] = 1.0\n",
        "        masked_input_embeddings[document_counter][sentence_counter] = torch.randn_like(input_embeddings[document_counter][sentence_counter])\n",
        "      sentence_counter += 1\n",
        "    document_counter += 1\n",
        "\n",
        "  label_embeddings[~masked_indices] = 0\n",
        "  label_mask = torch.Tensor.bool(label_mask)\n",
        "\n",
        "  return (input_embeddings, masked_input_embeddings, label_embeddings, label_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PG4skgOqPbxd"
      },
      "source": [
        "- Algorithm for masking sentences\n",
        "- Extracting the masked token embedding from the DocumentEncoder\n",
        "- Compare vs ground truth and define loss function on it (use huggingface optimizer??)\n",
        "from transformers import AdamW #example\n",
        "optimizer = AdamW(...)\n",
        "https://huggingface.co/transformers/training.html\n",
        "- SMITH adds loss from sentence encoder + document encoder, make both trainable simultaneously???"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFQ_HrliRfml"
      },
      "source": [
        "### 4.2.2 Language Modeling Head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zC2Kn-tug0bk"
      },
      "source": [
        "Define the LM Head(s) and its loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ue_TauSD2gWp"
      },
      "source": [
        "#### Word Level Language Modeling Head"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFclI3Hu210g"
      },
      "source": [
        "class PredictionHeadTransform(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "    if isinstance(config.hidden_act, str):\n",
        "      self.transform_act_fn = ACT2FN[config.hidden_act]\n",
        "    else:\n",
        "      self.transform_act_fn = config.hidden_act\n",
        "    self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "\n",
        "  def forward(self, hidden_states):\n",
        "    hidden_states = self.dense(hidden_states)\n",
        "    hidden_states = self.transform_act_fn(hidden_states)\n",
        "    hidden_states = self.LayerNorm(hidden_states)\n",
        "    return hidden_states\n",
        "\n",
        "\n",
        "\n",
        "class LMPredictionHead(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.transform = PredictionHeadTransform(config)\n",
        "\n",
        "    # The output weights are the same as the input embeddings, but there is\n",
        "    # an output-only bias for each token.\n",
        "    self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
        "\n",
        "    self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n",
        "\n",
        "    # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`\n",
        "    self.decoder.bias = self.bias\n",
        "\n",
        "  def forward(self, hidden_states):\n",
        "    hidden_states = self.transform(hidden_states)\n",
        "    hidden_states = self.decoder(hidden_states)\n",
        "    return hidden_states\n",
        "\n",
        "\n",
        "\n",
        "class OnlyMLMHead(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.predictions = LMPredictionHead(config)\n",
        "\n",
        "  def forward(self, sequence_output):\n",
        "    prediction_scores = self.predictions(sequence_output)\n",
        "    return prediction_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17qoRvtU2oEL"
      },
      "source": [
        "#### Sentence Level Language Modeling Head"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wstkTR6hOicR"
      },
      "source": [
        "class SentencePredictionHeadOutput(ModelOutput):\r\n",
        "  logits = None\r\n",
        "  probabilites = None\r\n",
        "  log_probabilities = None\r\n",
        "  labels_one_hot = None\r\n",
        "  per_example_loss_distance=None\r\n",
        "  per_example_loss_product=None\r\n",
        "  loss_distance=None\r\n",
        "  loss_product=None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XiNeDnD22bV"
      },
      "source": [
        "class SentencePredictionHead(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.dense = nn.Linear(in_features=config.hidden_size, out_features=config.hidden_size)\n",
        "    self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps) # Compare to the word-level LM Head\n",
        "    self.config = config\n",
        "\n",
        "  def forward(self, masked_sentence_prediction, label_embeddings, label_mask):\n",
        "    \"\"\"\n",
        "    In order to compute the sentence-level prediction loss we apply a similar\n",
        "    loss function as during the word-level masked word prediction tast. Since\n",
        "    we don't have a fixed size vocabulary over the training sentences we have\n",
        "    to build a dynamic sentence vocabulary.\n",
        "    Args:\n",
        "      masked_sentence_prediction [batch_size, max_doc_length, hidden_size]:\n",
        "      label_embeddings [batch_size, max_doc_length, hidden_size]:\n",
        "      label_mask [batch_size, max_doc_length, hidden_size]:\n",
        "    Returns:\n",
        "      per_batch_sentence_loss:\n",
        "      per_example_sentence_loss:\n",
        "    \"\"\"\n",
        "    label_mask = torch.Tensor.bool(label_mask)\n",
        "    predictions = masked_sentence_prediction.clone()\n",
        "    # Zero out all sentence embeddings that aren't at a masked position\n",
        "    predictions[~label_mask] = 0.0\n",
        "    label_embeddings[~label_mask] = 0.0\n",
        "    \n",
        "    # Tensors will have size [batch_size * padded_doc_length, hidden_size]\n",
        "    predictions = torch.reshape(predictions, (predictions.size()[0] * predictions.size()[1], -1)) \n",
        "    label_embeddings = torch.reshape(label_embeddings, (label_embeddings.size()[0] * label_embeddings.size()[1], -1))\n",
        "    label_mask = torch.reshape(label_mask, (label_mask.size()[0] * label_mask.size()[1], -1))\n",
        "\n",
        "    output_embedding_list = []\n",
        "    label_embedding_list = []\n",
        "    label_mask_index =  0\n",
        "\n",
        "    for mask_index in label_mask:\n",
        "      if mask_index.item():\n",
        "        output_embedding_list.append(predictions[label_mask_index])\n",
        "        label_embedding_list.append(label_embeddings[label_mask_index])\n",
        "      label_mask_index += 1\n",
        "\n",
        "    output_embeddings = torch.stack(output_embedding_list, dim=0)\n",
        "    label_embeddings = torch.stack(label_embedding_list, dim=0)\n",
        "\n",
        "    output_embeddings = self.dense(output_embeddings)\n",
        "    output_embeddings = self.LayerNorm(output_embeddings)\n",
        "\n",
        "    # TODO add bias like in SMITH? (smith/layers.py)\n",
        "\n",
        "    logits = torch.matmul(output_embeddings, torch.transpose(input=label_embeddings, dim0=0, dim1=1))\n",
        "    probabilities = f.softmax(logits, dim=1)\n",
        "    log_probabilities = f.log_softmax(logits, dim=1)\n",
        "    labels_one_hot = torch.diag(torch.Tensor([1] * log_probabilities.size()[0]))\n",
        "\n",
        "    # Computes the pairwise distance between each row of the inputs, meaning that\n",
        "    # it computes the elementwise difference between probabilities and labels and\n",
        "    # sums them up per one prediction (row).\n",
        "    per_example_loss_distance = f.pairwise_distance(probabilities, labels_one_hot, p=1.0, keepdim=False)\n",
        "    # Another option is to compute the pairwise product per element of the log_probs\n",
        "    # and the one hot labels and sum them per prediction (romw). This emphasizes\n",
        "    # very bad predictions less and keeps the loss smaller.\n",
        "    per_example_loss_product = -torch.sum(log_probabilities * labels_one_hot, 1)\n",
        "\n",
        "    #Shape: [1]\n",
        "    numerator_distance = torch.sum(per_example_loss_distance)\n",
        "    numerator_product = torch.sum(per_example_loss_product)\n",
        "\n",
        "    # Shape: [1], small fraction added so we never divide by 0\n",
        "    denominator =  labels_one_hot.size()[0] + 1e-5\n",
        "\n",
        "    # Shape: [1]\n",
        "    loss_distance = numerator_distance / denominator\n",
        "    loss_product = numerator_product / denominator\n",
        "\n",
        "\n",
        "    return SentencePredictionHeadOutput(logits=logits,\n",
        "                                        probabilites=probabilities,\n",
        "                                        log_probabilities=log_probabilities,\n",
        "                                        labels_one_hot=labels_one_hot,\n",
        "                                        per_example_loss_distance=per_example_loss_distance,\n",
        "                                        per_example_loss_product=per_example_loss_product,\n",
        "                                        loss_distance=loss_distance,\n",
        "                                        loss_product=loss_product)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGoLVfY2i1P8"
      },
      "source": [
        "### 4.2.3 Alternative: Learning via Decoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uWqM6Avi_0j"
      },
      "source": [
        "Write a Decoder like in Hibert to have an alternative to the LM Head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyPDLz_kN7uE"
      },
      "source": [
        "## 4.3 Sentence Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pq3B99GAzqT3"
      },
      "source": [
        "#### Sentence Model Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgNBK8LN2aho"
      },
      "source": [
        "class SentenceModelingOutput(ModelOutput): #inherits from the huggingface class\r\n",
        "  \"\"\"\r\n",
        "    Return object for Sentence Model.\r\n",
        "\r\n",
        "    Args:\r\n",
        "      loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`labels` is provided):\r\n",
        "        Masked language modeling (MLM) loss.\r\n",
        "      logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, config.vocab_size)`):\r\n",
        "        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\r\n",
        "      hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\r\n",
        "        Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\r\n",
        "        of shape :obj:`(batch_size, sequence_length, hidden_size)`.\r\n",
        "        Hidden-states of the model at the output of each layer plus the initial embedding outputs.\r\n",
        "      last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`):\r\n",
        "        Sequence of hidden-states at the output of the last layer of the model.\r\n",
        "      attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\r\n",
        "        Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\r\n",
        "        sequence_length, sequence_length)`.\r\n",
        "        Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\r\n",
        "        heads.\r\n",
        "  \"\"\"\r\n",
        "  loss: Optional[torch.FloatTensor] = None\r\n",
        "  logits: torch.FloatTensor = None\r\n",
        "  hidden_states: Optional[Tuple[torch.FloatTensor]] = None\r\n",
        "  last_hidden_state: torch.FloatTensor = None\r\n",
        "  attentions: Optional[Tuple[torch.FloatTensor]] = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ika4rqTyztVJ"
      },
      "source": [
        "#### Sentence Model Confuguration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wgmalynCeMr"
      },
      "source": [
        "class HATESentenceModelConfig(PretrainedConfig):\r\n",
        "    \"\"\"\r\n",
        "    This is the configuration class to store the configuration of a TransformerModel. \r\n",
        "    It is used to instantiate a BERT model according to the specified arguments,\r\n",
        "    defining the model architecture. Instantiating a configuration with the defaults \r\n",
        "    will yield a similar configuration to that of the BERT `bert-base-uncased \r\n",
        "    <https://huggingface.co/bert-base-uncased>`__ architecture. Configuration objects \r\n",
        "    inherit from :class:`~transformers.PretrainedConfig` and can be used to control \r\n",
        "    the model outputs. Read the documentation from :class:`~transformers.PretrainedConfig` \r\n",
        "    for more information.\r\n",
        "    Args:\r\n",
        "        vocab_size (:obj:`int`, `optional`, defaults to 30522):\r\n",
        "            Vocabulary size of the BERT model. Defines the number of different tokens that can be represented by the\r\n",
        "            :obj:`inputs_ids` passed when calling :class:`~transformers.BertModel` or\r\n",
        "            :class:`~transformers.TFBertModel`.\r\n",
        "        hidden_size (:obj:`int`, `optional`, defaults to 768):\r\n",
        "            Dimensionality of the encoder layers and the pooler layer.\r\n",
        "        num_hidden_layers (:obj:`int`, `optional`, defaults to 12):\r\n",
        "            Number of hidden layers in the Transformer encoder.\r\n",
        "        num_attention_heads (:obj:`int`, `optional`, defaults to 12):\r\n",
        "            Number of attention heads for each attention layer in the Transformer encoder.\r\n",
        "        intermediate_size (:obj:`int`, `optional`, defaults to 3072):\r\n",
        "            Dimensionality of the \"intermediate\" (often named feed-forward) layer in the Transformer encoder.\r\n",
        "        hidden_act (:obj:`str` or :obj:`Callable`, `optional`, defaults to :obj:`\"gelu\"`):\r\n",
        "            The non-linear activation function (function or string) in the encoder and pooler. If string,\r\n",
        "            :obj:`\"gelu\"`, :obj:`\"relu\"`, :obj:`\"silu\"` and :obj:`\"gelu_new\"` are supported.\r\n",
        "        hidden_dropout_prob (:obj:`float`, `optional`, defaults to 0.1):\r\n",
        "            The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\r\n",
        "        attention_probs_dropout_prob (:obj:`float`, `optional`, defaults to 0.1):\r\n",
        "            The dropout ratio for the attention probabilities.\r\n",
        "        max_position_embeddings (:obj:`int`, `optional`, defaults to 512):\r\n",
        "            The maximum sequence length that this model might ever be used with. Typically set this to something large\r\n",
        "            just in case (e.g., 512 or 1024 or 2048).\r\n",
        "        type_vocab_size (:obj:`int`, `optional`, defaults to 2):\r\n",
        "            The vocabulary size of the :obj:`token_type_ids` passed when calling :class:`~transformers.BertModel` or\r\n",
        "            :class:`~transformers.TFBertModel`.\r\n",
        "        initializer_range (:obj:`float`, `optional`, defaults to 0.02):\r\n",
        "            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\r\n",
        "        layer_norm_eps (:obj:`float`, `optional`, defaults to 1e-12):\r\n",
        "            The epsilon used by the layer normalization layers.\r\n",
        "        gradient_checkpointing (:obj:`bool`, `optional`, defaults to :obj:`False`):\r\n",
        "            If True, use gradient checkpointing to save memory at the expense of slower backward pass.\r\n",
        "        position_embedding_type (:obj:`str`, `optional`, defaults to :obj:`\"absolute\"`):\r\n",
        "            Type of position embedding. Choose one of :obj:`\"absolute\"`, :obj:`\"relative_key\"`,\r\n",
        "            :obj:`\"relative_key_query\"`. For positional embeddings use :obj:`\"absolute\"`. For more information on\r\n",
        "            :obj:`\"relative_key\"`, please refer to `Self-Attention with Relative Position Representations (Shaw et al.)\r\n",
        "            <https://arxiv.org/abs/1803.02155>`__. For more information on :obj:`\"relative_key_query\"`, please refer to\r\n",
        "            `Method 4` in `Improve Transformer Models with Better Relative Position Embeddings (Huang et al.)\r\n",
        "            <https://arxiv.org/abs/2009.13658>`__.\r\n",
        "    Examples::\r\n",
        "        >>> from transformers import BertModel, BertConfig\r\n",
        "        >>> # Initializing a BERT bert-base-uncased style configuration\r\n",
        "        >>> configuration = BertConfig()\r\n",
        "        >>> # Initializing a model from the bert-base-uncased style configuration\r\n",
        "        >>> model = BertModel(configuration)\r\n",
        "        >>> # Accessing the model configuration\r\n",
        "        >>> configuration = model.config\r\n",
        "    \"\"\"\r\n",
        "    model_type = \"bert\"\r\n",
        "\r\n",
        "    def __init__(\r\n",
        "        self,\r\n",
        "        vocab_size=30522,\r\n",
        "        hidden_size=768,\r\n",
        "        num_hidden_layers=12,\r\n",
        "        num_attention_heads=12,\r\n",
        "        intermediate_size=3072,\r\n",
        "        hidden_act=\"gelu\",\r\n",
        "        hidden_dropout_prob=0.1,\r\n",
        "        attention_probs_dropout_prob=0.1,\r\n",
        "        max_position_embeddings=512,\r\n",
        "        type_vocab_size=2,\r\n",
        "        initializer_range=0.02,\r\n",
        "        layer_norm_eps=1e-12,\r\n",
        "        pad_token_id=0,\r\n",
        "        gradient_checkpointing=False,\r\n",
        "        position_embedding_type=\"absolute\",\r\n",
        "        **kwargs\r\n",
        "    ):\r\n",
        "        super().__init__(pad_token_id=pad_token_id, **kwargs)\r\n",
        "\r\n",
        "        self.vocab_size = vocab_size\r\n",
        "        self.hidden_size = hidden_size\r\n",
        "        self.num_hidden_layers = num_hidden_layers\r\n",
        "        self.num_attention_heads = num_attention_heads\r\n",
        "        self.hidden_act = hidden_act\r\n",
        "        self.intermediate_size = intermediate_size\r\n",
        "        self.hidden_dropout_prob = hidden_dropout_prob\r\n",
        "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\r\n",
        "        self.max_position_embeddings = max_position_embeddings\r\n",
        "        self.type_vocab_size = type_vocab_size\r\n",
        "        self.initializer_range = initializer_range\r\n",
        "        self.layer_norm_eps = layer_norm_eps\r\n",
        "        self.gradient_checkpointing = gradient_checkpointing\r\n",
        "        self.position_embedding_type = position_embedding_type"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPOCt2ECzw9Q"
      },
      "source": [
        "#### Sentence Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1QYoqpgRlef"
      },
      "source": [
        "class HATESentenceModel(TransformerPreTrainedModel):\n",
        "  \n",
        "  _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
        "  _keys_to_ignore_on_load_missing = [r\"position_ids\", r\"predictions.decoder.bias\"]\n",
        "\n",
        "  \n",
        "  def __init__(self, config):\n",
        "    super().__init__(config)\n",
        "\n",
        "    if config.is_decoder:\n",
        "      logger.warning(\n",
        "        \"If you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for \"\n",
        "        \"bi-directional self-attention.\"\n",
        "        )\n",
        "\n",
        "    self.transformer = TransformerBase(config, add_pooling_layer=False)\n",
        "    self.cls = OnlyMLMHead(config)\n",
        "\n",
        "    #self.init_weights()\n",
        "\n",
        "  def get_output_embeddings(self):\n",
        "    return self.cls.predictions.decoder\n",
        "\n",
        "  def set_output_embeddings(self, new_embeddings):\n",
        "    self.cls.predictions.decoder = new_embeddings\n",
        "\n",
        "  def forward(self,\n",
        "              input_ids=None,\n",
        "              attention_mask=None,\n",
        "              token_type_ids=None,\n",
        "              position_ids=None,\n",
        "              head_mask=None,\n",
        "              inputs_embeds=None,\n",
        "              encoder_hidden_states=None,\n",
        "              encoder_attention_mask=None,\n",
        "              labels=None,\n",
        "              output_attentions=None,\n",
        "              output_hidden_states=None,\n",
        "              return_dict=None,):\n",
        "    # TODO replace batch_size with document_length in here in the docfile\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      inputs_ids (torch.LongTensor of shape (batch_size, sequence_length)):\n",
        "        Indices of input sequence tokens in the vocabulary.\n",
        "      attention_mask (torch.FloatTensor of shape (batch_size, sequence_length), optional):\n",
        "        Mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]:\n",
        "        - 1 for tokens that are not masked,\n",
        "        - 0 for tokens that are masked.\n",
        "      token_type_ids  (torch.LongTensor of shape (batch_size, sequence_length), optional):\n",
        "        Segment token indices to indicate first and second portions of the inputs. Indices are selected in [0, 1]:\n",
        "        - 0 corresponds to a sentence A token,\n",
        "        - 1 corresponds to a sentence B token.\n",
        "      position_ids (torch.LongTensor of shape (batch_size, sequence_length), optional):\n",
        "        Indices of positions of each input sequence tokens in the position embeddings. \n",
        "        Selected in the range [0, config.max_position_embeddings - 1].\n",
        "      head_mask (torch.FloatTensor of shape (num_heads,) or (num_layers, num_heads), optional):\n",
        "        Mask to nullify selected heads of the self-attention modules. Mask values selected in [0, 1]:\n",
        "        - 1 indicates the head is not masked,\n",
        "        - 0 indicates the head is masked.\n",
        "      inputs_embeds (torch.FloatTensor of shape (batch_size, sequence_length, hidden_size), optional):\n",
        "        Optionally, instead of passing input_ids you can choose to directly pass\n",
        "         an embedded representation. This is useful if you want more control over \n",
        "         how to convert input_ids indices into associated vectors than the modelâ€™s \n",
        "         internal embedding lookup matrix.\n",
        "      encoder_hidden_states (torch.FloatTensor of shape (batch_size, sequence_length, hidden_size), optional):\n",
        "        Sequence of hidden-states at the output of the last layer of the encoder. \n",
        "        Used in the cross-attention if the model is configured as a decoder.\n",
        "      encoder_attention_mask (torch.FloatTensor of shape (batch_size, sequence_length), optional):\n",
        "        Mask to avoid performing attention on the padding token indices of the encoder \n",
        "        input. This mask is used in the cross-attention if the model is configured \n",
        "        as a decoder. Mask values selected in [0, 1]:\n",
        "        - 1 for tokens that are not masked,\n",
        "        - 0 for tokens that are masked.\n",
        "      labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "        Labels for computing the masked language modeling loss. Indices should be in ``[-100, 0, ...,\n",
        "        config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are ignored\n",
        "        (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``\n",
        "      output_attentions (bool, optional): \n",
        "        Whether or not to return the attentions tensors of all attention layers. \n",
        "        See attentions under returned tensors for more detail.\n",
        "      output_hidden_states (bool, optional):\n",
        "        Whether or not to return the hidden states of all layers. See hidden_states \n",
        "        under returned tensors for more detail.\n",
        "      return_dict (bool, optional):\n",
        "        Whether or not to return a ModelOutput instead of a plain tuple.\n",
        "    Returns:\n",
        "      SentenceModelingOutput:\n",
        "    \"\"\"\n",
        "    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "    outputs = self.transformer(input_ids,\n",
        "                               attention_mask=attention_mask,\n",
        "                               token_type_ids=token_type_ids,\n",
        "                               position_ids=position_ids,\n",
        "                               head_mask=head_mask,\n",
        "                               inputs_embeds=inputs_embeds,\n",
        "                               encoder_hidden_states=encoder_hidden_states,\n",
        "                               encoder_attention_mask=encoder_attention_mask,\n",
        "                               output_attentions=output_attentions,\n",
        "                               output_hidden_states=output_hidden_states,\n",
        "                               return_dict=return_dict)\n",
        "    \n",
        "    sequence_output = outputs[0]\n",
        "    prediction_scores = self.cls(sequence_output)\n",
        "\n",
        "    masked_lm_loss = None\n",
        "    if labels is not None:\n",
        "      loss_fct = nn.CrossEntropyLoss()  # -100 index = padding token\n",
        "      masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
        "\n",
        "    if not return_dict:\n",
        "      output = (prediction_scores,) + outputs[2:]\n",
        "      return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n",
        "\n",
        "    return SentenceModelingOutput(loss=masked_lm_loss,\n",
        "                                  logits=prediction_scores,\n",
        "                                  hidden_states=outputs.hidden_states,\n",
        "                                  last_hidden_state=sequence_output,\n",
        "                                  attentions=outputs.attentions,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ah6qLiazN-xP"
      },
      "source": [
        "## 4.4 Document Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVbDO-544Hb6"
      },
      "source": [
        "#### Document Model Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0lUc4tXBnAv"
      },
      "source": [
        "class DocumentModelingOutput(ModelOutput):\r\n",
        "  \"\"\"\r\n",
        "    Return object for Document Model.\r\n",
        "\r\n",
        "    Args:\r\n",
        "      loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`labels` is provided):\r\n",
        "        Masked language modeling (MLM) loss.\r\n",
        "      logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, config.vocab_size)`):\r\n",
        "        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\r\n",
        "      hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\r\n",
        "        Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\r\n",
        "        of shape :obj:`(batch_size, sequence_length, hidden_size)`.\r\n",
        "        Hidden-states of the model at the output of each layer plus the initial embedding outputs.\r\n",
        "      last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`):\r\n",
        "        Sequence of hidden-states at the output of the last layer of the model.\r\n",
        "      attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\r\n",
        "        Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\r\n",
        "        sequence_length, sequence_length)`.\r\n",
        "        Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\r\n",
        "        heads.\r\n",
        "  \"\"\"\r\n",
        "  per_example_loss_distance: Optional[torch.FloatTensor] =None\r\n",
        "  per_example_loss_product: Optional[torch.FloatTensor] =None\r\n",
        "  loss_distance: Optional[torch.FloatTensor] = None\r\n",
        "  loss_product: Optional[torch.FloatTensor] = None\r\n",
        "  logits: torch.FloatTensor = None\r\n",
        "  hidden_states: Optional[Tuple[torch.FloatTensor]] = None\r\n",
        "  last_hidden_state: torch.FloatTensor = None\r\n",
        "  attentions: Optional[Tuple[torch.FloatTensor]] = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJqgfSbh4OYN"
      },
      "source": [
        "#### Document Model Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72NE5FLzBkuu"
      },
      "source": [
        "class HATEDocumentModelConfig(PretrainedConfig):\r\n",
        "    \"\"\"\r\n",
        "    This is the configuration class to store the configuration of a TransformerModel. \r\n",
        "    It is used to instantiate a BERT model according to the specified arguments,\r\n",
        "    defining the model architecture. Instantiating a configuration with the defaults \r\n",
        "    will yield a similar configuration to that of the BERT `bert-base-uncased \r\n",
        "    <https://huggingface.co/bert-base-uncased>`__ architecture. Configuration objects \r\n",
        "    inherit from :class:`~transformers.PretrainedConfig` and can be used to control \r\n",
        "    the model outputs. Read the documentation from :class:`~transformers.PretrainedConfig` \r\n",
        "    for more information.\r\n",
        "    Args:\r\n",
        "        vocab_size (:obj:`int`, `optional`, defaults to 30522):\r\n",
        "            Vocabulary size of the BERT model. Defines the number of different tokens that can be represented by the\r\n",
        "            :obj:`inputs_ids` passed when calling :class:`~transformers.BertModel` or\r\n",
        "            :class:`~transformers.TFBertModel`.\r\n",
        "        hidden_size (:obj:`int`, `optional`, defaults to 768):\r\n",
        "            Dimensionality of the encoder layers and the pooler layer.\r\n",
        "        num_hidden_layers (:obj:`int`, `optional`, defaults to 12):\r\n",
        "            Number of hidden layers in the Transformer encoder.\r\n",
        "        num_attention_heads (:obj:`int`, `optional`, defaults to 12):\r\n",
        "            Number of attention heads for each attention layer in the Transformer encoder.\r\n",
        "        intermediate_size (:obj:`int`, `optional`, defaults to 3072):\r\n",
        "            Dimensionality of the \"intermediate\" (often named feed-forward) layer in the Transformer encoder.\r\n",
        "        hidden_act (:obj:`str` or :obj:`Callable`, `optional`, defaults to :obj:`\"gelu\"`):\r\n",
        "            The non-linear activation function (function or string) in the encoder and pooler. If string,\r\n",
        "            :obj:`\"gelu\"`, :obj:`\"relu\"`, :obj:`\"silu\"` and :obj:`\"gelu_new\"` are supported.\r\n",
        "        hidden_dropout_prob (:obj:`float`, `optional`, defaults to 0.1):\r\n",
        "            The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\r\n",
        "        attention_probs_dropout_prob (:obj:`float`, `optional`, defaults to 0.1):\r\n",
        "            The dropout ratio for the attention probabilities.\r\n",
        "        max_position_embeddings (:obj:`int`, `optional`, defaults to 512):\r\n",
        "            The maximum sequence length that this model might ever be used with. Typically set this to something large\r\n",
        "            just in case (e.g., 512 or 1024 or 2048).\r\n",
        "        type_vocab_size (:obj:`int`, `optional`, defaults to 2):\r\n",
        "            The vocabulary size of the :obj:`token_type_ids` passed when calling :class:`~transformers.BertModel` or\r\n",
        "            :class:`~transformers.TFBertModel`.\r\n",
        "        initializer_range (:obj:`float`, `optional`, defaults to 0.02):\r\n",
        "            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\r\n",
        "        layer_norm_eps (:obj:`float`, `optional`, defaults to 1e-12):\r\n",
        "            The epsilon used by the layer normalization layers.\r\n",
        "        gradient_checkpointing (:obj:`bool`, `optional`, defaults to :obj:`False`):\r\n",
        "            If True, use gradient checkpointing to save memory at the expense of slower backward pass.\r\n",
        "        position_embedding_type (:obj:`str`, `optional`, defaults to :obj:`\"absolute\"`):\r\n",
        "            Type of position embedding. Choose one of :obj:`\"absolute\"`, :obj:`\"relative_key\"`,\r\n",
        "            :obj:`\"relative_key_query\"`. For positional embeddings use :obj:`\"absolute\"`. For more information on\r\n",
        "            :obj:`\"relative_key\"`, please refer to `Self-Attention with Relative Position Representations (Shaw et al.)\r\n",
        "            <https://arxiv.org/abs/1803.02155>`__. For more information on :obj:`\"relative_key_query\"`, please refer to\r\n",
        "            `Method 4` in `Improve Transformer Models with Better Relative Position Embeddings (Huang et al.)\r\n",
        "            <https://arxiv.org/abs/2009.13658>`__.\r\n",
        "    Examples::\r\n",
        "        >>> from transformers import BertModel, BertConfig\r\n",
        "        >>> # Initializing a BERT bert-base-uncased style configuration\r\n",
        "        >>> configuration = BertConfig()\r\n",
        "        >>> # Initializing a model from the bert-base-uncased style configuration\r\n",
        "        >>> model = BertModel(configuration)\r\n",
        "        >>> # Accessing the model configuration\r\n",
        "        >>> configuration = model.config\r\n",
        "    \"\"\"\r\n",
        "    model_type = \"bert\"\r\n",
        "\r\n",
        "    def __init__(\r\n",
        "        self,\r\n",
        "        vocab_size=30522,\r\n",
        "        hidden_size=768,\r\n",
        "        num_hidden_layers=12,\r\n",
        "        num_attention_heads=12,\r\n",
        "        intermediate_size=3072,\r\n",
        "        hidden_act=\"gelu\",\r\n",
        "        hidden_dropout_prob=0.1,\r\n",
        "        attention_probs_dropout_prob=0.1,\r\n",
        "        max_position_embeddings=512,\r\n",
        "        type_vocab_size=2,\r\n",
        "        initializer_range=0.02,\r\n",
        "        layer_norm_eps=1e-12,\r\n",
        "        pad_token_id=0,\r\n",
        "        gradient_checkpointing=False,\r\n",
        "        position_embedding_type=\"absolute\",\r\n",
        "        **kwargs\r\n",
        "    ):\r\n",
        "        super().__init__(pad_token_id=pad_token_id, **kwargs)\r\n",
        "\r\n",
        "        self.vocab_size = vocab_size\r\n",
        "        self.hidden_size = hidden_size\r\n",
        "        self.num_hidden_layers = num_hidden_layers\r\n",
        "        self.num_attention_heads = num_attention_heads\r\n",
        "        self.hidden_act = hidden_act\r\n",
        "        self.intermediate_size = intermediate_size\r\n",
        "        self.hidden_dropout_prob = hidden_dropout_prob\r\n",
        "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\r\n",
        "        self.max_position_embeddings = max_position_embeddings\r\n",
        "        self.type_vocab_size = type_vocab_size\r\n",
        "        self.initializer_range = initializer_range\r\n",
        "        self.layer_norm_eps = layer_norm_eps\r\n",
        "        self.gradient_checkpointing = gradient_checkpointing\r\n",
        "        self.position_embedding_type = position_embedding_type"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1834mgo4S5k"
      },
      "source": [
        "#### Document Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIlngzQ-R91S"
      },
      "source": [
        "class HATEDocumentModel(TransformerPreTrainedModel):\n",
        "\n",
        "  _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
        "  _keys_to_ignore_on_load_missing = [r\"position_ids\", r\"predictions.decoder.bias\"]\n",
        "\n",
        "  def __init__(self, config):\n",
        "    super().__init__(config)\n",
        "\n",
        "    if config.is_decoder:\n",
        "      logger.warning(\n",
        "        \"If you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for \"\n",
        "        \"bi-directional self-attention.\"\n",
        "        )\n",
        "\n",
        "    self.transformer = TransformerBase(config, add_pooling_layer=False)\n",
        "    self.cls = SentencePredictionHead(config)\n",
        "    \n",
        "    #self.init_weights()\n",
        "\n",
        "  def forward(self,\n",
        "              input_ids=None,\n",
        "              attention_mask=None,\n",
        "              token_type_ids=None,\n",
        "              position_ids=None,\n",
        "              head_mask=None,\n",
        "              inputs_embeds=None,\n",
        "              encoder_hidden_states=None,\n",
        "              encoder_attention_mask=None,\n",
        "              labels_embeddings=None,\n",
        "              labels_mask=None,\n",
        "              output_attentions=None,\n",
        "              output_hidden_states=None,\n",
        "              return_dict=None,):\n",
        "    # TODO replace batch_size with document_length in here in the docfile\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      inputs_ids (torch.LongTensor of shape (batch_size, sequence_length)):\n",
        "        Indices of input sequence tokens in the vocabulary.\n",
        "      attention_mask (torch.FloatTensor of shape (batch_size, sequence_length), optional):\n",
        "        Mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]:\n",
        "        - 1 for tokens that are not masked,\n",
        "        - 0 for tokens that are masked.\n",
        "      token_type_ids  (torch.LongTensor of shape (batch_size, sequence_length), optional):\n",
        "        Segment token indices to indicate first and second portions of the inputs. Indices are selected in [0, 1]:\n",
        "        - 0 corresponds to a sentence A token,\n",
        "        - 1 corresponds to a sentence B token.\n",
        "      position_ids (torch.LongTensor of shape (batch_size, sequence_length), optional):\n",
        "        Indices of positions of each input sequence tokens in the position embeddings. \n",
        "        Selected in the range [0, config.max_position_embeddings - 1].\n",
        "      head_mask (torch.FloatTensor of shape (num_heads,) or (num_layers, num_heads), optional):\n",
        "        Mask to nullify selected heads of the self-attention modules. Mask values selected in [0, 1]:\n",
        "        - 1 indicates the head is not masked,\n",
        "        - 0 indicates the head is masked.\n",
        "      inputs_embeds (torch.FloatTensor of shape (batch_size, sequence_length, hidden_size), optional):\n",
        "        Optionally, instead of passing input_ids you can choose to directly pass\n",
        "         an embedded representation. This is useful if you want more control over \n",
        "         how to convert input_ids indices into associated vectors than the modelâ€™s \n",
        "         internal embedding lookup matrix.\n",
        "      encoder_hidden_states (torch.FloatTensor of shape (batch_size, sequence_length, hidden_size), optional):\n",
        "        Sequence of hidden-states at the output of the last layer of the encoder. \n",
        "        Used in the cross-attention if the model is configured as a decoder.\n",
        "      encoder_attention_mask (torch.FloatTensor of shape (batch_size, sequence_length), optional):\n",
        "        Mask to avoid performing attention on the padding token indices of the encoder \n",
        "        input. This mask is used in the cross-attention if the model is configured \n",
        "        as a decoder. Mask values selected in [0, 1]:\n",
        "        - 1 for tokens that are not masked,\n",
        "        - 0 for tokens that are masked.\n",
        "      labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "        Labels for computing the masked language modeling loss. Indices should be in ``[-100, 0, ...,\n",
        "        config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are ignored\n",
        "        (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``\n",
        "      output_attentions (bool, optional): \n",
        "        Whether or not to return the attentions tensors of all attention layers. \n",
        "        See attentions under returned tensors for more detail.\n",
        "      output_hidden_states (bool, optional):\n",
        "        Whether or not to return the hidden states of all layers. See hidden_states \n",
        "        under returned tensors for more detail.\n",
        "      return_dict (bool, optional):\n",
        "        Whether or not to return a ModelOutput instead of a plain tuple.\n",
        "    Returns:\n",
        "      DocumentModelingOutput:\n",
        "    \"\"\"\n",
        "\n",
        "    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "    outputs = self.transformer(input_ids,\n",
        "                               attention_mask=attention_mask,\n",
        "                               token_type_ids=token_type_ids,\n",
        "                               position_ids=position_ids,\n",
        "                               head_mask=head_mask,\n",
        "                               inputs_embeds=inputs_embeds,\n",
        "                               encoder_hidden_states=encoder_hidden_states,\n",
        "                               encoder_attention_mask=encoder_attention_mask,\n",
        "                               output_attentions=output_attentions,\n",
        "                               output_hidden_states=output_hidden_states,\n",
        "                               return_dict=return_dict,)\n",
        "    last_hidden_state = outputs['last_hidden_state']\n",
        "    \n",
        "    if labels_embeddings is not None:\n",
        "      sentence_prediction_output = self.cls(outputs['last_hidden_state'], labels_embeddings, labels_mask)\n",
        "      per_example_loss_distance=sentence_prediction_output[4],\n",
        "      per_example_loss_product=sentence_prediction_output[5],\n",
        "      loss_distance=sentence_prediction_output[6],\n",
        "      loss_product=sentence_prediction_output[7],\n",
        "      logits=sentence_prediction_output[0]\n",
        "\n",
        "      return DocumentModelingOutput(per_example_loss_distance=sentence_prediction_output[4],\n",
        "                                    per_example_loss_product=sentence_prediction_output[5],\n",
        "                                    loss_distance=sentence_prediction_output[6],\n",
        "                                    loss_product=sentence_prediction_output[7],\n",
        "                                    logits=sentence_prediction_output[0],\n",
        "                                    hidden_states=outputs.hidden_states,\n",
        "                                    last_hidden_state=last_hidden_state,\n",
        "                                    attentions=outputs.attentions,)\n",
        "    else:\n",
        "      return DocumentModelingOutput(hidden_states=outputs.hidden_states,\n",
        "                                    last_hidden_state=last_hidden_state,\n",
        "                                    attentions=outputs.attentions,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IVJdy-_qAPt"
      },
      "source": [
        "## 4.5 Hierachical Attention-Based Document Encoder (HATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3evzCMPzdeL"
      },
      "source": [
        "#### HATE Model Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0WDofhneV06"
      },
      "source": [
        "class HATEOutput(ModelOutput):\r\n",
        "  \"\"\"\r\n",
        "  Class for the whole model output\r\n",
        "  \"\"\"\r\n",
        "  sentence_model_hidden_states = None\r\n",
        "  document_model_hidden_states = None\r\n",
        "\r\n",
        "  sentence_model_last_hidden_states = None\r\n",
        "  document_model_last_hidden_states = None\r\n",
        "\r\n",
        "  sentence_model_attentions = None\r\n",
        "  document_model_attentions = None\r\n",
        "\r\n",
        "  sentence_model_logits = None\r\n",
        "  document_model_logits = None\r\n",
        "\r\n",
        "  sentence_model_loss = None\r\n",
        "  document_model_loss = None\r\n",
        "  total_loss = None\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSXwRXFRziOi"
      },
      "source": [
        "#### HATE Model Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VglcydbCqvL_"
      },
      "source": [
        "class HATEConfig ():\n",
        "  def __init__(self,\n",
        "               sentence_model_config,\n",
        "               document_model_config,\n",
        "               is_pretraining=False,\n",
        "               use_product_loss=True):\n",
        "    \n",
        "    \"\"\"\n",
        "    Constructs ModelConfig.\n",
        "    Args:\n",
        "      Stuff\n",
        "    Returns:\n",
        "      Stuff\n",
        "    \"\"\"\n",
        "    self.sentence_model_config = HATESentenceModelConfig()\n",
        "    self.document_model_config = HATEDocumentModelConfig()\n",
        "    self.is_pretraining = is_pretraining\n",
        "    self.use_product_loss = use_product_loss\n",
        "    \n",
        "  # @classmethod\n",
        "  # def from_dict(cls, json_object):\n",
        "  #   \"\"\"Constructs a `BertConfig` from a Python dictionary of parameters.\"\"\"\n",
        "  #   config = BertConfig(vocab_size=None)\n",
        "  #   for (key, value) in six.iteritems(json_object):\n",
        "  #     config.__dict__[key] = value\n",
        "  #   return config\n",
        "\n",
        "  # @classmethod\n",
        "  # def from_json_file(cls, json_file):\n",
        "  #   \"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"\n",
        "  #   with tf.gfile.GFile(json_file, \"r\") as reader:\n",
        "  #     text = reader.read()\n",
        "  #   return cls.from_dict(json.loads(text))\n",
        "\n",
        "  # def to_dict(self):\n",
        "  #   \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
        "  #   output = copy.deepcopy(self.__dict__)\n",
        "  #   return output\n",
        "\n",
        "  # def to_json_string(self):\n",
        "  #   \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
        "  #   return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Pc3_xs3zmp-"
      },
      "source": [
        "#### HATE Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxULKxFYz7uz"
      },
      "source": [
        "class HATEModel (torch.nn.Module):\n",
        "  def __init__(self, hate_config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.hate_config = hate_config\n",
        "\n",
        "    self.sentence_model = HATESentenceModel(hate_config.sentence_model_config)\n",
        "    self.document_model = HATEDocumentModel(hate_config.document_model_config)\n",
        "\n",
        "  # TODO add another paramter pretraining: Bool to the forward() parameters that\n",
        "  # gets checked and if it's false it doesn't mask tokens/embeddings and passes\n",
        "  # the inputs embeddings without labels to the Sent/Doc Model.\n",
        "  def forward(self, batch = None): \n",
        "    \"\"\"\n",
        "    Args:\n",
        "      batch: List with 3 entries from our custom collate function\n",
        "        batch[0]: The length of the largest document in the batch, the other\n",
        "          documents in the batch will be padded to this length with 0-sentences \n",
        "        batch[1]: A list of the original unpadded size of every document in the\n",
        "          batch, will be used when the intermediary sentence representations are\n",
        "          padded with the appropriate attention mask and special token mask for\n",
        "          the document model.\n",
        "        batch[2]: Dictionary that contains the padded tensors.\n",
        "          ['input_ids'] (batch_size, max_doc_length, padding_strategy)\n",
        "          ['special_tokens_mask'] (batch_size, max_doc_length, padding_strategy)\n",
        "          ['attention_mask'] (batch_size, max_doc_length, padding_strategy)\n",
        "          ['token_type_ids'] (batch_size, max_doc_length, padding_strategy)\n",
        "    Returns:\n",
        "      A HATEModelOutput Object, entries are filled depending on whether model is\n",
        "      in pretraining mode or not. During pretraining every entry is filled. If\n",
        "      model is not in pretraining it won't return losses and logits.\n",
        "    \"\"\"\n",
        "    max_doc_length = batch[0]\n",
        "    doc_length_before_padding = batch[1]\n",
        "\n",
        "    sentence_level_loss = 0\n",
        "    document_level_loss = 0\n",
        "\n",
        "    # Initiates lists that aggregate input for the document model\n",
        "    intermediary_embeddings = []\n",
        "    intermediary_attention_mask = []\n",
        "    intermediary_special_tokens_mask = []\n",
        "\n",
        "    # Initiates lists later used in HATEModelOutput\n",
        "    sentence_model_hidden_states = []\n",
        "    sentence_model_last_hidden_states = []\n",
        "    sentence_model_attentions = []\n",
        "    sentence_model_logits = []\n",
        "\n",
        "    # Iterate over documents and apply the Sentence Model per doc\n",
        "    for doc_counter, doc in enumerate(batch[2]):\n",
        "      \n",
        "      # Initiate embedding tensor, attention mask and special tokens mask for one\n",
        "      # document. The individual sentence embeddings and their according attention\n",
        "      # mask label as well as special token label will be appended to this. We \n",
        "      # initiate them with a random tensor at position 0 to denote the CLS token.\n",
        "      sentence_embeddings_per_doc = [torch.randn(768)]\n",
        "      attention_mask_per_doc = [1]\n",
        "      special_tokens_mask_per_doc = [1]\n",
        "\n",
        "      # Don't perform masking if pretrain=False\n",
        "      if self.hate_config.is_pretraining:\n",
        "        # Mask input IDs for one document\n",
        "        masking_output = mask_input_ids(doc['input_ids'], doc['special_tokens_mask'])\n",
        "        # Feed all the inputs to the Sentence Model for one document\n",
        "        sentence_model_output = sentence_model(input_ids=masking_output[0], \n",
        "                                               attention_mask=doc['attention_mask'], \n",
        "                                               token_type_ids=doc['token_type_ids'],\n",
        "                                               inputs_embeds=None,\n",
        "                                               labels=masking_output[1],\n",
        "                                               output_attentions=True,\n",
        "                                               output_hidden_states=True)\n",
        "        sentence_level_loss += sentence_model_output['loss']\n",
        "      else:\n",
        "        sentence_model_output = sentence_model(input_ids=batch[2]['input_ids'], \n",
        "                                               attention_mask=batch[2]['attention_mask'], \n",
        "                                               token_type_ids=batch[2]['token_type_ids'],)\n",
        "      \n",
        "      # Aggregate releveant sentence model outputs for HATEModelOutput\n",
        "      sentence_model_hidden_states.append(sentence_model_output['hidden_states'])\n",
        "      sentence_model_last_hidden_states.append(sentence_model_output['last_hidden_state'])\n",
        "      sentence_model_attentions.append(sentence_model_output['attentions'])\n",
        "      sentence_model_logits.append(sentence_model_output['logits'])\n",
        "\n",
        "      # Iterate over sequence embeddings returned by the model for one document\n",
        "      # sentence_model_output['last_hidden_state'] has size \n",
        "      # (doc_length, max_position_embeddings, hidden_size)\n",
        "      for sentence_counter, sentence in enumerate(sentence_model_output['last_hidden_state']):\n",
        "        # CLS embedding for a sentence at sentence_counter position in the document\n",
        "        # CLS is at position 0 out of max_position_embeddings (512 usually)\n",
        "        \n",
        "        # Pad the embeddings and additional necessary inputs to the maximum document length in the batch\n",
        "        sentence_embeddings_per_doc.append(sentence[0])\n",
        "        # If the sentence representations are still from 'real' sentences then we\n",
        "        # add positive attention mask and negative special token mask\n",
        "        if sentence_counter < doc_length_before_padding[doc_counter]:\n",
        "          attention_mask_per_doc.append(1)\n",
        "          special_tokens_mask_per_doc.append(0)\n",
        "        # If the sentence representation is just a padding added by our custom\n",
        "        # collate function then we add the masks the other way around.\n",
        "        else:\n",
        "          attention_mask_per_doc.append(0)\n",
        "          special_tokens_mask_per_doc.append(1)\n",
        "\n",
        "      # Appends the padded tensors to the list of document-wise sentence embeddings\n",
        "      intermediary_embeddings.append(torch.stack(sentence_embeddings_per_doc))\n",
        "      intermediary_attention_mask.append(torch.FloatTensor(attention_mask_per_doc))\n",
        "      intermediary_special_tokens_mask.append(torch.FloatTensor(special_tokens_mask_per_doc))\n",
        "\n",
        "    # Stacks the relevant sentence model outputs into tensors for HATEModelOutput\n",
        "    sentence_model_last_hidden_states_tensor = torch.stack(sentence_model_last_hidden_states)\n",
        "    sentence_model_logits_tensor = torch.stack(sentence_model_logits)\n",
        "\n",
        "    # Stacks the list into a Torch Tensor of fixed size\n",
        "    # Embedding tensor has size (batch_size, max_doc_length+1, hidden_size)\n",
        "    intermediary_embeddings_tensor = torch.stack(intermediary_embeddings)\n",
        "    intermediary_attention_mask_tensor = torch.stack(intermediary_attention_mask)\n",
        "    intermediary_special_tokens_mask_tensor = torch.stack(intermediary_special_tokens_mask)\n",
        "\n",
        "    # TODO add dense layer and normalization on intermediary_embeddings_tensor?\n",
        "\n",
        "    # Don't perform masking if pretraining=False:\n",
        "    if self.hate_config.is_pretraining:\n",
        "      masked_input_embeddings = mask_input_embeddings(intermediary_embeddings_tensor, \n",
        "                                                      intermediary_special_tokens_mask_tensor)\n",
        "      \n",
        "      document_model_output = document_model(attention_mask=intermediary_attention_mask_tensor,\n",
        "                                             inputs_embeds=masked_input_embeddings[1],\n",
        "                                             labels_embeddings=masked_input_embeddings[2],\n",
        "                                             labels_mask=masked_input_embeddings[3])\n",
        "      \n",
        "      # Allow switching between loss functions in document model\n",
        "      if self.hate_config.use_product_loss:\n",
        "        document_level_loss += document_model_output['loss_product']\n",
        "      else:\n",
        "        document_level_loss += document_model_output['loss_sum']\n",
        "\n",
        "      total_loss = sentence_level_loss + document_level_loss\n",
        "\n",
        "      return HATEOutput(sentence_model_hidden_states=sentence_model_hidden_states,\n",
        "                        document_model_hidden_states=document_model_output['hidden_states'],\n",
        "                        sentence_model_last_hidden_states=sentence_model_last_hidden_states_tensor,\n",
        "                        document_model_last_hidden_states=document_model_output['last_hidden_state'],\n",
        "                        sentence_model_attentions=sentence_model_attentions,\n",
        "                        document_model_attentions=document_model_output['attentions'],\n",
        "                        sentence_model_logits=sentence_model_logits_tensor,\n",
        "                        document_model_logits=document_model_output['logits'],\n",
        "                        sentence_model_loss=sentence_level_loss,\n",
        "                        document_model_loss=document_level_loss,\n",
        "                        total_loss=total_loss)\n",
        "    else:\n",
        "      document_model_output = document_model(attention_mask=intermediary_attention_mask_tensor,\n",
        "                                             inputs_embeds=masked_input_embeddings[1],)\n",
        "      \n",
        "      return HATEOutput(sentence_model_hidden_states=sentence_model_hidden_states,\n",
        "                        document_model_hidden_states=document_model_output['hidden_states'],\n",
        "                        sentence_model_last_hidden_states=sentence_model_last_hidden_states_tensor,\n",
        "                        document_model_last_hidden_states=document_model_output['last_hidden_state'],\n",
        "                        sentence_model_attentions=sentence_model_attentions,\n",
        "                        document_model_attentions=document_model_output['attentions'])\n",
        "\n",
        "    # TODO prune layers? transformers.modeling_utils.find_pruneable_heads_and_indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YleJunzjQSh-"
      },
      "source": [
        "# **5.** Pretraining and Finetuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYLHvvvFOkuU"
      },
      "source": [
        "## 5.1 Pretraining"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTTNBPu6MMsX"
      },
      "source": [
        "### 5.1.1 Evidence for the need for Pretraining\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boIilmUBvAWw"
      },
      "source": [
        "- HIBERT https://arxiv.org/pdf/1905.06566.pdf\n",
        "- Language Model Pre-training for Hierarchical Document Representations https://arxiv.org/pdf/1901.09128.pdf\n",
        "- Pre-training Tasks for Embedding-based Large-scale Retrieval https://arxiv.org/pdf/2002.03932.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-smV5mIMXzj"
      },
      "source": [
        "### 5.1.2 Pretraining Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kLltdEAWvd7"
      },
      "source": [
        "Describe Dataset used for pretraining, where to get it and how to load it\n",
        "https://dumps.wikimedia.org/enwiki/20200920/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwlLYYpKHAj2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6e6ce4e-94f7-4c03-b540-0d47b9b22eae"
      },
      "source": [
        "wiki_data_raw = load_dataset('wikipedia', '20200501.en', split='train[:1%]')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reusing dataset wikipedia (/root/.cache/huggingface/datasets/wikipedia/20200501.en/1.0.0/f92599dfccab29832c442b82870fa8f6983e5b4ebbf5e6e2dcbe894e325339cd)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SofsbkO1Tf_2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0393d3f-1bb4-47bc-845c-1f3d6ad0bd9a"
      },
      "source": [
        "# Print out some basic info about the dataset to better understand the structure\n",
        "print(\"size: \", wiki_data_raw.dataset_size)\n",
        "print(\"column names: \", wiki_data_raw.column_names)\n",
        "print(\"shape: \", wiki_data_raw.shape)\n",
        "print(\"format: \", wiki_data_raw.format)\n",
        "print(\"description: \", wiki_data_raw.description)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "size:  18330235071\n",
            "column names:  ['title', 'text']\n",
            "shape:  (60784, 2)\n",
            "format:  {'type': None, 'format_kwargs': {}, 'columns': ['title', 'text'], 'output_all_columns': False}\n",
            "description:  Wikipedia dataset containing cleaned articles of all languages.\n",
            "The datasets are built from the Wikipedia dump\n",
            "(https://dumps.wikimedia.org/) with one split per language. Each example\n",
            "contains the content of one full Wikipedia article with cleaning to strip\n",
            "markdown and unwanted sections (references, etc.).\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTJuia_PGDMx"
      },
      "source": [
        "# Create short version dataset containing 10 documents for quick testing\n",
        "wiki_data_test = wiki_data_raw[:2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DriQikaMnxv"
      },
      "source": [
        "### 5.1.3 Process Pretraining Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCXBiH9uiNBO"
      },
      "source": [
        "Some theory on preprocessing: https://mlexplained.com/2019/11/06/a-deep-dive-into-the-wonderful-world-of-preprocessing-in-nlp/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBnJKkLtc7YE"
      },
      "source": [
        "#### Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mwz_T5I5v_zw"
      },
      "source": [
        "class PretrainingData(Dataset):\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.tokenizer = transformers.BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "    self.wiki_dump = wiki_data_test # dataset #load_dataset('wikipedia', '20200501.en', split='train')\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.wiki_dump['text'])\n",
        "\n",
        "  # TODO which of batch_encode_plus/encode_plus/prepare_for_model is better?\n",
        "  def __getitem__(self, id):\n",
        "    # Split document at dataset[id] into sentences\n",
        "    doc_split = sent_tokenize(self.wiki_dump['text'][id])\n",
        "    # Batch-encodes a whole document at dataset[id] at once\n",
        "    doc_tokenized =  self.tokenizer.batch_encode_plus(doc_split,\n",
        "                                                      padding='longest',\n",
        "                                                      truncation='longest_first',\n",
        "                                                      return_tensors='pt',\n",
        "                                                      return_special_tokens_mask=True)\n",
        "    return doc_tokenized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IO_CxT4c1Kb"
      },
      "source": [
        "#### Data Processing Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLhuaZogTHn_"
      },
      "source": [
        "def pad_input_embeds(batch):\r\n",
        "  \"\"\"\r\n",
        "  Receives a batch of documents and pads them to the longest document in the \r\n",
        "  batch so that each batch has documents with the same length but different\r\n",
        "  batches can have documents with different length.\r\n",
        "  NOTE: This does not affect the padding of sentences *within* documents! The\r\n",
        "        individual sentences of a document get padded to the length of the\r\n",
        "        document's longest sentence by the tokenizer.\r\n",
        "  Args:\r\n",
        "    batch: A list of length [batch_size] specified within the DataLoader that\r\n",
        "      contains documents and their model-specific information as a Dictionary\r\n",
        "      which itself contains torch.Tensors of size (doc_length, longest_seq_in_doc)\r\n",
        "      where longest_seq_in_doc is computed by the Tokenizer during retrieval \r\n",
        "      from the Dataset.\r\n",
        "  Returns:\r\n",
        "    (longest_document, longest_sentence, batch): \r\n",
        "      A triple where the first entry is the longest doc in the batch before padding. \r\n",
        "      It's passed to the model as a utility because the model uses that size for \r\n",
        "      intermediary padding.\r\n",
        "      The second entry is the length of longest sentence as a list with length\r\n",
        "      [batch_size] that holds the length of the longest sentence of each document.\r\n",
        "      That number was used by the Tokenizer to pad sentences document-wise.\r\n",
        "      The third entry is the appropriately padded batch ready to be passed to\r\n",
        "      the model.\r\n",
        "  \"\"\"\r\n",
        "  longest_document = max(len(doc['input_ids']) for doc in batch)\r\n",
        "  doc_length_before_padding = []\r\n",
        "  \r\n",
        "  for doc in batch:\r\n",
        "    doc_length_before_padding.append(len(doc['input_ids']))\r\n",
        "    if len(doc['input_ids']) < longest_document:\r\n",
        "      doc['input_ids'] = torch.Tensor.long(torch.cat((doc['input_ids'], torch.zeros(longest_document - len(doc['input_ids']), doc['input_ids'].size()[1])), dim=0))\r\n",
        "      doc['token_type_ids'] = torch.Tensor.long(torch.cat((doc['token_type_ids'], torch.zeros(longest_document - len(doc['token_type_ids']), doc['token_type_ids'].size()[1])), dim=0))\r\n",
        "      doc['attention_mask'] = torch.Tensor.long(torch.cat((doc['attention_mask'], torch.zeros(longest_document - len(doc['attention_mask']), doc['attention_mask'].size()[1])), dim=0))\r\n",
        "      doc['special_tokens_mask'] = torch.Tensor.long(torch.cat((doc['special_tokens_mask'], torch.zeros(longest_document - len(doc['special_tokens_mask']), doc['special_tokens_mask'].size()[1])), dim=0))\r\n",
        "\r\n",
        "  return (longest_document, doc_length_before_padding, batch)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXQFUfbHU76V"
      },
      "source": [
        "### 5.1.4 Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXwYKZCOUCuY"
      },
      "source": [
        "#### Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQWBgEbxblTN"
      },
      "source": [
        "# https://github.com/cs230-stanford/cs230-code-examples/blob/master/pytorch/nlp/train.py\n",
        "# https://github.com/henryre/pytorch-fitmodule/blob/master/pytorch_fitmodule/fit_module.py\n",
        "# https://stackoverflow.com/questions/59584457/pytorch-is-there-a-definitive-training-loop-similar-to-keras-fit\n",
        "\n",
        "\n",
        "def train(wiki_dump_filepath=None, checkpoint_filepath=None, epochs=5):\n",
        "\n",
        "  dataset = PretrainingData(wiki_dump_filepath)\n",
        "  train_dataset, validation_dataset = random_split(dataset, [90,10])\n",
        "\n",
        "  train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=pad_input_embeds)\n",
        "  validation_dataloader = Dataloader(validation_dataset, batch_size=32, shuffle=True, collate_fn=pad_input_embeds)\n",
        "\n",
        "\n",
        "  config = HATEModelConfig\n",
        "  model = HATEModel(config) # what if we use a pretrained BertModel as the Sentence model and fix the weights and only train the Document Mode?\n",
        "\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  if torch.cuda.device_count() > 1:\n",
        "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "    # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
        "    model = nn.DataParallel(model)\n",
        "  model.to(device)\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters())\n",
        "  scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer)\n",
        "\n",
        "  if checkpoint is not None:\n",
        "    checkpoint = torch.load(checkpoint_filepath)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epoch = checkpoint['epoch']\n",
        "    loss = checkpoint['loss']\n",
        "  \n",
        "  # possibly add gradient clipping here: torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "  # https://huggingface.co/transformers/training.html\n",
        "  \n",
        "\n",
        "  # TODO add logging\n",
        "  for epoch in range(epochs):\n",
        "    model.train()\n",
        "    # TODO double-check if implementing the scheduler at this position is the right way to do it\n",
        "    scheduler.step()\n",
        "    for iteration, batch in enumerate(train_dataloader):\n",
        "      batch.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(batch)\n",
        "      loss = outputs['loss']\n",
        "      loss.backwards() # Avarage GPU-losses? See medium post below\n",
        "      # TODO does it make sense to accumulate gradients over multiple passes? See https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255\n",
        "      optimizer.step()\n",
        "      # TODO specify the last dateset ID where we left of so we don't restart from the first document\n",
        "      if (iteration % 100 == 0):\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'loss': loss,\n",
        "            }, PATH)\n",
        "    model.eval()\n",
        "    for iteration, batch in enumerate(validation_dataloader):\n",
        "      batch.to(device)\n",
        "      outputs = model(batch)\n",
        "      total_loss = outputs['loss']\n",
        "      sentence_loss = outputs['sentence_model_loss']\n",
        "      document_loss = outputs['document_model_loss']\n",
        "      if (iteration % 100 == 0):\n",
        "        # TODO print statistics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scUYJxoKT8MX"
      },
      "source": [
        "#### Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gE5ahz7-59Q"
      },
      "source": [
        "dataset = PretrainingData()\r\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=pad_input_embeds)\r\n",
        "\r\n",
        "sentence_config = HATESentenceModelConfig()\r\n",
        "sentence_model = HATESentenceModel(sentence_config)\r\n",
        "document_config = HATEDocumentModelConfig()\r\n",
        "document_model = HATEDocumentModel(document_config)\r\n",
        "hate_config = HATEConfig(sentence_config, document_config, is_pretraining=True)\r\n",
        "hate_model = HATEModel(hate_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTpnSy2Hg0yA",
        "outputId": "234b96bd-eb48-4a85-cf22-da03332583da"
      },
      "source": [
        "for batch in dataloader:\r\n",
        "  for doc_counter, doc in enumerate(batch[2]):\r\n",
        "    masking_output = mask_input_ids(doc['input_ids'], doc['special_tokens_mask'])\r\n",
        "    sentence_model_output = sentence_model(input_ids=masking_output[0], \r\n",
        "                                               attention_mask=doc['attention_mask'], \r\n",
        "                                               token_type_ids=doc['token_type_ids'],\r\n",
        "                                               inputs_embeds=None,\r\n",
        "                                               labels=masking_output[1],\r\n",
        "                                               output_attentions=True,\r\n",
        "                                               output_hidden_states=True)\r\n",
        "    print(sentence_model_output['logits'].type())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.FloatTensor\n",
            "torch.FloatTensor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4P0RrmHAdAk"
      },
      "source": [
        "for batch in dataloader:\r\n",
        "  outputs = hate_model(batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIlN9E_41qux",
        "outputId": "df338b20-f228-47f3-ff13-dfcb3e6825eb"
      },
      "source": [
        "outputs['document_model_last_hidden_states'][0].size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([26, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PMpLLrKHm2p"
      },
      "source": [
        "doc = dataset.__getitem__(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1qf786TqtF9"
      },
      "source": [
        "masked_doc = mask_input_ids(doc['input_ids'], doc['special_tokens_mask'])\r\n",
        "\r\n",
        "sentence_output1 = sentence_model(input_ids=masked_doc[0], attention_mask=doc['attention_mask'], token_type_ids=doc['token_type_ids'],inputs_embeds=None,labels=masked_doc[1],output_attentions=True,output_hidden_states=True)\r\n",
        "\r\n",
        "sentence_output2 = sentence_model(attention_mask=doc['attention_mask'], token_type_ids=doc['token_type_ids'],inputs_embeds=torch.randn(25,66,768),labels=masked_doc[1],output_attentions=True,output_hidden_states=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0EMMbFvlAsy",
        "outputId": "063ac3e1-e22d-49e6-824b-ea42f3bb4150"
      },
      "source": [
        "print(masked_doc[0].type())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.LongTensor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiOcy21H70pn",
        "outputId": "26ad5f44-5bad-4a16-ddd6-7d9c894a2711"
      },
      "source": [
        "print(sentence_output1['last_hidden_state'])\r\n",
        "print(sentence_output2['last_hidden_state'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[-1.4772, -0.1446,  1.6632,  ..., -0.4423, -2.2877,  0.5593],\n",
            "         [-0.6135,  1.8591,  0.1678,  ...,  0.0650, -2.5850,  0.7380],\n",
            "         [-0.7336,  1.2801,  0.4921,  ..., -0.5017, -0.4749, -0.0804],\n",
            "         ...,\n",
            "         [-1.1357,  0.5250, -0.5649,  ..., -0.1531, -0.4998,  0.6912],\n",
            "         [ 0.0822,  1.8056,  1.1023,  ...,  1.0246, -0.8442,  0.9127],\n",
            "         [-0.8101,  1.3218,  1.0677,  ..., -0.1809, -1.1173,  0.3368]],\n",
            "\n",
            "        [[-1.8497, -0.0979,  1.3914,  ..., -1.0873, -0.7782,  0.7275],\n",
            "         [-0.8851,  1.7529,  1.3515,  ...,  0.7395, -0.7889,  0.6637],\n",
            "         [-1.1711,  0.4401, -0.4902,  ..., -0.3227, -0.3582,  0.2955],\n",
            "         ...,\n",
            "         [-1.4793,  1.2667,  0.3273,  ...,  0.4088,  0.1079,  0.7485],\n",
            "         [-0.7861,  1.5163, -0.7532,  ...,  1.0860, -0.3750,  0.6023],\n",
            "         [-0.2210,  2.1555, -0.5344,  ...,  0.7108, -1.1311, -0.4062]],\n",
            "\n",
            "        [[-1.3217,  0.1941, -0.3382,  ..., -0.8870, -2.3667,  1.2355],\n",
            "         [-1.0023,  0.7388,  0.8454,  ...,  0.2809, -0.7056,  0.4762],\n",
            "         [-1.7484,  0.8705, -0.0542,  ...,  0.3686, -0.8203,  1.6753],\n",
            "         ...,\n",
            "         [-0.3868,  1.2035, -0.0272,  ..., -0.0380, -0.6941,  0.9229],\n",
            "         [-0.9413,  1.4789, -0.1104,  ...,  0.2159, -0.6007,  0.6818],\n",
            "         [-0.3024,  2.2684,  1.5510,  ...,  0.6008, -1.2991,  0.7871]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.7311, -0.6884,  0.1540,  ..., -0.4591, -1.1571,  1.1794],\n",
            "         [-0.9054,  0.5510,  0.2255,  ...,  1.6480, -1.2173, -0.1731],\n",
            "         [-1.6639,  1.2176, -0.6359,  ..., -0.6474, -0.5395, -0.7591],\n",
            "         ...,\n",
            "         [-0.7092,  2.1554,  0.1478,  ...,  0.5523, -1.0730,  1.3008],\n",
            "         [-0.0119,  1.7204,  0.1823,  ...,  1.3458, -0.5274,  0.8497],\n",
            "         [-0.8542,  2.0085,  0.9684,  ...,  0.7222, -0.2448, -0.1639]],\n",
            "\n",
            "        [[-1.5316,  0.0257,  0.4731,  ..., -1.2146, -1.9798,  0.8350],\n",
            "         [-0.1045,  0.8227,  1.5230,  ...,  0.5017, -0.7752,  0.7743],\n",
            "         [-1.0698,  1.7757, -1.0208,  ..., -0.3912, -0.3277,  0.0186],\n",
            "         ...,\n",
            "         [-1.8783,  1.6287, -0.2431,  ...,  1.2871, -0.4578,  1.5905],\n",
            "         [-0.5650,  1.8048,  0.1175,  ...,  1.0741, -1.3721,  0.8728],\n",
            "         [-0.7401,  1.2402, -0.7779,  ...,  0.0233, -1.7778,  0.2869]],\n",
            "\n",
            "        [[-0.2495, -0.4212,  0.0954,  ..., -0.3233, -1.0860,  0.8339],\n",
            "         [-1.2365,  1.0240,  0.5131,  ...,  0.3089, -0.6114,  1.4075],\n",
            "         [-0.6547,  0.8063, -0.1914,  ..., -0.2824, -0.4039,  0.4353],\n",
            "         ...,\n",
            "         [-0.8716,  0.8110, -0.4576,  ...,  0.2993,  0.0049,  0.5115],\n",
            "         [-1.5591,  1.4264, -0.0758,  ...,  0.7909, -1.1423,  0.9725],\n",
            "         [-0.7564,  2.0346,  0.5531,  ..., -0.0663, -0.9858,  1.2113]]],\n",
            "       grad_fn=<NativeLayerNormBackward>)\n",
            "tensor([[[ 0.0922,  0.4203,  1.4363,  ...,  1.0704, -1.2408, -0.3020],\n",
            "         [-0.0296, -0.0881, -0.0271,  ..., -0.3553,  0.8408,  1.1642],\n",
            "         [ 0.5322,  0.3359,  0.2210,  ...,  0.1078, -1.4074,  0.3925],\n",
            "         ...,\n",
            "         [ 1.5179,  1.2511, -0.5591,  ...,  0.5810, -1.7374,  0.0402],\n",
            "         [-0.3297,  0.6592,  0.1039,  ...,  1.1259,  0.5951, -0.3897],\n",
            "         [ 1.3303,  0.2113,  0.1887,  ..., -1.1212, -1.1388, -0.9446]],\n",
            "\n",
            "        [[ 1.4792, -0.1221,  0.6890,  ...,  0.5311, -1.6214,  1.0164],\n",
            "         [-0.5430,  1.8691,  0.0314,  ...,  0.7344,  0.6229,  0.0261],\n",
            "         [-0.0561,  1.2256, -0.5109,  ...,  0.0923, -2.4608, -1.3921],\n",
            "         ...,\n",
            "         [ 0.4188,  1.2532, -0.0299,  ...,  2.1893, -0.8958,  0.1409],\n",
            "         [ 1.5177, -0.6072, -0.8829,  ...,  0.3687,  1.2338,  0.4823],\n",
            "         [ 1.3967,  1.2122, -1.9675,  ...,  1.4307, -0.9124, -0.6330]],\n",
            "\n",
            "        [[-1.1473,  1.2537,  1.2343,  ...,  0.3887, -0.3068,  0.2819],\n",
            "         [-1.9284,  1.0344, -0.4540,  ...,  0.1584, -1.0508, -2.0996],\n",
            "         [-1.2530,  1.0110,  0.4344,  ...,  1.4671, -0.2941,  0.0377],\n",
            "         ...,\n",
            "         [-1.1503,  1.0959, -0.2410,  ...,  1.3195, -1.1158, -0.0037],\n",
            "         [ 0.4369,  2.2277, -1.6610,  ...,  0.5572, -0.4010,  0.4194],\n",
            "         [-0.6240,  1.7894, -0.4522,  ...,  1.2180, -2.9769, -0.6363]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.4239,  0.1250, -0.3665,  ...,  0.4202,  0.7352,  0.5872],\n",
            "         [-0.9078, -1.7084, -0.7643,  ...,  0.0881,  0.4489,  0.2527],\n",
            "         [ 1.5909, -0.2831,  0.6821,  ..., -0.1449,  0.8245, -0.5505],\n",
            "         ...,\n",
            "         [ 0.7309,  0.4335, -0.2004,  ...,  0.6673,  0.6338,  0.0812],\n",
            "         [ 1.0080,  0.5845, -0.5214,  ...,  0.6275,  1.3173,  0.1475],\n",
            "         [-0.2108, -0.0204,  1.2930,  ...,  1.8143, -0.7690,  0.8606]],\n",
            "\n",
            "        [[-0.5395, -0.2657, -1.5318,  ..., -1.5249,  0.3718, -1.2336],\n",
            "         [-0.3380,  1.8689, -1.1106,  ..., -0.8938, -1.3402,  1.1991],\n",
            "         [ 0.5149,  0.3169, -1.4449,  ..., -1.3665, -1.4516, -1.2793],\n",
            "         ...,\n",
            "         [-0.6799, -1.7779, -1.1825,  ..., -0.2432, -1.5835, -2.3528],\n",
            "         [-1.7167, -2.2210,  0.0444,  ...,  0.5795, -0.5956, -0.0868],\n",
            "         [ 0.0425, -0.4185, -0.8103,  ...,  0.8134,  0.3763,  0.5880]],\n",
            "\n",
            "        [[-0.7841,  0.5103, -0.7123,  ...,  0.4551, -1.2472, -1.3582],\n",
            "         [ 1.6247,  0.6809,  0.9142,  ..., -1.3674, -2.4849,  0.2200],\n",
            "         [-0.6558, -0.8193, -1.5838,  ...,  1.1271, -0.2513, -0.1312],\n",
            "         ...,\n",
            "         [ 1.4776, -0.4496, -0.0410,  ...,  0.1132, -1.0855, -0.1403],\n",
            "         [ 1.8932,  2.6573, -2.2112,  ...,  0.8830, -1.5188, -0.9296],\n",
            "         [-1.3853, -0.3081, -1.7223,  ..., -0.1373, -1.1963,  0.3070]]],\n",
            "       grad_fn=<NativeLayerNormBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIPIvk5G9oaj",
        "outputId": "d51292c9-d6d8-410f-d40e-ced00733859b"
      },
      "source": [
        "print(sentence_output1['last_hidden_state'].size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([25, 66, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dP7DH0IIwiW",
        "outputId": "876d4442-47a2-4169-8e45-0e0dfc761a13"
      },
      "source": [
        "sentence_embeds = []\r\n",
        "for sentence in sentence_output1['last_hidden_state']:\r\n",
        "  sentence_embeds.append(sentence[0])\r\n",
        "sentence_embeds_tensor = torch.unsqueeze(torch.stack(sentence_embeds),0)\r\n",
        "print(sentence_embeds_tensor.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 25, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7wpPbhJLdSh"
      },
      "source": [
        "masked_embeds = mask_input_embeddings(sentence_embeds_tensor, torch.zeros(1,25))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBMNpT1hPcyn",
        "outputId": "119d1e5a-622a-47c8-d767-ff85fcd6e8d7"
      },
      "source": [
        "masked_embeds[3].size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 25])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKO02fKp-t_V"
      },
      "source": [
        "document_output1 = document_model(attention_mask=torch.ones(1,25),inputs_embeds=masked_embeds[1],labels_embeddings=masked_embeds[2],labels_mask=masked_embeds[3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iToZaQyec2W",
        "outputId": "23ad1aa2-d965-45e0-8e95-8defd23b8c52"
      },
      "source": [
        "print(document_output1['last_hidden_state'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[-1.0028, -1.4851,  1.3859,  ..., -0.2413, -1.9556, -0.2141],\n",
            "         [ 0.4826,  1.0509,  0.1061,  ..., -0.7588, -0.0587,  0.7415],\n",
            "         [-0.5193,  0.2696,  2.3080,  ...,  0.8370, -0.3844,  0.6825],\n",
            "         ...,\n",
            "         [-0.3358,  0.5698,  2.2102,  ..., -0.3288, -1.3837, -0.9115],\n",
            "         [ 0.6643, -0.0458,  0.8573,  ..., -0.3380, -1.7469, -0.1620],\n",
            "         [ 0.5193,  0.2101,  1.2381,  ..., -0.0412, -1.2951, -0.0524]]],\n",
            "       grad_fn=<NativeLayerNormBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhbGeKQ-AJew",
        "outputId": "58012f75-ddd0-4006-ba4c-dbaed42d0ac5"
      },
      "source": [
        "for i in range(24):\r\n",
        "  print(document_output1['last_hidden_state'][0][i][0], \" with \", masked_embeds[3][0][i])\r\n",
        "  # SOME FUNKY OVERWRITE HAPPENING--> fixed"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(-1.0028, grad_fn=<SelectBackward>)  with  tensor(False)\n",
            "tensor(0.4826, grad_fn=<SelectBackward>)  with  tensor(True)\n",
            "tensor(-0.5193, grad_fn=<SelectBackward>)  with  tensor(False)\n",
            "tensor(0.4093, grad_fn=<SelectBackward>)  with  tensor(False)\n",
            "tensor(-0.2367, grad_fn=<SelectBackward>)  with  tensor(False)\n",
            "tensor(0.5782, grad_fn=<SelectBackward>)  with  tensor(True)\n",
            "tensor(-1.0138, grad_fn=<SelectBackward>)  with  tensor(False)\n",
            "tensor(-0.2971, grad_fn=<SelectBackward>)  with  tensor(False)\n",
            "tensor(-0.2792, grad_fn=<SelectBackward>)  with  tensor(False)\n",
            "tensor(-1.0838, grad_fn=<SelectBackward>)  with  tensor(False)\n",
            "tensor(-0.0171, grad_fn=<SelectBackward>)  with  tensor(True)\n",
            "tensor(-0.9826, grad_fn=<SelectBackward>)  with  tensor(False)\n",
            "tensor(-1.3130, grad_fn=<SelectBackward>)  with  tensor(False)\n",
            "tensor(-0.9423, grad_fn=<SelectBackward>)  with  tensor(False)\n",
            "tensor(-0.1768, grad_fn=<SelectBackward>)  with  tensor(False)\n",
            "tensor(0.2488, grad_fn=<SelectBackward>)  with  tensor(False)\n",
            "tensor(-0.3365, grad_fn=<SelectBackward>)  with  tensor(False)\n",
            "tensor(-1.0083, grad_fn=<SelectBackward>)  with  tensor(False)\n",
            "tensor(-0.3221, grad_fn=<SelectBackward>)  with  tensor(False)\n",
            "tensor(-0.4361, grad_fn=<SelectBackward>)  with  tensor(True)\n",
            "tensor(-0.1741, grad_fn=<SelectBackward>)  with  tensor(False)\n",
            "tensor(-0.1724, grad_fn=<SelectBackward>)  with  tensor(False)\n",
            "tensor(-0.3358, grad_fn=<SelectBackward>)  with  tensor(True)\n",
            "tensor(0.6643, grad_fn=<SelectBackward>)  with  tensor(False)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjjlTmQRMz5h"
      },
      "source": [
        "### 5.1.5 Perform Pretraining"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcC8fKiAuzx-"
      },
      "source": [
        "Possibly train here: https://www.rz.ifi.lmu.de/infos/slurm_de.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZunx0yb6v81"
      },
      "source": [
        "# TensorBoard of loss and all other sorts of important info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUDbAwcVUkqJ"
      },
      "source": [
        "### 5.1.6 Save Pretrained Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_Zdk-zSjp8X"
      },
      "source": [
        "Imagine not saving your model and losing all training progress #uff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9qJXR9fju4q"
      },
      "source": [
        "### 5.1.7 End-to-End Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVJ-Kqp2j0Ro"
      },
      "source": [
        "Write all the trained modules in such a compact way that we can just hand it a document or sentence and it will infer its representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nw1qyXh4rMm"
      },
      "source": [
        "class HATEModel(nn.Module):"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3s9HHqxQlUz"
      },
      "source": [
        "## 5.2 Finetuning on MS MARCO Document Ranking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcvB0xjtQtNE"
      },
      "source": [
        "### 5.2.1 Details on the MS MARCO Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-vDGYbWdz3l"
      },
      "source": [
        "- https://microsoft.github.io/msmarco (has list of other document reranking models)\n",
        "\n",
        "- MS MARCO: A Human Generated MAchine Reading COmprehension Dataset https://arxiv.org/pdf/1611.09268.pdf\n",
        "\n",
        "- https://microsoft.github.io/TREC-2020-Deep-Learning/\n",
        "\n",
        "\n",
        "\n",
        "also relevant for finetuning:\n",
        "\n",
        "- RepBERT: Contextualized Text Embeddings for First-Stage Retrieval https://arxiv.org/pdf/2006.15498.pdf\n",
        "- TwinBERT: Distilling Knowledge to Twin-Structured BERT Models for Efficient Retrieval (1) https://arxiv.org/pdf/2002.06275.pdf (2) https://github.com/deepampatel/TwinBert/blob/master/TwinBert.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyOv4ZUquCdd"
      },
      "source": [
        "Alternative Tasks:\n",
        "- https://research.google/tools/datasets/ Wikipedia and arXiv similarity triplets\n",
        "- https://github.com/LiqunW/Long-document-dataset with this paper: Long Document Classification From Local Word Glimpses via Recurrent Attention Learning https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8675939\n",
        "- https://datasets.quantumstat.com/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-to3CpsX02s"
      },
      "source": [
        "### 5.2.2 Load and Tokenize Finetuning Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlY-Oj9GX-v8"
      },
      "source": [
        "### 5.2.3 Document Ranking Task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22wOTsScwy40"
      },
      "source": [
        "- https://github.com/microsoft/MSMARCO-Document-Ranking\n",
        "\n",
        "- Baseline: Longformer: The Long-Document Transformer (1) https://arxiv.org/pdf/2004.05150.pdf (2) https://github.com/isekulic/longformer-marco/blob/master/src/TransformerMarco.py\n",
        "Baseline: Conformer-Kernel with Query Term Independence for Document Retrieval (1) https://arxiv.org/pdf/2007.10434.pdf "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnjX9LYNahuY"
      },
      "source": [
        "### 5.2.4 *TODO* \n",
        "- loss function for ranking task -> cosine contrastive loss? check this TwinBERT implementation https://github.com/deepampatel/TwinBert/blob/master/TwinBert.ipynb\n",
        "- what training objective is actually used in the finetuning here? is it already a ranking task? could the loss function be the same as in pretraining? \n",
        "- ranking algorithm for the document representations\n",
        "- Maybe train a cross-encoder like in the sentence transformer library?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5tLrkSmQn8_"
      },
      "source": [
        "# **6.** Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hw0eV70xRVyi"
      },
      "source": [
        "## 6.1 Document Ranking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJcXlFRuRYvp"
      },
      "source": [
        "## 6.2 Answer Passage Highlighting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hn4UdPhkaIH"
      },
      "source": [
        "## 6.3 Qualitative Study on Contextual Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsn-oha4yFWi"
      },
      "source": [
        "# **7.** Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBc__W4vyL5U"
      },
      "source": [
        "# **8.** Outlook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-KmTKws5rqj"
      },
      "source": [
        "- Deeper look at attention: transformers are graph neural networks https://thegradient.pub/transformers-are-graph-neural-networks/\n",
        "- Generalization to Hopfield Nets: (1) https://ml-jku.github.io/hopfield-layers/ (2) http://franksworld.com/2020/08/10/explaining-the-paper-hopfield-networks-is-all-you-need/ (3) https://analyticsindiamag.com/modern-hopfield-network-transformers-attention/ (4) https://towardsdatascience.com/hopfield-networks-are-useless-heres-why-you-should-learn-them-f0930ebeadcd\n",
        "- Implications of these findings for language representation?\n",
        "- Check http://nlp.seas.harvard.edu/code/ for more ideas\n",
        "- Reducing time-complexity of Attention"
      ]
    }
  ]
}