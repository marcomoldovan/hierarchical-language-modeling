{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hierarchical-transformer-based-document-encoder.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "kpwbDUkHtLu9",
        "-B019eaBuLMx",
        "oItvz3k-uFSp",
        "gQzW6fxmtWEN",
        "gmX65TyjwxM-",
        "wO88nttwP0hi",
        "TZLIP6O_KRQP",
        "Ll4GDtfJKg76",
        "ZYYMKT3ILK5j",
        "65pD0jHzthMl",
        "2Vx6Jhu7oRW_",
        "W8MVxIXFoYEI",
        "kmJT1yF3i9k8",
        "aFQ_HrliRfml",
        "ue_TauSD2gWp",
        "17qoRvtU2oEL",
        "GGoLVfY2i1P8",
        "oyPDLz_kN7uE",
        "ah6qLiazN-xP",
        "7IVJdy-_qAPt",
        "aTTNBPu6MMsX",
        "1-smV5mIMXzj",
        "_DriQikaMnxv",
        "f3s9HHqxQlUz",
        "v5tLrkSmQn8_",
        "XBc__W4vyL5U"
      ],
      "mount_file_id": "1b4pp9mpqeumjRnUEMkhywInYeic480uk",
      "authorship_tag": "ABX9TyPMEu2OLAhZQoJlwaEcM/vZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9084e13a33a742b2a64621c670702cd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4464e8f5cd4441ad99fcc4ffb8ff5cb0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_21f7a072271b4cb6b1645f9d854091b9",
              "IPY_MODEL_11d96d057ca942fd88f1c90685fccd05"
            ]
          }
        },
        "4464e8f5cd4441ad99fcc4ffb8ff5cb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "21f7a072271b4cb6b1645f9d854091b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_990b6005830f40bf80f5554754d84175",
            "_dom_classes": [],
            "description": "Downloading: ",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 4417,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4417,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c0bbac2304a048a6a88466817cb8d541"
          }
        },
        "11d96d057ca942fd88f1c90685fccd05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_91c9937c10f648b78e03551e9d312f9c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 13.1k/? [00:00&lt;00:00, 60.0kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c8ff3760b2e24756b301db0da9b7b320"
          }
        },
        "990b6005830f40bf80f5554754d84175": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c0bbac2304a048a6a88466817cb8d541": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "91c9937c10f648b78e03551e9d312f9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c8ff3760b2e24756b301db0da9b7b320": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9a25a7c119464465ad2133f17a382fe3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6ad73c54c76847c691a0e76ac05e219b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b01fd112d5be435a82b4f5a091517512",
              "IPY_MODEL_7e5bd73272384ac4b1e366b00607246b"
            ]
          }
        },
        "6ad73c54c76847c691a0e76ac05e219b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b01fd112d5be435a82b4f5a091517512": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_164e2fa662904f0cb2ac2817040dc91e",
            "_dom_classes": [],
            "description": "Downloading: ",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 6866,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 6866,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d57cc71604b24e368e56df78cd73a337"
          }
        },
        "7e5bd73272384ac4b1e366b00607246b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d82b0c33bad049898cc5e25d6fe742e2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 28.4k/? [00:00&lt;00:00, 296kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_319d4d2ea23349b9bf4776bff4e16ebd"
          }
        },
        "164e2fa662904f0cb2ac2817040dc91e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d57cc71604b24e368e56df78cd73a337": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d82b0c33bad049898cc5e25d6fe742e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "319d4d2ea23349b9bf4776bff4e16ebd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bb2f480dd63d4e64927598d8de2fc2df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a9c4f2e220344daaa57477ff0159a8a5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fbca6f1e03754ede9b2a02c14c2f8fd3",
              "IPY_MODEL_18ec7824a4f1464e83f4463814e1f9d9"
            ]
          }
        },
        "a9c4f2e220344daaa57477ff0159a8a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fbca6f1e03754ede9b2a02c14c2f8fd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_192fc5e053fb43e787de9145754afb04",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 14554,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 14554,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b21e3048cfdc4c04a5f2071e7431a4ea"
          }
        },
        "18ec7824a4f1464e83f4463814e1f9d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e32906b99e8d4f629e0bf5dc094f36c4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 14.6k/14.6k [05:20&lt;00:00, 45.3B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_444b1e20a8d24a238e1c47b356f68887"
          }
        },
        "192fc5e053fb43e787de9145754afb04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b21e3048cfdc4c04a5f2071e7431a4ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e32906b99e8d4f629e0bf5dc094f36c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "444b1e20a8d24a238e1c47b356f68887": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4610af6240da4e24a439f42f8d5d41e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_243cd8874a3e4a20a65d76a7931d6d16",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e14ce6f62b9f4ec199fd0d4834169b8a",
              "IPY_MODEL_1f12992a710c4b88b3ca97110355070b"
            ]
          }
        },
        "243cd8874a3e4a20a65d76a7931d6d16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e14ce6f62b9f4ec199fd0d4834169b8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6b2b592a12a04d84842be4f74285498a",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 18307873280,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 18307873280,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_38bf115b867a4cc498a045fd114eedff"
          }
        },
        "1f12992a710c4b88b3ca97110355070b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_57a5fd856b7c409b8ba5bdbf5e7ef2db",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 18.3G/18.3G [05:20&lt;00:00, 57.1MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_81a298dfb17345ac8ef583a7286372c9"
          }
        },
        "6b2b592a12a04d84842be4f74285498a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "38bf115b867a4cc498a045fd114eedff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "57a5fd856b7c409b8ba5bdbf5e7ef2db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "81a298dfb17345ac8ef583a7286372c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcomoldovan/hierarchical-text-encoder/blob/master/hierarchical_transformer_based_document_encoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz5UHkyUWWrx"
      },
      "source": [
        "# **Hierarchical Transformer-based Tocument Encoder** \n",
        "\n",
        "We present a multi-purpose hierarchical tocument encoder (HATE) based on stacking sentence-level and document-level transformers on top of each other. By doing this we try to capture as many semantic facets of a given, arbitrarily long document. The model implicitly learns contextualized word and sentence representations as well a robust document representation. All encodings live in the same representation space and therefore similarity metrics such as cosine similarity can be applied on all levels of representations interchangeably. This has the purpose that only one model would have to be trained in order to encode both a query and a candidate document as well as all its contextualized sentences in one representation space where similarity measures could be applied in order to retrieve most relevant documents as well as scoring answer passages candidates by their likelihood of relevancy. Due to the robust sentence embeddings that are also produced, the model could be used for document segmentation as well: since intuitively there would be a semantic break between paragraphs this would also be reflected in the representations of the sentences. One could apply an algorithms that separates unstructured texts into paragraphs at points where the likelihood of a semantic break, and therefore the start of a separate paragraph is high."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6LstRZsO_YI"
      },
      "source": [
        "# **1.** Introduction\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpwbDUkHtLu9"
      },
      "source": [
        "# **2.** Background"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-B019eaBuLMx"
      },
      "source": [
        "## 2.1 Statistical foundations of machine learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRjb0HdQRGPW"
      },
      "source": [
        "(1) https://towardsdatascience.com/the-statistical-foundations-of-machine-learning-973c356a95f\n",
        "\n",
        "More on regression, classification, etc. in a probabilistic context. Show that these problems are essentially MAP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oItvz3k-uFSp"
      },
      "source": [
        "## 2.2 Mathematics of optimization for deep learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACsq9qRNRK7V"
      },
      "source": [
        "(1) https://towardsdatascience.com/the-mathematics-of-optimization-for-deep-learning-11af2b1fda30\n",
        "\n",
        "- Optimization: (1) visualizing loss landscape https://arxiv.org/pdf/1712.09913.pdf\n",
        "- Momentum based optimizers\n",
        "- Dropout (1) https://arxiv.org/pdf/1207.0580.pdf\n",
        "- Batch normalization (1) https://arxiv.org/abs/1502.03167 (2) https://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf\n",
        "- Weight initialization\n",
        "- Reguralization (1) NLP specific: https://mlexplained.com/2018/03/02/regularization-techniques-for-natural-language-processing-with-code-examples/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQzW6fxmtWEN"
      },
      "source": [
        "## 2.3 Representation Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejoRhg9wWkOK"
      },
      "source": [
        "Touch briefly on the theory of representation learning, independent of language. Main focus on the Bengio paper: https://arxiv.org/pdf/1206.5538.pdf Also refer to representation learning slides from DL&AI course from last semester as an appropriate introduction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmX65TyjwxM-"
      },
      "source": [
        "## 2.4 Language Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxwbJYXIQUpq"
      },
      "source": [
        "- Language modeling via auto-encoding or auto-regressive methods in general\n",
        "\n",
        "\n",
        "- Embeddings in Language Models (1) https://jalammar.github.io/skipgram-recommender-talk/Text (2) https://dspace.mit.edu/handle/1721.1/118079\n",
        "-  Word embeddings (1) https://ruder.io/word-embeddings-1/index.html (2) https://ruder.io/word-embeddings-softmax/index.html (3) https://ruder.io/secret-word2vec/index.html (4) https://ruder.io/word-embeddings-2017/index.html (5) https://jalammar.github.io/illustrated-word2vec/ (6) Glove https://mlexplained.com/2018/04/29/paper-dissected-glove-global-vectors-for-word-representation-explained/ (7) ELMo https://mlexplained.com/2018/06/15/paper-dissected-deep-contextualized-word-representations-explained/ (8) https://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html (9) https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html#loss-functions\n",
        "- Sentence embeddings (1) https://supernlp.github.io/2018/11/26/sentreps/ (2) https://mlexplained.com/2017/12/28/an-overview-of-sentence-embedding-methods/ (3) https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a\n",
        "- Document Embeddings (1) https://towardsdatascience.com/document-embedding-techniques-fed3e7a6a25d#1409 (2) https://graphaware.com/nlp/2018/09/03/advanced-document-representation.html\n",
        "\n",
        "- General early language models that are based on DL: RNNs/LSTM (touch shortly) (1) https://arxiv.org/pdf/1312.6026.pdf (2) https://distill.pub/2019/memorization-in-rnns/\n",
        "- On the diffictuly of training recurrent neural networks https://arxiv.org/pdf/1211.5063.pdf\n",
        "- Transition to attention mechanism, at first in RNNs\n",
        "- Why these large DL-based models are so important for transfer learning in NLP (1) https://ruder.io/transfer-learning/ (2) https://thegradient.pub/nlp-imagenet/ (3) very linguistic study of word embeddings for transfer tasks https://arxiv.org/pdf/1903.08855.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wO88nttwP0hi"
      },
      "source": [
        "# **3.** Related Work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZLIP6O_KRQP"
      },
      "source": [
        "## 3.1 Attention and why it's all you need"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACcPMJQLOvQU"
      },
      "source": [
        "- Attention is all you need (1) https://mlexplained.com/2017/12/29/attention-is-all-you-need-explained/ (2) https://arxiv.org/pdf/1706.03762.pdf (3) https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec (4) https://blog.floydhub.com/attention-mechanism/ (5) https://jalammar.github.io/illustrated-transformer/ (6) https://distill.pub/2016/augmented-rnns/ (7) https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html\n",
        "- BERT (1) https://mlexplained.com/2019/01/07/paper-dissected-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-explained/ (2) https://arxiv.org/pdf/1810.04805.pdf (3) https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/ (4) https://jalammar.github.io/illustrated-bert/\n",
        "- What Does BERT Look At? An Analysis of BERT’s Attention \n",
        "https://arxiv.org/pdf/1906.04341.pdf https://arxiv.org/abs/1909.10430v2\n",
        "- BERT raw embeddings https://arxiv.org/abs/1909.00512v1 https://towardsdatascience.com/examining-berts-raw-embeddings-fd905cb22df7\n",
        "- Sentence embeddings revisited: BERT methods -> transition to intuition for my document representation model\n",
        "- Call-back to section 2.4 where I touch on classical embeddings and compare to deep, high-parameter, attention based models such as BERT\n",
        "- Why these large pretrained models are so important for transfer learning in NLP (1) https://ruder.io/transfer-learning/ (2) https://thegradient.pub/nlp-imagenet/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ll4GDtfJKg76"
      },
      "source": [
        "## 3.2 Hierarchical Attention-Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R44qFjnbj6o5"
      },
      "source": [
        "- SMITH (1) https://arxiv.org/pdf/2004.12297v1.pdf (2) https://github.com/dmolony3/SMITH\n",
        "- HIBERT (1) https://arxiv.org/pdf/1905.06566.pdf (2) https://github.com/liangsi03/hibert_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYYMKT3ILK5j"
      },
      "source": [
        "## 3.3 Information Retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhoR7CDFbCS2"
      },
      "source": [
        "- TANDA: Transfer and Adapt Pre-Trained Transformer Modelsfor Answer Sentence Selection https://arxiv.org/pdf/1911.04118.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmWFH0wZQMeF"
      },
      "source": [
        "# **4.** The Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBIOm4PPUx39"
      },
      "source": [
        "Introduce all necessary top-level imports and install the transformer module from huggingface"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hv0fZAmgRgTt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9bc5579-782d-48e9-e4e1-09c977475765"
      },
      "source": [
        "!pip install transformers #installs transformer module from huggingface\n",
        "!pip install datasets #installs dataset module from huggingface\n",
        "!pip install tokenizers #installs tokenizer module from huggingface\n",
        "\n",
        "import re\n",
        "import math\n",
        "\n",
        "import nltk\n",
        "from nltk import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as f\n",
        "import torchtext.datasets\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import transformers\n",
        "from transformers import BertConfig, BertModel, BertForMaskedLM\n",
        "from transformers import configuration_utils\n",
        "from transformers.activations import ACT2FN \n",
        "from transformers.file_utils import ModelOutput\n",
        "from transformers.modeling_utils import PreTrainedModel, apply_chunking_to_forward\n",
        "from transformers.configuration_utils import PretrainedConfig\n",
        "\n",
        "\n",
        "from pprint import pprint\n",
        "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.4)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.6/dist-packages (1.1.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.6/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from datasets) (1.19.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.6/dist-packages (from datasets) (2.0.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from datasets) (0.8)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.6/dist-packages (0.9.4)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmWmUTykN22p"
      },
      "source": [
        "## 4.1 Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65pD0jHzthMl"
      },
      "source": [
        "#### Internal Embedding Lookup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqn5_6P2ezri"
      },
      "source": [
        "class EmbeddingsLookup(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    # Initialize the lookup matrix for input IDs, positional embeddings and token types\n",
        "    self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
        "    self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "    self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
        "    \n",
        "    # Adds to Layer Normalization and Dropout on inital word embeddings\n",
        "    self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "    self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n",
        "    self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
        "    self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
        "\n",
        "  def forward(self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None):\n",
        "    if input_ids is not None:\n",
        "      input_shape = input_ids.size()\n",
        "    else:\n",
        "      input_shape = inputs_embeds.size()[:-1]\n",
        "\n",
        "    seq_length = input_shape[1]\n",
        "\n",
        "    if position_ids is None:\n",
        "      position_ids = self.position_ids[:, :seq_length]\n",
        "\n",
        "    if token_type_ids is None:\n",
        "      token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n",
        "\n",
        "    if inputs_embeds is None:\n",
        "      inputs_embeds = self.word_embeddings(input_ids)\n",
        "    token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "\n",
        "    embeddings = inputs_embeds + token_type_embeddings\n",
        "    if self.position_embedding_type == \"absolute\":\n",
        "      position_embeddings = self.position_embeddings(position_ids)\n",
        "      embeddings += position_embeddings\n",
        "    embeddings = self.LayerNorm(embeddings)\n",
        "    embeddings = self.dropout(embeddings)\n",
        "    return embeddings"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Vx6Jhu7oRW_"
      },
      "source": [
        "#### Encoder Stack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VOKcXtf4gml"
      },
      "source": [
        "class BaseModelOutputWithCrossAttentions(ModelOutput):\r\n",
        "    \"\"\"\r\n",
        "    Base class for model's outputs, with potential hidden states and attentions.\r\n",
        "    Args:\r\n",
        "        last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`):\r\n",
        "            Sequence of hidden-states at the output of the last layer of the model.\r\n",
        "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\r\n",
        "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\r\n",
        "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\r\n",
        "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\r\n",
        "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\r\n",
        "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\r\n",
        "            sequence_length, sequence_length)`.\r\n",
        "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\r\n",
        "            heads.\r\n",
        "        cross_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` and ``config.add_cross_attention=True`` is passed or when ``config.output_attentions=True``):\r\n",
        "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\r\n",
        "            sequence_length, sequence_length)`.\r\n",
        "            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\r\n",
        "            weighted average in the cross-attention heads.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    last_hidden_state: torch.FloatTensor = None\r\n",
        "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\r\n",
        "    attentions: Optional[Tuple[torch.FloatTensor]] = None\r\n",
        "    cross_attentions: Optional[Tuple[torch.FloatTensor]] = None"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AICiXa9JfZ59"
      },
      "source": [
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
        "      raise ValueError(\n",
        "        \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "        \"heads (%d)\" % (config.hidden_size, config.num_attention_heads)\n",
        "      )\n",
        "\n",
        "    self.num_attention_heads = config.num_attention_heads\n",
        "    self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "    self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "    self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "    self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "    self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "    self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "    self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
        "    if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
        "      self.max_position_embeddings = config.max_position_embeddings\n",
        "      self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n",
        "\n",
        "  def transpose_for_scores(self, x):\n",
        "    new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "    x = x.view(*new_x_shape)\n",
        "    return x.permute(0, 2, 1, 3)\n",
        "\n",
        "  def forward(self,\n",
        "              hidden_states,\n",
        "              attention_mask=None,\n",
        "              head_mask=None,\n",
        "              encoder_hidden_states=None,\n",
        "              encoder_attention_mask=None,\n",
        "              output_attentions=False):\n",
        "    \n",
        "    mixed_query_layer = self.query(hidden_states)\n",
        "\n",
        "    # If this is instantiated as a cross-attention module, the keys\n",
        "    # and values come from an encoder; the attention mask needs to be\n",
        "    # such that the encoder's padding tokens are not attended to.\n",
        "    if encoder_hidden_states is not None:\n",
        "        mixed_key_layer = self.key(encoder_hidden_states)\n",
        "        mixed_value_layer = self.value(encoder_hidden_states)\n",
        "        attention_mask = encoder_attention_mask\n",
        "    else:\n",
        "        mixed_key_layer = self.key(hidden_states)\n",
        "        mixed_value_layer = self.value(hidden_states)\n",
        "\n",
        "    query_layer = self.transpose_for_scores(mixed_query_layer)    \n",
        "    key_layer = self.transpose_for_scores(mixed_key_layer)\n",
        "    value_layer = self.transpose_for_scores(mixed_value_layer)\n",
        "\n",
        "    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "\n",
        "    if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
        "      seq_length = hidden_states.size()[1]\n",
        "      position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n",
        "      position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n",
        "      distance = position_ids_l - position_ids_r\n",
        "      positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n",
        "      positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n",
        "\n",
        "      if self.position_embedding_type == \"relative_key\":\n",
        "        relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
        "        attention_scores = attention_scores + relative_position_scores\n",
        "      elif self.position_embedding_type == \"relative_key_query\":\n",
        "        relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
        "        relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n",
        "        attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n",
        "\n",
        "    attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "    if attention_mask is not None:\n",
        "      # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
        "      attention_scores = attention_scores + attention_mask\n",
        "\n",
        "    # Normalize the attention scores to probabilities.\n",
        "    attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "\n",
        "    # This is actually dropping out entire tokens to attend to, which might\n",
        "    # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "    attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "    # Mask heads if we want to\n",
        "    if head_mask is not None:\n",
        "      attention_probs = attention_probs * head_mask\n",
        "\n",
        "    context_layer = torch.matmul(attention_probs, value_layer)\n",
        "\n",
        "    context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "    new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "    context_layer = context_layer.view(*new_context_layer_shape)\n",
        "\n",
        "    outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
        "    return outputs\n",
        "  \n",
        "\n",
        "class SelfOutput(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "    self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "    self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "  def forward(self, hidden_states, input_tensor):\n",
        "    hidden_states = self.dense(hidden_states)\n",
        "    hidden_states = self.dropout(hidden_states)\n",
        "    hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "    return hidden_states\n",
        "\n",
        "\n",
        "class AttentionModule(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.self = SelfAttention(config)\n",
        "    self.output = SelfOutput(config)\n",
        "    self.pruned_heads = set()\n",
        "\n",
        "  def prune_head(self, heads):\n",
        "    if len(heads) == 0:\n",
        "      return\n",
        "    heads, index = find_pruneable_heads_and_indices(                            # don't foget this!!\n",
        "      heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n",
        "    )\n",
        "    # Prune linear layers\n",
        "    self.self.query = prune_linear_layer(self.self.query, index)\n",
        "    self.self.key = prune_linear_layer(self.self.key, index)\n",
        "    self.self.value = prune_linear_layer(self.self.value, index)\n",
        "    self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n",
        "\n",
        "    # Update hyper params and store pruned heads\n",
        "    self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n",
        "    self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n",
        "    self.pruned_heads = self.pruned_heads.union(heads)\n",
        "\n",
        "  def forward(self,\n",
        "              hidden_states,\n",
        "              attention_mask=None,\n",
        "              head_mask=None,\n",
        "              encoder_hidden_states=None,\n",
        "              encoder_attention_mask=None,\n",
        "              output_attentions=False):\n",
        "    self_outputs = self.self(\n",
        "            hidden_states,\n",
        "            attention_mask,\n",
        "            head_mask,\n",
        "            encoder_hidden_states,\n",
        "            encoder_attention_mask,\n",
        "            output_attentions)\n",
        "    attention_output = self.output(self_outputs[0], hidden_states)\n",
        "    outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
        "    return outputs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "    if isinstance(config.hidden_act, str):\n",
        "      self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
        "    else:\n",
        "      self.intermediate_act_fn = config.hidden_act\n",
        "\n",
        "  def forward(self, hidden_states):\n",
        "    hidden_states = self.dense(hidden_states)\n",
        "    hidden_states = self.intermediate_act_fn(hidden_states)\n",
        "    return hidden_states\n",
        "\n",
        "\n",
        "class Output(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "    self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "    self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "  def forward(self, hidden_states, input_tensor):\n",
        "    hidden_states = self.dense(hidden_states)\n",
        "    hidden_states = self.dropout(hidden_states)\n",
        "    hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "    return hidden_states\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class EncoderLayer (nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
        "    self.seq_len_dim = 1\n",
        "    self.attention = AttentionModule(config)\n",
        "    self.is_decoder = config.is_decoder\n",
        "    self.add_cross_attention = config.add_cross_attention\n",
        "    if self.add_cross_attention:\n",
        "      assert self.is_decoder, f\"{self} should be used as a decoder model if cross attention is added\"\n",
        "      self.crossattention = AttentionModule(config)\n",
        "    self.intermediate = FeedForward(config)\n",
        "    self.output = Output(config)\n",
        "\n",
        "  def forward(self,\n",
        "              hidden_states,\n",
        "              attention_mask=None,\n",
        "              head_mask=None,\n",
        "              encoder_hidden_states=None,\n",
        "              encoder_attention_mask=None,\n",
        "              output_attentions=False):\n",
        "    \n",
        "    self_attention_outputs = self.attention(hidden_states,\n",
        "                                            attention_mask,\n",
        "                                            head_mask,\n",
        "                                            output_attentions=output_attentions)\n",
        "    attention_output = self_attention_outputs[0]\n",
        "    outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
        "    if self.is_decoder and encoder_hidden_states is not None:\n",
        "      assert hasattr(self, \"crossattention\"), f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers by setting `config.add_cross_attention=True`\"\n",
        "      cross_attention_outputs = self.crossattention(attention_output,\n",
        "                                                    attention_mask,\n",
        "                                                    head_mask,\n",
        "                                                    encoder_hidden_states,\n",
        "                                                    encoder_attention_mask,\n",
        "                                                    output_attentions)\n",
        "      attention_output = cross_attention_outputs[0]\n",
        "      outputs = outputs + cross_attention_outputs[1:]  # add cross attentions if we output attention weights\n",
        "\n",
        "    layer_output = apply_chunking_to_forward(self.feed_forward_chunk,           # don't forget this!!\n",
        "                                             self.chunk_size_feed_forward, \n",
        "                                             self.seq_len_dim, attention_output)\n",
        "      \n",
        "    outputs = (layer_output,) + outputs\n",
        "    return outputs\n",
        "\n",
        "  def feed_forward_chunk(self, attention_output):\n",
        "    intermediate_output = self.intermediate(attention_output)\n",
        "    layer_output = self.output(intermediate_output, attention_output)\n",
        "    return layer_output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class EncoderStack(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.layer = nn.ModuleList([EncoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "  def forward(self,\n",
        "              hidden_states,\n",
        "              attention_mask=None,\n",
        "              head_mask=None,\n",
        "              encoder_hidden_states=None,\n",
        "              encoder_attention_mask=None,\n",
        "              output_attentions=False,\n",
        "              output_hidden_states=False,\n",
        "              return_dict=True):\n",
        "    all_hidden_states = () if output_hidden_states else None\n",
        "    all_self_attentions = () if output_attentions else None\n",
        "    all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
        "    for i, layer_module in enumerate(self.layer):\n",
        "      if output_hidden_states:\n",
        "        all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "      \n",
        "      layer_head_mask = head_mask[i] if head_mask is not None else None\n",
        "\n",
        "      if getattr(self.config, \"gradient_checkpointing\", False):\n",
        "        def create_custom_forward(module):\n",
        "          def custom_forward(*inputs):\n",
        "            return module(*inputs, output_attentions)\n",
        "          return custom_forward\n",
        "        layer_outputs = torch.utils.checkpoint.checkpoint(create_custom_forward(layer_module),\n",
        "                                                          hidden_states,\n",
        "                                                          attention_mask,\n",
        "                                                          layer_head_mask,\n",
        "                                                          encoder_hidden_states,\n",
        "                                                          encoder_attention_mask,)\n",
        "      else:\n",
        "        layer_outputs = layer_module(hidden_states,\n",
        "                                     attention_mask,\n",
        "                                     layer_head_mask,\n",
        "                                     encoder_hidden_states,\n",
        "                                     encoder_attention_mask,\n",
        "                                     output_attentions,)\n",
        "      \n",
        "      hidden_states = layer_outputs[0]\n",
        "      if output_attentions:\n",
        "        all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
        "        if self.config.add_cross_attention:\n",
        "          all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
        "\n",
        "    if output_hidden_states:\n",
        "      all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "    if not return_dict:\n",
        "      return tuple(v\n",
        "                   for v in [hidden_states, all_hidden_states, all_self_attentions, all_cross_attentions]\n",
        "                   if v is not None)\n",
        "    \n",
        "    return BaseModelOutputWithCrossAttentions(last_hidden_state=hidden_states,\n",
        "                                              hidden_states=all_hidden_states,\n",
        "                                              attentions=all_self_attentions,\n",
        "                                              cross_attentions=all_cross_attentions,)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7REE2kpgFBV"
      },
      "source": [
        "class Pooler(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "    self.activation = nn.Tanh()\n",
        "\n",
        "  def forward(self, hidden_states):\n",
        "    # We \"pool\" the model by simply taking the hidden state corresponding\n",
        "    # to the first token.\n",
        "    first_token_tensor = hidden_states[:, 0]\n",
        "    pooled_output = self.dense(first_token_tensor)\n",
        "    pooled_output = self.activation(pooled_output)\n",
        "    return pooled_output"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8MVxIXFoYEI"
      },
      "source": [
        "#### Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8AsEjOE9tOo"
      },
      "source": [
        "class BaseModelOutputWithPoolingAndCrossAttentions(ModelOutput):\r\n",
        "  \"\"\"\r\n",
        "  Base class for model's outputs that also contains a pooling of the last hidden states.\r\n",
        "  Args:\r\n",
        "    last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`):\r\n",
        "      Sequence of hidden-states at the output of the last layer of the model.\r\n",
        "    pooler_output (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, hidden_size)`):\r\n",
        "      Last layer hidden-state of the first token of the sequence (classification token) further processed by a\r\n",
        "      Linear layer and a Tanh activation function. The Linear layer weights are trained from the next sentence\r\n",
        "      prediction (classification) objective during pretraining.\r\n",
        "    hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\r\n",
        "      Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\r\n",
        "      of shape :obj:`(batch_size, sequence_length, hidden_size)`.\r\n",
        "      Hidden-states of the model at the output of each layer plus the initial embedding outputs.\r\n",
        "    attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\r\n",
        "      Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\r\n",
        "      sequence_length, sequence_length)`.\r\n",
        "      Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\r\n",
        "      heads.\r\n",
        "    cross_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` and ``config.add_cross_attention=True`` is passed or when ``config.output_attentions=True``):\r\n",
        "      Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\r\n",
        "      sequence_length, sequence_length)`.\r\n",
        "      Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\r\n",
        "      weighted average in the cross-attention heads.\r\n",
        "  \"\"\"\r\n",
        "  last_hidden_state: torch.FloatTensor = None\r\n",
        "  pooler_output: torch.FloatTensor = None\r\n",
        "  hidden_states: Optional[Tuple[torch.FloatTensor]] = None\r\n",
        "  attentions: Optional[Tuple[torch.FloatTensor]] = None\r\n",
        "  cross_attentions: Optional[Tuple[torch.FloatTensor]] = None"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXxk31lYWr0Y"
      },
      "source": [
        "class TransformerPreTrainedModel(PreTrainedModel):\r\n",
        "  \"\"\"\r\n",
        "  An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\r\n",
        "  models.\r\n",
        "  \"\"\"\r\n",
        "  _keys_to_ignore_on_load_missing = [r\"position_ids\"]\r\n",
        "\r\n",
        "  def _init_weights(self, module):\r\n",
        "    \"\"\" Initialize the weights \"\"\"\r\n",
        "    if isinstance(module, (nn.Linear, nn.Embedding)):\r\n",
        "      # Slightly different from the TF version which uses truncated_normal for initialization\r\n",
        "      # cf https://github.com/pytorch/pytorch/pull/5617\r\n",
        "      module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\r\n",
        "    elif isinstance(module, nn.LayerNorm):\r\n",
        "      module.bias.data.zero_()\r\n",
        "      module.weight.data.fill_(1.0)\r\n",
        "    if isinstance(module, nn.Linear) and module.bias is not None:\r\n",
        "      module.bias.data.zero_()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lcr5I4EZcWdu"
      },
      "source": [
        "class TransformerConfig(PretrainedConfig):\r\n",
        "    \"\"\"\r\n",
        "    This is the configuration class to store the configuration of a TransformerModel. \r\n",
        "    It is used to instantiate a BERT model according to the specified arguments,\r\n",
        "    defining the model architecture. Instantiating a configuration with the defaults \r\n",
        "    will yield a similar configuration to that of the BERT `bert-base-uncased \r\n",
        "    <https://huggingface.co/bert-base-uncased>`__ architecture. Configuration objects \r\n",
        "    inherit from :class:`~transformers.PretrainedConfig` and can be used to control \r\n",
        "    the model outputs. Read the documentation from :class:`~transformers.PretrainedConfig` \r\n",
        "    for more information.\r\n",
        "    Args:\r\n",
        "        vocab_size (:obj:`int`, `optional`, defaults to 30522):\r\n",
        "            Vocabulary size of the BERT model. Defines the number of different tokens that can be represented by the\r\n",
        "            :obj:`inputs_ids` passed when calling :class:`~transformers.BertModel` or\r\n",
        "            :class:`~transformers.TFBertModel`.\r\n",
        "        hidden_size (:obj:`int`, `optional`, defaults to 768):\r\n",
        "            Dimensionality of the encoder layers and the pooler layer.\r\n",
        "        num_hidden_layers (:obj:`int`, `optional`, defaults to 12):\r\n",
        "            Number of hidden layers in the Transformer encoder.\r\n",
        "        num_attention_heads (:obj:`int`, `optional`, defaults to 12):\r\n",
        "            Number of attention heads for each attention layer in the Transformer encoder.\r\n",
        "        intermediate_size (:obj:`int`, `optional`, defaults to 3072):\r\n",
        "            Dimensionality of the \"intermediate\" (often named feed-forward) layer in the Transformer encoder.\r\n",
        "        hidden_act (:obj:`str` or :obj:`Callable`, `optional`, defaults to :obj:`\"gelu\"`):\r\n",
        "            The non-linear activation function (function or string) in the encoder and pooler. If string,\r\n",
        "            :obj:`\"gelu\"`, :obj:`\"relu\"`, :obj:`\"silu\"` and :obj:`\"gelu_new\"` are supported.\r\n",
        "        hidden_dropout_prob (:obj:`float`, `optional`, defaults to 0.1):\r\n",
        "            The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\r\n",
        "        attention_probs_dropout_prob (:obj:`float`, `optional`, defaults to 0.1):\r\n",
        "            The dropout ratio for the attention probabilities.\r\n",
        "        max_position_embeddings (:obj:`int`, `optional`, defaults to 512):\r\n",
        "            The maximum sequence length that this model might ever be used with. Typically set this to something large\r\n",
        "            just in case (e.g., 512 or 1024 or 2048).\r\n",
        "        type_vocab_size (:obj:`int`, `optional`, defaults to 2):\r\n",
        "            The vocabulary size of the :obj:`token_type_ids` passed when calling :class:`~transformers.BertModel` or\r\n",
        "            :class:`~transformers.TFBertModel`.\r\n",
        "        initializer_range (:obj:`float`, `optional`, defaults to 0.02):\r\n",
        "            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\r\n",
        "        layer_norm_eps (:obj:`float`, `optional`, defaults to 1e-12):\r\n",
        "            The epsilon used by the layer normalization layers.\r\n",
        "        gradient_checkpointing (:obj:`bool`, `optional`, defaults to :obj:`False`):\r\n",
        "            If True, use gradient checkpointing to save memory at the expense of slower backward pass.\r\n",
        "        position_embedding_type (:obj:`str`, `optional`, defaults to :obj:`\"absolute\"`):\r\n",
        "            Type of position embedding. Choose one of :obj:`\"absolute\"`, :obj:`\"relative_key\"`,\r\n",
        "            :obj:`\"relative_key_query\"`. For positional embeddings use :obj:`\"absolute\"`. For more information on\r\n",
        "            :obj:`\"relative_key\"`, please refer to `Self-Attention with Relative Position Representations (Shaw et al.)\r\n",
        "            <https://arxiv.org/abs/1803.02155>`__. For more information on :obj:`\"relative_key_query\"`, please refer to\r\n",
        "            `Method 4` in `Improve Transformer Models with Better Relative Position Embeddings (Huang et al.)\r\n",
        "            <https://arxiv.org/abs/2009.13658>`__.\r\n",
        "    Examples::\r\n",
        "        >>> from transformers import BertModel, BertConfig\r\n",
        "        >>> # Initializing a BERT bert-base-uncased style configuration\r\n",
        "        >>> configuration = BertConfig()\r\n",
        "        >>> # Initializing a model from the bert-base-uncased style configuration\r\n",
        "        >>> model = BertModel(configuration)\r\n",
        "        >>> # Accessing the model configuration\r\n",
        "        >>> configuration = model.config\r\n",
        "    \"\"\"\r\n",
        "    model_type = \"bert\"\r\n",
        "\r\n",
        "    def __init__(\r\n",
        "        self,\r\n",
        "        vocab_size=30522,\r\n",
        "        hidden_size=768,\r\n",
        "        num_hidden_layers=12,\r\n",
        "        num_attention_heads=12,\r\n",
        "        intermediate_size=3072,\r\n",
        "        hidden_act=\"gelu\",\r\n",
        "        hidden_dropout_prob=0.1,\r\n",
        "        attention_probs_dropout_prob=0.1,\r\n",
        "        max_position_embeddings=512,\r\n",
        "        type_vocab_size=2,\r\n",
        "        initializer_range=0.02,\r\n",
        "        layer_norm_eps=1e-12,\r\n",
        "        pad_token_id=0,\r\n",
        "        gradient_checkpointing=False,\r\n",
        "        position_embedding_type=\"absolute\",\r\n",
        "        **kwargs\r\n",
        "    ):\r\n",
        "        super().__init__(pad_token_id=pad_token_id, **kwargs)\r\n",
        "\r\n",
        "        self.vocab_size = vocab_size\r\n",
        "        self.hidden_size = hidden_size\r\n",
        "        self.num_hidden_layers = num_hidden_layers\r\n",
        "        self.num_attention_heads = num_attention_heads\r\n",
        "        self.hidden_act = hidden_act\r\n",
        "        self.intermediate_size = intermediate_size\r\n",
        "        self.hidden_dropout_prob = hidden_dropout_prob\r\n",
        "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\r\n",
        "        self.max_position_embeddings = max_position_embeddings\r\n",
        "        self.type_vocab_size = type_vocab_size\r\n",
        "        self.initializer_range = initializer_range\r\n",
        "        self.layer_norm_eps = layer_norm_eps\r\n",
        "        self.gradient_checkpointing = gradient_checkpointing\r\n",
        "        self.position_embedding_type = position_embedding_type"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6A_DB9Tf1aP4"
      },
      "source": [
        "class TransformerBase(TransformerPreTrainedModel):\n",
        "  \"\"\"\n",
        "  The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n",
        "  cross-attention is added between the self-attention layers, following the architecture described in `Attention is\n",
        "  all you need <https://arxiv.org/abs/1706.03762>`__ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\n",
        "  Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.\n",
        "  To behave as an decoder the model needs to be initialized with the :obj:`is_decoder` argument of the configuration\n",
        "  set to :obj:`True`. To be used in a Seq2Seq model, the model needs to initialized with both :obj:`is_decoder`\n",
        "  argument and :obj:`add_cross_attention` set to :obj:`True`; an :obj:`encoder_hidden_states` is then expected as an\n",
        "  input to the forward pass.\n",
        "  \"\"\"\n",
        "  def __init__(self, config, add_pooling_layer=True):\n",
        "    super().__init__(config)\n",
        "    self.config = config\n",
        "\n",
        "    self.embeddings = EmbeddingsLookup(config)\n",
        "    self.encoder = EncoderStack(config)\n",
        "\n",
        "    self.pooler = Pooler(config) if add_pooling_layer else None\n",
        "\n",
        "    #self.init_weights() # Don't forget this\n",
        "\n",
        "  def get_input_embeddings(self):\n",
        "    return self.embeddings.word_embeddings\n",
        "\n",
        "  def set_input_embeddings(self, value):\n",
        "    self.embeddings.word_embeddings = value\n",
        "\n",
        "  def _prune_heads(self, heads_to_prune):\n",
        "    \"\"\"\n",
        "    Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n",
        "    class PreTrainedModel\n",
        "    \"\"\"\n",
        "    for layer, heads in heads_to_prune.items():\n",
        "      self.encoder.layer[layer].attention.prune_heads(heads)\n",
        "\n",
        "  def forward(self,\n",
        "              input_ids=None,\n",
        "              attention_mask=None,\n",
        "              token_type_ids=None,\n",
        "              position_ids=None,\n",
        "              head_mask=None,\n",
        "              inputs_embeds=None,\n",
        "              encoder_hidden_states=None,\n",
        "              encoder_attention_mask=None,\n",
        "              output_attentions=None,\n",
        "              output_hidden_states=None,\n",
        "              return_dict=None,):\n",
        "    \"\"\"\n",
        "    encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
        "      Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
        "      the model is configured as a decoder.\n",
        "    encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "      Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
        "      the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n",
        "      - 1 for tokens that are **not masked**,\n",
        "      - 0 for tokens that are **masked**.\n",
        "    \"\"\"\n",
        "    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "    output_hidden_states = output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "    if input_ids is not None and inputs_embeds is not None:\n",
        "      raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
        "    elif input_ids is not None:\n",
        "      input_shape = input_ids.size()\n",
        "    elif inputs_embeds is not None:\n",
        "      input_shape = inputs_embeds.size()[:-1]\n",
        "    else:\n",
        "      raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
        "\n",
        "    device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
        "\n",
        "    if attention_mask is None:\n",
        "      attention_mask = torch.ones(input_shape, device=device)\n",
        "    if token_type_ids is None:\n",
        "      token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
        "\n",
        "    # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
        "    # ourselves in which case we just need to make it broadcastable to all heads.\n",
        "    extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device) # can we throw this away?\n",
        "\n",
        "    # If a 2D or 3D attention mask is provided for the cross-attention\n",
        "    # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
        "    if self.config.is_decoder and encoder_hidden_states is not None:\n",
        "      encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
        "      encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
        "      if encoder_attention_mask is None:\n",
        "        encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
        "      encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
        "    else:\n",
        "      encoder_extended_attention_mask = None\n",
        "\n",
        "    # Prepare head mask if needed\n",
        "    # 1.0 in head_mask indicate we keep the head\n",
        "    # attention_probs has shape bsz x n_heads x N x N\n",
        "    # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
        "    # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
        "    head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
        "\n",
        "    embedding_output = self.embeddings(input_ids=input_ids,\n",
        "                                       position_ids=position_ids,\n",
        "                                       token_type_ids=token_type_ids,\n",
        "                                       inputs_embeds=inputs_embeds)\n",
        "    encoder_outputs = self.encoder(embedding_output,\n",
        "                                   attention_mask=extended_attention_mask,\n",
        "                                   head_mask=head_mask,\n",
        "                                   encoder_hidden_states=encoder_hidden_states,\n",
        "                                   encoder_attention_mask=encoder_extended_attention_mask,\n",
        "                                   output_attentions=output_attentions,\n",
        "                                   output_hidden_states=output_hidden_states,\n",
        "                                   return_dict=return_dict,)\n",
        "    sequence_output = encoder_outputs[0]\n",
        "    pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
        "\n",
        "    if not return_dict:\n",
        "      return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
        "\n",
        "    return BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=sequence_output,\n",
        "                                                        pooler_output=pooled_output,\n",
        "                                                        hidden_states=encoder_outputs.hidden_states,\n",
        "                                                        attentions=encoder_outputs.attentions,\n",
        "                                                        cross_attentions=encoder_outputs.cross_attentions,)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYWwncTdp0sg"
      },
      "source": [
        "## 4.2 Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qgh7LvX2RWME"
      },
      "source": [
        "### 4.2.1 Input Masking Algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmJT1yF3i9k8"
      },
      "source": [
        "#### Token Masking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3pECwdHBae9"
      },
      "source": [
        "# word_mask_probability = 0.15\n",
        "# replace_with_mask_probability = 0.8\n",
        "# replace_randomly_probability = 0.1\n",
        "# keep_token_probability = 0.1\n",
        "\n",
        "def mask_input_ids(inputs: torch.tensor,\n",
        "                   tokenizer: transformers.BertTokenizerFast,\n",
        "                   special_tokens_mask: Optional[torch.Tensor] = None,\n",
        "                   word_mask_probability = 0.15,\n",
        "                   replace_with_mask_probability = 0.8,\n",
        "                   replace_randomly_probability = 0.1,\n",
        "                   keep_token_probability = 0.1\n",
        "                   ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "  \"\"\"\n",
        "  We specifiy the probability with which to mask token for the language modeling\n",
        "  task. Generally 15% of tokens are considered for masking. If we just mask \n",
        "  naively then a problem arises: some masked token will never actually have been \n",
        "  seen at fine-tuning. The solution is to not replace the token with [MASK] 100%\n",
        "  of the time, instead:\n",
        "  - 80% of the time, replace the token with [MASK]\n",
        "    went to the store -> went to the [MASK]\n",
        "  - 10% of the time, replace random token\n",
        "    went to the store -> went to the running\n",
        "  - 10% of the time, keep same\n",
        "    went to the store -> went to the store\n",
        "  The same principle is also appilicable with masked sentence prediction, only\n",
        "  that we have to establish a sentence vocabulary beforehand\n",
        "\n",
        "  Args:\n",
        "    inputs: tensor, containing all the token IDs\n",
        "    special_tokens_mask: tensor, denotes whether a token is a word [0] or a \n",
        "      special token [1], [CLS] tokens and padding tokens are all counted as \n",
        "      special tokens. This will be used to create a mask so that only actual\n",
        "      words are considered for random masking\n",
        "\n",
        "  Returns:\n",
        "    masked_inputs:\n",
        "    labels:\n",
        "  \"\"\"\n",
        "  labels = inputs.clone()\n",
        "  # Tensor that hold the probability values for the Bernoulli function\n",
        "  probability_matrix = torch.full(inputs.shape, word_mask_probability)\n",
        "\n",
        "  # Get special token indices in order to exclude special tokens from masking\n",
        "  if special_tokens_mask is None:\n",
        "    special_tokens_mask = [\n",
        "      tokenizer.get_special_tokens_mask(entry, already_has_special_tokens=True) for entry in labels.tolist()\n",
        "    ]\n",
        "    special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
        "  else:\n",
        "    special_tokens_mask = special_tokens_mask.bool()\n",
        "\n",
        "  # Fill the probability matrix with 0.0 values where there are special tokens\n",
        "  probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
        "  # Draws from a bernoulli distribution where probability_matrix holds the \n",
        "  # probablitites for drawing the binary random number. The probablity matrix\n",
        "  # was previously filled with 0.0 values where special tokens are present so\n",
        "  # that only tokens containing words/sentences are considered\n",
        "  masked_indices = torch.bernoulli(probability_matrix).bool()\n",
        "  # In order to compute the loss only on the masked indices all the unmasked\n",
        "  # tokens in the label tensor are set to -100\n",
        "  labels[~masked_indices] = -100\n",
        "\n",
        "  # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
        "  indices_replaced = torch.bernoulli(torch.full(labels.shape, replace_with_mask_probability)).bool() & masked_indices\n",
        "  # Since we're dealing with tensors with numerical values we convert the [MASK]\n",
        "  # token right back to its token_id representation\n",
        "  inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
        "\n",
        "  # 10% of the time, we replace masked input tokens with random word\n",
        "  indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
        "  random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n",
        "  inputs[indices_random] = random_words[indices_random]\n",
        "\n",
        "  return (inputs, labels)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvdSZfthl-QN"
      },
      "source": [
        "#### Embedding Masking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FELb70M2Ibnf"
      },
      "source": [
        "def mask_input_embeddings(input_embeddings: torch.tensor,\n",
        "                          special_embeddings_mask: torch.tensor,\n",
        "                          sentence_mask_probability = 0.15):\n",
        "  \"\"\"\n",
        "  Randomly masks sentences with a probability of 15%. The masked sentence\n",
        "  embeddings are replaced with a random tensor and the original embedding will\n",
        "  be stored in a labels tensor that has the same size as the input tensor. The\n",
        "  ground truth embedding will sit at the same position as is did in the input\n",
        "  tensor to make it easier to identify the correct ground truth for loss\n",
        "  computing.\n",
        "\n",
        "  Args:\n",
        "    input_embeddings: A torch.tensor containing all sentence embeddings computed\n",
        "      by the Sentence Model for a given batch. The size of the tensor is\n",
        "      [batch_size, max_doc_length, embedding_size]. Note that the documents are\n",
        "      already padded to the length of the longest document in the batch.\n",
        "    special_embeddings_mask: A torch.tensor of the same size as input_embeddings\n",
        "      [batch_size, max_doc_length] which hold 0s where there is a real sentence \n",
        "      present and 1s where there is a special token embedding, that includes \n",
        "      CLS, SEP and PAD tokens.\n",
        "  Returns:\n",
        "    masked_input_embeddings: Same shape as input embeddings, only that it holds\n",
        "      a random tensor wherever a sentence embedding was masked.\n",
        "    label_embeddings: Same shape as the masked_input_embeddings but all entries \n",
        "      are filled with 0s except where there is a masked sentence embedding. That\n",
        "      entry will be filled with the original input embedding.\n",
        "    label_mask: torch.BoolTensor\n",
        "  \"\"\"\n",
        "  masked_input_embeddings = input_embeddings.clone()\n",
        "  label_embeddings = torch.zeros_like(input_embeddings)\n",
        "  label_mask = torch.zeros_like(special_embeddings_mask)\n",
        "\n",
        "  probability_matrix = torch.full(special_embeddings_mask.shape, sentence_mask_probability)\n",
        "\n",
        "  probability_matrix.masked_fill_(special_embeddings_mask, value=0.0)\n",
        "\n",
        "  masked_indices = torch.bernoulli(probability_matrix).bool()\n",
        "\n",
        "  document_counter = 0\n",
        "  sentence_counter = 0\n",
        "\n",
        "  for document in input_embeddings:\n",
        "    sentence_counter = 0\n",
        "    for sentence in document:\n",
        "      if masked_indices[document_counter][sentence_counter]:\n",
        "        label_embeddings[document_counter][sentence_counter] = input_embeddings[document_counter][sentence_counter]\n",
        "        label_mask[document_counter][sentence_counter] = 1.0\n",
        "        masked_input_embeddings[document_counter][sentence_counter] = torch.randn_like(input_embeddings[document_counter][sentence_counter])\n",
        "      sentence_counter += 1\n",
        "    document_counter += 1\n",
        "\n",
        "  label_embeddings[~masked_indices] = 0\n",
        "  label_mask.bool()\n",
        "\n",
        "  return (input_embeddings, masked_input_embeddings, label_embeddings, label_mask)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PG4skgOqPbxd"
      },
      "source": [
        "- Algorithm for masking sentences\n",
        "- Extracting the masked token embedding from the DocumentEncoder\n",
        "- Compare vs ground truth and define loss function on it (use huggingface optimizer??)\n",
        "from transformers import AdamW #example\n",
        "optimizer = AdamW(...)\n",
        "https://huggingface.co/transformers/training.html\n",
        "- SMITH adds loss from sentence encoder + document encoder, make both trainable simultaneously???"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFQ_HrliRfml"
      },
      "source": [
        "### 4.2.2 Language Modeling Head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zC2Kn-tug0bk"
      },
      "source": [
        "Define the LM Head(s) and its loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ue_TauSD2gWp"
      },
      "source": [
        "#### Word Level Language Modeling Head"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFclI3Hu210g"
      },
      "source": [
        "class PredictionHeadTransform(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "    if isinstance(config.hidden_act, str):\n",
        "      self.transform_act_fn = ACT2FN[config.hidden_act]\n",
        "    else:\n",
        "      self.transform_act_fn = config.hidden_act\n",
        "    self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "\n",
        "  def forward(self, hidden_states):\n",
        "    hidden_states = self.dense(hidden_states)\n",
        "    hidden_states = self.transform_act_fn(hidden_states)\n",
        "    hidden_states = self.LayerNorm(hidden_states)\n",
        "    return hidden_states\n",
        "\n",
        "\n",
        "\n",
        "class LMPredictionHead(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.transform = PredictionHeadTransform(config)\n",
        "\n",
        "    # The output weights are the same as the input embeddings, but there is\n",
        "    # an output-only bias for each token.\n",
        "    self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
        "\n",
        "    self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n",
        "\n",
        "    # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`\n",
        "    self.decoder.bias = self.bias\n",
        "\n",
        "  def forward(self, hidden_states):\n",
        "    hidden_states = self.transform(hidden_states)\n",
        "    hidden_states = self.decoder(hidden_states)\n",
        "    return hidden_states\n",
        "\n",
        "\n",
        "\n",
        "class OnlyLMHead(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.predictions = LMPredictionHead(config)\n",
        "\n",
        "  def forward(self, sequence_output):\n",
        "    prediction_scores = self.predictions(sequence_output)\n",
        "    return prediction_scores"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17qoRvtU2oEL"
      },
      "source": [
        "#### Sentence Level Language Modeling Head"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XiNeDnD22bV"
      },
      "source": [
        "class SentencePredictionHead(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.dense = nn.Linear(in_features=config.hidden_size, out_features=config.hidden_size)\n",
        "    self.LayerNorm = nn.LayerNorm() # Compare to the word-level LM Head\n",
        "\n",
        "  def forward(self, masked_sentence_prediction, label_embeddings, label_mask):\n",
        "    \"\"\"\n",
        "    In order to compute the sentence-level prediction loss we apply a similar\n",
        "    loss function as during the word-level masked word prediction tast. Since\n",
        "    we don't have a fixed size vocabulary over the training sentences we have\n",
        "    to build a dynamic sentence vocabulary.\n",
        "    Args:\n",
        "      masked_sentence_prediction [batch_size, max_doc_length, hidden_size]:\n",
        "      label_embeddings [batch_size, max_doc_length, hidden_size]:\n",
        "      label_mask [batch_size, max_doc_length, hidden_size]:\n",
        "    Returns:\n",
        "      per_batch_sentence_loss:\n",
        "      per_example_sentence_loss:\n",
        "    \"\"\"\n",
        "    # Zero out all sentence embeddings that aren't at a masked position\n",
        "    masked_sentence_prediction[~label_mask] = 0.0\n",
        "    label_embeddings[~label_mask] = 0.0\n",
        "    \n",
        "    # Tensors will have size [batch_size * padded_doc_length, hidden_size]\n",
        "    masked_sentence_prediction = torch.reshape(masked_sentence_prediction, (config.batch_size * config.doc_length, -1))\n",
        "    label_embeddings = torch.reshape(label_embeddings, (config.batch_size * config.doc_length, -1))\n",
        "    label_mask = torch.reshape(label_mask, (config.batch_size * config.doc_length, -1))\n",
        "\n",
        "    output_embedding_list = []\n",
        "    label_embedding_list = []\n",
        "    label_mask_index =  0\n",
        "\n",
        "    for mask_index in label_mask:\n",
        "      if mask_index.item():\n",
        "        output_embedding_list.append(masked_sentence_prediction[mask_index_int])\n",
        "        label_embedding_list.append(label_embeddings[mask_index_int])\n",
        "      mask_index_int += 1\n",
        "\n",
        "    output_embeddings = torch.stack(output_embedding_list, dim=0)\n",
        "    label_embeddings = torch.stack(label_embedding_list, dim=0)\n",
        "\n",
        "    output_embeddings = dense(output_embeddings)\n",
        "    output_embeddings = LayerNorm(output_embeddings)\n",
        "\n",
        "    # TODO add bias like in SMITH?\n",
        "\n",
        "    logits = torch.matmul(output_embeddings, torch.transpose(input=label_embeddings, dim0=0, dim1=1))\n",
        "    log_probabilities = nn.functional.log_softmax(logits, dim=1)\n",
        "    labels_one_hot = torch.diag(torch.Tensor([1] * log_probs.size()[0]))\n",
        "\n",
        "\n",
        "    return (logits, per_batch_sentence_loss, per_example_sentence_loss)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGoLVfY2i1P8"
      },
      "source": [
        "### 4.2.3 Alternative: Learning via Decoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uWqM6Avi_0j"
      },
      "source": [
        "Write a Decoder like in Hibert to have an alternative to the LM Head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyPDLz_kN7uE"
      },
      "source": [
        "## 4.3 Sentence Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgNBK8LN2aho"
      },
      "source": [
        "class SentenceModelingOutput(ModelOutput): #inherits from the huggingface class\r\n",
        "  \"\"\"\r\n",
        "    Return object for Sentence Model.\r\n",
        "\r\n",
        "    Args:\r\n",
        "      loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`labels` is provided):\r\n",
        "        Masked language modeling (MLM) loss.\r\n",
        "      logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, config.vocab_size)`):\r\n",
        "        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\r\n",
        "      hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\r\n",
        "        Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\r\n",
        "        of shape :obj:`(batch_size, sequence_length, hidden_size)`.\r\n",
        "        Hidden-states of the model at the output of each layer plus the initial embedding outputs.\r\n",
        "      last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`):\r\n",
        "        Sequence of hidden-states at the output of the last layer of the model.\r\n",
        "      attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\r\n",
        "        Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\r\n",
        "        sequence_length, sequence_length)`.\r\n",
        "        Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\r\n",
        "        heads.\r\n",
        "  \"\"\"\r\n",
        "  loss: Optional[torch.FloatTensor] = None\r\n",
        "  logits: torch.FloatTensor = None\r\n",
        "  hidden_states: Optional[Tuple[torch.FloatTensor]] = None\r\n",
        "  last_hidden_state: torch.FloatTensor = None\r\n",
        "  attentions: Optional[Tuple[torch.FloatTensor]] = None"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1QYoqpgRlef"
      },
      "source": [
        "class HATESentenceModel(TransformerPreTrainedModel):\n",
        "  \n",
        "  _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
        "  _keys_to_ignore_on_load_missing = [r\"position_ids\", r\"predictions.decoder.bias\"]\n",
        "\n",
        "  \n",
        "  def __init__(self, config):\n",
        "    super().__init__(config)\n",
        "\n",
        "    if config.is_decoder:\n",
        "      logger.warning(\n",
        "        \"If you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for \"\n",
        "        \"bi-directional self-attention.\"\n",
        "        )\n",
        "\n",
        "    self.transformer = TransformerBase(config, add_pooling_layer=False)\n",
        "    self.lmhead = OnlyLMHead(config)\n",
        "\n",
        "  def get_output_embeddings(self):\n",
        "    return self.lmhead.predictions.decoder\n",
        "\n",
        "  def set_output_embeddings(self, new_embeddings):\n",
        "    self.lmhead.predictions.decoder = new_embeddings\n",
        "\n",
        "  def forward(self,\n",
        "              input_ids=None,\n",
        "              attention_mask=None,\n",
        "              token_type_ids=None,\n",
        "              position_ids=None,\n",
        "              head_mask=None,\n",
        "              inputs_embeds=None,\n",
        "              encoder_hidden_states=None,\n",
        "              encoder_attention_mask=None,\n",
        "              labels=None,\n",
        "              output_attentions=None,\n",
        "              output_hidden_states=None,\n",
        "              return_dict=None,):\n",
        "    # TODO replace batch_size with document_length in here in the docfile\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      inputs_ids (torch.LongTensor of shape (batch_size, sequence_length)):\n",
        "        Indices of input sequence tokens in the vocabulary.\n",
        "      attention_mask (torch.FloatTensor of shape (batch_size, sequence_length), optional):\n",
        "        Mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]:\n",
        "        - 1 for tokens that are not masked,\n",
        "        - 0 for tokens that are masked.\n",
        "      token_type_ids  (torch.LongTensor of shape (batch_size, sequence_length), optional):\n",
        "        Segment token indices to indicate first and second portions of the inputs. Indices are selected in [0, 1]:\n",
        "        - 0 corresponds to a sentence A token,\n",
        "        - 1 corresponds to a sentence B token.\n",
        "      position_ids (torch.LongTensor of shape (batch_size, sequence_length), optional):\n",
        "        Indices of positions of each input sequence tokens in the position embeddings. \n",
        "        Selected in the range [0, config.max_position_embeddings - 1].\n",
        "      head_mask (torch.FloatTensor of shape (num_heads,) or (num_layers, num_heads), optional):\n",
        "        Mask to nullify selected heads of the self-attention modules. Mask values selected in [0, 1]:\n",
        "        - 1 indicates the head is not masked,\n",
        "        - 0 indicates the head is masked.\n",
        "      inputs_embeds (torch.FloatTensor of shape (batch_size, sequence_length, hidden_size), optional):\n",
        "        Optionally, instead of passing input_ids you can choose to directly pass\n",
        "         an embedded representation. This is useful if you want more control over \n",
        "         how to convert input_ids indices into associated vectors than the model’s \n",
        "         internal embedding lookup matrix.\n",
        "      encoder_hidden_states (torch.FloatTensor of shape (batch_size, sequence_length, hidden_size), optional):\n",
        "        Sequence of hidden-states at the output of the last layer of the encoder. \n",
        "        Used in the cross-attention if the model is configured as a decoder.\n",
        "      encoder_attention_mask (torch.FloatTensor of shape (batch_size, sequence_length), optional):\n",
        "        Mask to avoid performing attention on the padding token indices of the encoder \n",
        "        input. This mask is used in the cross-attention if the model is configured \n",
        "        as a decoder. Mask values selected in [0, 1]:\n",
        "        - 1 for tokens that are not masked,\n",
        "        - 0 for tokens that are masked.\n",
        "      labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "        Labels for computing the masked language modeling loss. Indices should be in ``[-100, 0, ...,\n",
        "        config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are ignored\n",
        "        (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``\n",
        "      output_attentions (bool, optional): \n",
        "        Whether or not to return the attentions tensors of all attention layers. \n",
        "        See attentions under returned tensors for more detail.\n",
        "      output_hidden_states (bool, optional):\n",
        "        Whether or not to return the hidden states of all layers. See hidden_states \n",
        "        under returned tensors for more detail.\n",
        "      return_dict (bool, optional):\n",
        "        Whether or not to return a ModelOutput instead of a plain tuple.\n",
        "    Returns:\n",
        "      SentenceModelingOutput:\n",
        "    \"\"\"\n",
        "    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "    outputs = self.transformer(input_ids,\n",
        "                               attention_mask=attention_mask,\n",
        "                               token_type_ids=token_type_ids,\n",
        "                               position_ids=position_ids,\n",
        "                               head_mask=head_mask,\n",
        "                               inputs_embeds=inputs_embeds,\n",
        "                               encoder_hidden_states=encoder_hidden_states,\n",
        "                               encoder_attention_mask=encoder_attention_mask,\n",
        "                               output_attentions=output_attentions,\n",
        "                               output_hidden_states=output_hidden_states,\n",
        "                               return_dict=return_dict)\n",
        "    \n",
        "    sequence_output = outputs[0]\n",
        "    prediction_scores = self.lmhead(sequence_output)\n",
        "\n",
        "    masked_lm_loss = None\n",
        "    if labels is not None:\n",
        "      loss_fct = nn.CrossEntropyLoss()  # -100 index = padding token\n",
        "      masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
        "\n",
        "    if not return_dict:\n",
        "      output = (prediction_scores,) + outputs[2:]\n",
        "      return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n",
        "\n",
        "    return SentenceModelingOutput(loss=masked_lm_loss,\n",
        "                                  logits=prediction_scores,\n",
        "                                  hidden_states=outputs.hidden_states,\n",
        "                                  last_hidden_state=sequence_output,\n",
        "                                  attentions=outputs.attentions,)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ah6qLiazN-xP"
      },
      "source": [
        "## 4.4 Document Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0lUc4tXBnAv"
      },
      "source": [
        "class DocumentModelingOutput(ModelOutput):\r\n",
        "  \"\"\"\r\n",
        "    Return object for Document Model.\r\n",
        "\r\n",
        "    Args:\r\n",
        "      loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`labels` is provided):\r\n",
        "        Masked language modeling (MLM) loss.\r\n",
        "      logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, config.vocab_size)`):\r\n",
        "        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\r\n",
        "      hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\r\n",
        "        Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\r\n",
        "        of shape :obj:`(batch_size, sequence_length, hidden_size)`.\r\n",
        "        Hidden-states of the model at the output of each layer plus the initial embedding outputs.\r\n",
        "      last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`):\r\n",
        "        Sequence of hidden-states at the output of the last layer of the model.\r\n",
        "      attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\r\n",
        "        Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\r\n",
        "        sequence_length, sequence_length)`.\r\n",
        "        Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\r\n",
        "        heads.\r\n",
        "  \"\"\"\r\n",
        "  loss: Optional[torch.FloatTensor] = None\r\n",
        "  logits: torch.FloatTensor = None\r\n",
        "  hidden_states: Optional[Tuple[torch.FloatTensor]] = None\r\n",
        "  last_hidden_state: torch.FloatTensor = None\r\n",
        "  attentions: Optional[Tuple[torch.FloatTensor]] = None"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIlngzQ-R91S"
      },
      "source": [
        "class HATEDocumentModel(TransformerPreTrainedModel):\n",
        "  def __init__():\n",
        "    super().__init__(config)\n",
        "    self.transformer = TransformerBase(config, add_pooling_layer=False)\n",
        "    self.lmhead = SentencePredictionHead(config)\n",
        "\n",
        "  def forward(self,\n",
        "              input_ids=None,\n",
        "              attention_mask=None,\n",
        "              token_type_ids=None,\n",
        "              position_ids=None,\n",
        "              head_mask=None,\n",
        "              inputs_embeds=None,\n",
        "              encoder_hidden_states=None,\n",
        "              encoder_attention_mask=None,\n",
        "              labels_embeddings=None,\n",
        "              labels_mask=None,\n",
        "              output_attentions=None,\n",
        "              output_hidden_states=None,\n",
        "              return_dict=None,):\n",
        "    \n",
        "    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "    outputs = self.transformer(input_ids,\n",
        "                               attention_mask=attention_mask,\n",
        "                               token_type_ids=token_type_ids,\n",
        "                               position_ids=position_ids,\n",
        "                               head_mask=head_mask,\n",
        "                               inputs_embeds=inputs_embeds,\n",
        "                               encoder_hidden_states=encoder_hidden_states,\n",
        "                               encoder_attention_mask=encoder_attention_mask,\n",
        "                               output_attentions=output_attentions,\n",
        "                               output_hidden_states=output_hidden_states,\n",
        "                               return_dict=return_dict,)\n",
        "    \n",
        "    sequence_output = outputs[0]\n",
        "    sentence_prediction_output = self.lmhead(sequence_output, labels_embeddings, labels_mask)\n",
        "\n",
        "    return DocumentModelingOutput(loss=sentence_prediction_output[1],\n",
        "                                  logits=sentence_prediction_output[0],\n",
        "                                  hidden_states=outputs.hidden_states,\n",
        "                                  last_hidden_state=sequence_output,\n",
        "                                  attentions=outputs.attentions,)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IVJdy-_qAPt"
      },
      "source": [
        "## 4.5 Hierachical Attention-Based Document Encoder (HATE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VglcydbCqvL_"
      },
      "source": [
        "class HATEConfig ():\n",
        "  def __init__(self,\n",
        "               sentence_model_config,\n",
        "               document_model_config,\n",
        "               is_pretraining=False):\n",
        "    \n",
        "    \"\"\"\n",
        "    Constructs ModelConfig.\n",
        "    Args:\n",
        "      Stuff\n",
        "    Returns:\n",
        "      Stuff\n",
        "    \"\"\"\n",
        "    self.sentence_model_config = sentence_model_config\n",
        "    self.document_model_config = document_model_config\n",
        "    self.is_pretraining = is_pretraining\n",
        "    \n",
        "  @classmethod\n",
        "  def from_dict(cls, json_object):\n",
        "    \"\"\"Constructs a `BertConfig` from a Python dictionary of parameters.\"\"\"\n",
        "    config = BertConfig(vocab_size=None)\n",
        "    for (key, value) in six.iteritems(json_object):\n",
        "      config.__dict__[key] = value\n",
        "    return config\n",
        "\n",
        "  @classmethod\n",
        "  def from_json_file(cls, json_file):\n",
        "    \"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"\n",
        "    with tf.gfile.GFile(json_file, \"r\") as reader:\n",
        "      text = reader.read()\n",
        "    return cls.from_dict(json.loads(text))\n",
        "\n",
        "  def to_dict(self):\n",
        "    \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
        "    output = copy.deepcopy(self.__dict__)\n",
        "    return output\n",
        "\n",
        "  def to_json_string(self):\n",
        "    \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
        "    return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0WDofhneV06"
      },
      "source": [
        "class HATEOutput():\r\n",
        "  \"\"\"\r\n",
        "  Class for the whole model output\r\n",
        "  \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxULKxFYz7uz"
      },
      "source": [
        "class HATEModel (torch.nn.Module):\n",
        "  def __init__(self, hate_config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.sentence_model = HATESentenceModel(hate_config.sentence_model_config)\n",
        "    self.document_model = HATEDocumentModel(hate_config.document_model_config)\n",
        "\n",
        "  # TODO write a config that implements the functionality that the forward\n",
        "  # function can take either an entire batch or a single document as input, so \n",
        "  # it has a mode for pretrain or inference\n",
        "  # Or is it all the same and a single document is just a batch with length one?\n",
        "  def forward(batch_token_ids: torch.Tensor,\n",
        "              batch_attention_mask: torch.Tensor,\n",
        "              batch_labels: torch.Tensor\n",
        "              max_doc_length: int,\n",
        "              pretraining=hate_config.is_pretraining: Bool): # Make it so that when the model is in training mode it accepts a batch by default, else it accepts a document or a sentence for inference (hint: a sentence is just a document with length=1)\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      batch_token_ids:\n",
        "      batch_attention_mask:\n",
        "      batch_labels:\n",
        "      max_doc_length (int): The number of sentences in the longest document of\n",
        "        the batch in order to pad the intermediary embedding tensor accordingly.\n",
        "      pretraining:\n",
        "    Returns:\n",
        "    \"\"\"\n",
        "    if pretraining:\n",
        "\n",
        "      sentence_model_embeddings = []\n",
        "\n",
        "      for input_ids, attention_mask, labels in zip(batch_token_ids, batch_attention_mask, batch_labels):\n",
        "        sentence_model_outputs = self.sentence_model(input_ids, attention_mask, labels)\n",
        "        sentence_model_embeddings.appen(sentence_model_outputs.hidden_states)\n",
        "\n",
        "      document_model_input =  prepare_for_document_model(sentence_model_embeddings)\n",
        "\n",
        "      document_model_output = self.document_model()\n",
        "\n",
        "      return HATEOutput()\n",
        "\n",
        "    else:\n",
        "      # inference routine, include checking for correct tensor sizes, etc.\n",
        "\n",
        "    \n",
        "    for sentence in document:\n",
        "      sentence_model(input_ids=sentence[0], attention_mask=sentence[1], )\n",
        "\n",
        "    for sent in doc:\n",
        "      # Apply the word-level BERT model for masked token prediction\n",
        "      sentence_output = self.sentence_model(input_ids, labels, out)\n",
        "      # The ouput object can already return the loss\n",
        "      sentence_loss = sentence_output.loss\n",
        "      # Returns the CLS token, has been shown to produce solid sentence representations\n",
        "      sentence_representation = sentence_output.hidden_states[0]\n",
        "      # Dense layer as in SMITH\n",
        "      sentence_representation = self.sentence_embedding(sentence_representation)\n",
        "      # Normalization as in SMITH\n",
        "      sentence_representation = f.normalize(sentence_representation)\n",
        "      # Append the features to list of sentence features for the according doc\n",
        "      sentence_representation_list.append((sentence_output, sentence_loss, sentence_representation))\n",
        "    \n",
        "    \n",
        "    masked_sentence_embeddings, sentence_labels, mask_indices = mask_sentences()\n",
        "\n",
        "    \n",
        "    # TODO after looping thru all sentence: create sentence level attention mask, special token mask, concatenate sentence model outputs, pad them to longest doc in batch\n",
        "\n",
        "  \n",
        "    # prune layers? transformers.modeling_utils.find_pruneable_heads_and_indices\n",
        "\n",
        "    return HATEOutput()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YleJunzjQSh-"
      },
      "source": [
        "# **5.** Pretraining and Finetuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYLHvvvFOkuU"
      },
      "source": [
        "## 5.1 Pretraining"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTTNBPu6MMsX"
      },
      "source": [
        "### 5.1.1 Evidence for the need for Pretraining\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boIilmUBvAWw"
      },
      "source": [
        "- HIBERT https://arxiv.org/pdf/1905.06566.pdf\n",
        "- Language Model Pre-training for Hierarchical Document Representations https://arxiv.org/pdf/1901.09128.pdf\n",
        "- Pre-training Tasks for Embedding-based Large-scale Retrieval https://arxiv.org/pdf/2002.03932.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-smV5mIMXzj"
      },
      "source": [
        "### 5.1.2 Pretraining Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kLltdEAWvd7"
      },
      "source": [
        "Describe Dataset used for pretraining, where to get it and how to load it\n",
        "https://dumps.wikimedia.org/enwiki/20200920/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwlLYYpKHAj2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288,
          "referenced_widgets": [
            "9084e13a33a742b2a64621c670702cd7",
            "4464e8f5cd4441ad99fcc4ffb8ff5cb0",
            "21f7a072271b4cb6b1645f9d854091b9",
            "11d96d057ca942fd88f1c90685fccd05",
            "990b6005830f40bf80f5554754d84175",
            "c0bbac2304a048a6a88466817cb8d541",
            "91c9937c10f648b78e03551e9d312f9c",
            "c8ff3760b2e24756b301db0da9b7b320",
            "9a25a7c119464465ad2133f17a382fe3",
            "6ad73c54c76847c691a0e76ac05e219b",
            "b01fd112d5be435a82b4f5a091517512",
            "7e5bd73272384ac4b1e366b00607246b",
            "164e2fa662904f0cb2ac2817040dc91e",
            "d57cc71604b24e368e56df78cd73a337",
            "d82b0c33bad049898cc5e25d6fe742e2",
            "319d4d2ea23349b9bf4776bff4e16ebd",
            "bb2f480dd63d4e64927598d8de2fc2df",
            "a9c4f2e220344daaa57477ff0159a8a5",
            "fbca6f1e03754ede9b2a02c14c2f8fd3",
            "18ec7824a4f1464e83f4463814e1f9d9",
            "192fc5e053fb43e787de9145754afb04",
            "b21e3048cfdc4c04a5f2071e7431a4ea",
            "e32906b99e8d4f629e0bf5dc094f36c4",
            "444b1e20a8d24a238e1c47b356f68887",
            "4610af6240da4e24a439f42f8d5d41e9",
            "243cd8874a3e4a20a65d76a7931d6d16",
            "e14ce6f62b9f4ec199fd0d4834169b8a",
            "1f12992a710c4b88b3ca97110355070b",
            "6b2b592a12a04d84842be4f74285498a",
            "38bf115b867a4cc498a045fd114eedff",
            "57a5fd856b7c409b8ba5bdbf5e7ef2db",
            "81a298dfb17345ac8ef583a7286372c9"
          ]
        },
        "outputId": "f8415d05-f2d1-42f4-c19f-5210dea4569e"
      },
      "source": [
        "from datasets import list_datasets, load_dataset\n",
        "datasets_list = list_datasets()\n",
        "print(\"This is how many datasets are available: \", len(datasets_list))\n",
        "# Load the Wikipedia dump into an OrderedDict\n",
        "wiki_data_raw = load_dataset('wikipedia', '20200501.en', split='train[:1%]')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is how many datasets are available:  183\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9084e13a33a742b2a64621c670702cd7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=4417.0, style=ProgressStyle(description…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a25a7c119464465ad2133f17a382fe3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=6866.0, style=ProgressStyle(description…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Downloading and preparing dataset wikipedia/20200501.en (download: 16.99 GiB, generated: 17.07 GiB, post-processed: Unknown size, total: 34.06 GiB) to /root/.cache/huggingface/datasets/wikipedia/20200501.en/1.0.0/f92599dfccab29832c442b82870fa8f6983e5b4ebbf5e6e2dcbe894e325339cd...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb2f480dd63d4e64927598d8de2fc2df",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=14554.0, style=ProgressStyle(descriptio…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4610af6240da4e24a439f42f8d5d41e9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=18307873280.0, style=ProgressStyle(desc…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Dataset wikipedia downloaded and prepared to /root/.cache/huggingface/datasets/wikipedia/20200501.en/1.0.0/f92599dfccab29832c442b82870fa8f6983e5b4ebbf5e6e2dcbe894e325339cd. Subsequent calls will reuse this data.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SofsbkO1Tf_2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d620798b-09e3-4f34-89dd-13c590c28aa5"
      },
      "source": [
        "# Print out some basic info about the dataset to better understand the structure\n",
        "print(\"size: \", wiki_data_raw.dataset_size)\n",
        "print(\"column names: \", wiki_data_raw.column_names)\n",
        "print(\"shape: \", wiki_data_raw.shape)\n",
        "print(\"format: \", wiki_data_raw.format)\n",
        "print(\"description: \", wiki_data_raw.description)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "size:  18330235071\n",
            "column names:  ['title', 'text']\n",
            "shape:  (60784, 2)\n",
            "format:  {'type': None, 'format_kwargs': {}, 'columns': ['title', 'text'], 'output_all_columns': False}\n",
            "description:  Wikipedia dataset containing cleaned articles of all languages.\n",
            "The datasets are built from the Wikipedia dump\n",
            "(https://dumps.wikimedia.org/) with one split per language. Each example\n",
            "contains the content of one full Wikipedia article with cleaning to strip\n",
            "markdown and unwanted sections (references, etc.).\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTJuia_PGDMx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0dcf7e6-54ff-4586-b5e1-fba1db4d157b"
      },
      "source": [
        "# Create short version dataset containing 10 documents for quick testing\n",
        "wiki_data_test = wiki_data_raw[:10]\n",
        "print(\"first 10 lines of OrderedDict: \", wiki_data_test)\n",
        "print(\"first 10 titles: \", wiki_data_test['title'])\n",
        "print(\"first 10 articles: \", wiki_data_test['text'])\n",
        "print(\"first article: \", wiki_data_test['text'][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "first 10 lines of OrderedDict:  OrderedDict([('title', ['Yangliuqing', 'Orana Australia Ltd', \"St. Mary's Church, Sønderborg\", 'Kalitta', 'Where Is Freedom?', 'Latin liturgical rites', 'Fernaldia pandurata', 'Chester Earl Merrow', 'Hightech Information System', 'AD 47']), ('text', ['Yangliuqing () is a market town in Xiqing District, in the western suburbs of Tianjin, People\\'s Republic of China. Despite its relatively small size, it has been named since 2006 in the \"famous historical and cultural market towns in China\".\\n\\nIt is best known in China for creating nianhua or Yangliuqing nianhua. For more than 400 years, Yangliuqing has in effect specialised in the creation of these woodcuts for the New Year.  wood block prints using vivid colourschemes to portray traditional scenes of children\\'s games often interwoven with auspiciouse objects.\\n\\n, it had 27 residential communities () and 25 villages under its administration.\\n\\nShi Family Grand Courtyard\\n\\nShi Family Grand Courtyard (Tiānjīn Shí Jiā Dà Yuàn, 天津石家大院) is situated in Yangliuqing Town of Xiqing District, which is the former residence of wealthy merchant Shi Yuanshi - the 4th son of Shi Wancheng, one of the eight great masters in Tianjin. First built in 1875, it covers over 6,000 square meters, including large and small yards and over 200 folk houses, a theater and over 275 rooms that served as apartments and places of business and worship for this powerful family. Shifu Garden, which finished its expansion in October 2003, covers 1,200 square meters, incorporates the elegance of imperial garden and delicacy of south garden. Now the courtyard of Shi family covers about 10,000 square meters, which is called the first mansion in North China. Now it serves as the folk custom museum in Yangliuqing, which has a large collection of folk custom museum in Yanliuqing, which has a large collection of folk art pieces like Yanliuqing New Year pictures, brick sculpture.\\n\\nShi\\'s ancestor came from Dong\\'e County in Shandong Province, engaged in water transport of grain. As the wealth gradually accumulated, the Shi Family moved to Yangliuqing and bought large tracts of land and set up their residence. Shi Yuanshi came from the fourth generation of the family, who was a successful businessman and a good household manager, and the residence was thus enlarged for several times until it acquired the present scale. It is believed to be the first mansion in the west of Tianjin.\\n\\nThe residence is symmetric based on the axis formed by a passageway in the middle, on which there are four archways. On the east side of the courtyard, there are traditional single-story houses with rows of rooms around the four sides, which was once the living area for the Shi Family. The rooms on north side were the accountants\\' office. On the west are the major constructions including the family hall for worshipping Buddha, theater and the south reception room. On both sides of the residence are side yard rooms for maids and servants.\\n\\nToday, the Shi mansion, located in the township of Yangliuqing to the west of central Tianjin, stands as a surprisingly well-preserved monument to China\\'s pre-revolution mercantile spirit. It also serves as an on-location shoot for many of China\\'s popular historical dramas. Many of the rooms feature period furniture, paintings and calligraphy, and the extensive Shifu Garden.\\n\\nPart of the complex has been turned into the Yangliuqing Museum, which includes displays focused on symbolic aspects of the courtyards\\'  construction, local folk art and customs, and traditional period furnishings and crafts.\\n\\nSee also \\n\\nList of township-level divisions of Tianjin\\n\\nReferences \\n\\n http://arts.cultural-china.com/en/65Arts4795.html\\n\\nCategory:Towns in Tianjin', 'Orana Australia Ltd is a not-for-profit organisation that provides a diverse range of training and support services to over 650 people with disabilities and their families in South Australia.\\n\\nHistory\\nThe Mentally Retarded Children’s Society of SA Inc. was established in 1950 by a group of parents who wanted education, employment and accommodation opportunities for their children within the local community at a time when institutionalised care in Adelaide was their only alternative.\\n\\nThe society’s aims were to seek education or training facilities for people with intellectual disabilities, to establish sheltered workshops, and to establish residential hostels.\\n\\nA number of sheltered workshops were established, and in 1980, the name was changed to the Aboriginal word \"Orana\", which means \"Welcome\".\\n\\nToday, Orana provides assisted employment, assisted accommodation and respite services to people with intellectual disabilities.\\n\\nOrana\\'s current and previous clients include Mitsubishi Motors, Clipsal, RAA, Elders Limited, and Billycart Kids.\\n\\nOrana was one of the first disability service organisations to achieve Quality Accreditation.  The services and products they offer are:\\n\\n Packaging\\n Assembly\\n Sewing\\n Collating & Mailing\\n Furniture - Retail\\n Furniture – Manufacture for Commercial Market\\n Worm Farming\\n Work Crews\\n Pet & Grain – Retail\\n\\nIn 2018, after 65 years of bettering people’s lives, Orana identified a community need and expanded their operations into the aged care sector.\\n\\nAfter the unveiling of the Australian Government’s Commonwealth Home Support Programme (CHSP) and seeing it as a natural step of progression, Orana now provides quality tailored aged care at home.\\n\\nThe well-resourced organization delivers help across a range of areas, helping the elderly remain where they want to be - in the comfort of their own home during their later years.\\n\\nOrana continues with its mission to support people remain independent, valued and productive members of the community.\\n\\nReferences\\n\\nExternal links \\n \\n\\nCategory:Disability organisations based in Australia\\nCategory:Organisations based in South Australia', \"The St. Mary's Church is a church owned by the Church of Denmark in Sønderborg, Denmark and the church of the parish with the same name. Thanks to its location on a hill, the church building is very iconic for the city.\\n\\nHistory \\nIn the Middle Ages there was a leper colony on a hill just outside the city. It was named after Saint George and around 1300 the chapel of this leper colony stood in the place of the present St. Mary's Church. After the old parish church of the city, the St. Nicholas Church, was demolished around 1530, the Saint-George chapel became the new main church. Towards the end of the 16th century, John II, Duke of Schleswig-Holstein-Sonderburg commissioned the enlargement of the building in order to make it suitable for the function of the parish church of his city.\\n\\nThe current St. Mary's Church \\nIn 1595 a start was made on the partial demolition of the old church and the construction of the new church. Only parts of the old medieval church remained. From the medieval church, a medieval wooden wall cupboard dating from about 1400 remained. The solemn inauguration of the new parish church took place just before Christmas in 1600. In 1649 the George Church was renamed as the Mary Church. The name of Saint George stayed in the Danish names Sankt Jørgensgade and Jørgensbjerg.\\n\\nReferences \\n\\nCategory:Buildings and structures in Sønderborg Municipality\\nCategory:Churches in Denmark\\nCategory:Church of Denmark churches\", 'Kalitta may refer to:\\n\\nConnie Kalitta (born 1938), a retired American drag racer and CEO of the eponymous Kallita Air.\\nDoug Kalitta (born 1964), an American drag racer, nephew of Connie Kalitta and owner of Kalitta Charters.\\nScott Kalitta (1962-2008), an American drag racer and son of Connie Kalitta.\\nKalitta Air, a cargo airline flying Boeing 747 aircraft.\\nKalitta Charters, a cargo airline flying medium-sized aircraft.', \"Where Is Freedom? () is a 1954 Italian comedy-drama film directed by Roberto Rossellini. \\n \\nThe film had a troubled production because, after shooting some scenes, Rossellini lost interest in the film and abandoned the set. The work was completed after about a year, mainly from Mario Monicelli, with some scenes also shot by Lucio Fulci and Federico Fellini. Despite that, Rossellini is the sole credited director of the film.\\n\\nPlot \\nDifficulties and troubles of an ex-convict. Embittered and disillusioned by life, he will soon plan his return to prison.\\n\\nCast \\nTotò: Salvatore Lo Jacono \\nVera Molnar: Agnesina \\nNita Dover:  maratoneta di danza \\nFranca Faldini: Maria \\nLeopoldo Trieste: Abramo Piperno \\nAntonio Nicotra: maresciallo \\nSalvo Libassi:  maresciallo #2 \\nGiacomo Rondinella:  carcerato \\nUgo D'Alessio:  giudice\\nMario Castellani: pubblico ministero \\nVincenzo Talarico: avvocato difensore\\nPietro Carloni:  Pietro\\n\\nReferences\\n\\nExternal links\\n\\n   \\n\\nCategory:1954 films\\nCategory:Italian comedy-drama films\\nCategory:1950s comedy-drama films\\nCategory:Films directed by Roberto Rossellini\\nCategory:Commedia all'italiana\\nCategory:Films set in Rome\\nCategory:Italian films\\nCategory:Films produced by Dino De Laurentiis\\nCategory:Films produced by Carlo Ponti\", 'Latin liturgical rites, or Western liturgical rites, are Catholic liturgical rites employed by the Latin Church, the largest particular church sui iuris of the Catholic Church, that originated in Europe where the Latin language once dominated. Its language is now known as Ecclesiastical Latin. The most used rite is the Roman Rite.\\n\\nThe Latin rites were for many centuries no less numerous than the liturgical rites of the Eastern autonomous particular Churches. Their number is now much reduced. In the aftermath of the Council of Trent, in 1568 and 1570 Pope Pius V suppressed the Breviaries and Missals that could not be shown to have an antiquity of at least two centuries (see Tridentine Mass and Roman Missal). Many local rites that remained legitimate even after this decree were abandoned voluntarily, especially in the 19th century. In the second half of the 20th century, most of the religious orders that had a distinct liturgical rite chose to adopt in its place the Roman Rite as revised in accordance with the decrees of the Second Vatican Council (see Mass of Paul VI). A few such liturgical rites persist today for the celebration of Mass, since 1965–1970 in revised forms, but the distinct liturgical rites for celebrating the other sacraments have been almost completely abandoned.\\n\\nLiturgical rites currently in use within the Latin Church\\n\\nRoman Rite\\n\\nThe Roman Rite is by far the most widely used. Like other liturgical rites, it developed over time, with newer forms replacing the older. It underwent many changes in the first millennium and a half of its existence (see Pre-Tridentine Mass). The forms that Pope Pius V, as requested by the Council of Trent, established in the 1560s and 1570s underwent repeated minor variations in the centuries immediately following. Each new typical edition (the edition to which other printings are to conform) of the Roman Missal (see Tridentine Mass) and of the other liturgical books superseded the previous one.\\n\\nThe 20th century saw more profound changes. Pope Pius X radically rearranged the Psalter of the Breviary and altered the rubrics of the Mass. Later popes continued to make such changes, beginning with Pope Pius XII, who significantly revised the Holy Week ceremonies and certain other aspects of the Roman Missal in 1955.\\n\\nOrdinary Form\\n\\nThe Second Vatican Council (1962–1965) was followed by a general revision of the rites of all the Roman Rite sacraments, including the Eucharist. As before, each new typical edition of an official liturgical book supersedes the previous one. Thus, the 1970 Roman Missal, which superseded the 1962 edition, was superseded by the edition of 1975. The 2002 edition in turn supersedes the 1975 edition both in Latin and, as official translations into each language appear, also in the vernacular languages. Under the terms of Summorum Pontificum by Pope Benedict XVI, the Mass of Paul VI is known as the Ordinary Form of the Roman Rite.\\n\\nExtraordinary Form\\n\\nThe Tridentine Mass, as in the 1962 Roman Missal, is still authorized for use as an extraordinary form of the Roman Rite under the conditions indicated in the document Summorum Pontificum.\\n\\nOrdinariate Use\\n\\nThe Ordinariate Use is a form or variation of the Roman Rite, rather than a unique rite itself. During the Liturgy of the Eucharist, especially the Eucharistic Prayer, it is closest to other forms of the Roman Rite, while it differs more during the Liturgy of the Word and the Penitential Rite. The language used, which differs from that of the ICEL translation of the Roman Rite of Mass, is based upon the Book of Common Prayer, originally written in the 16th century. Prior to the establishment of the personal ordinariates, parishes in the United States were called \"Anglican Use\" and used the Book of Divine Worship, an adaptation of the Book of Common Prayer. The Book of Divine Worship has been replaced with the similar Divine Worship: The Missal for use in the ordinariates worldwide. Anglican liturgical rituals, whether those used in the ordinariates of the Catholic Church or in the various prayer books and missals of the Anglican Communion and other denominations trace their origin back to the Sarum Use, which was a variation of the Roman Rite used in England before introduction during the reign of Edward VI of the 1549 Book of Common Prayer, following the break from the Roman church under the previous monarch Henry VIII.\\n\\nIn the United States, under a Pastoral Provision in 1980, personal parishes were established that introduced adapted Anglican traditions to the Catholic Church from members\\' former Episcopal parishes. That provision also permitted, as an exception and on a case by case basis, the ordination of married former Episcopal ministers as Catholic priests. As personal parishes, these parishes were formerly part of the local Roman Catholic diocese, but accepted as members any former Anglican who wished to make use of the provision.\\n\\nOn 9 November 2009, Pope Benedict XVI established a worldwide provision for Anglicans who joined the church. This process set up personal ordinariates for former Anglicans and other persons entering the full communion of the Catholic Church. These ordinariates would be similar to dioceses, but encompassing entire regions or nations. Parishes belonging to an ordinariate would not be part of the local diocese. These ordinariates are charged with maintaining the Anglican liturgical, spiritual and pastoral traditions, and they have full faculties to celebrate the Eucharist and the other sacraments, the Liturgy of the Hours and other liturgical functions in accordance with the liturgical books proper to Anglican tradition, in revisions approved by the Holy See. This faculty does not exclude liturgical celebrations according to the Roman Rite.\\n\\nThe Personal Ordinariate of Our Lady of Walsingham was set up for England and Wales on 15 January 2011, and the Personal Ordinariate of the Chair of Saint Peter for the United States and Canada on 1 January 2012, and the Personal Ordinariate of Our Lady of the Southern Cross for Australia on 15 June 2012. As of 2017 it was decreed that all parishes in the United States established under the Pastoral Provision be transferred to the Ordinariate. Bishop Steven Lopes of the Personal Ordinariate of the Chair of Saint Peter has requested that terms such as \"Anglican Use\" and \"Anglican Ordinariate\" be avoided, saying \"Our clergy and faithful do not like being called Anglican, both because this is insensitive to actual Anglicans, and because it is a subtle way of suggesting that their entrance into full communion is less that total. We are Catholic in every sense.\"\\n\\nAlgonquian and Iroquoian Uses\\n\\nAlso called \"Indian Masses\", a number of variations on the Roman Rite developed in the Indian missions of Canada and the United States. These originated in the 17th century, and some remained in use until the Second Vatican Council. The priest\\'s parts remained in Latin, while the ordinaries sung by the choir were translated into the vernacular (e.g., Mohawk, Algonquin, Micmac, and Huron). They also generally featured a reduced cycle of native-language propers and hymns. At present they are rarely used.\\n\\nZaire Use\\n\\nThe Zaire Use is an inculturated variation of the Ordinary Form of the Roman Rite of the Roman Catholic Church. It has been used to a very limited extent in some African countries since the late 1970s.\\n\\nWestern Rites of \"Gallican\" type\\n\\nAmbrosian Rite\\n\\nThe Ambrosian Rite is celebrated in most of the Archdiocese of Milan, Italy, and in parts of some neighbouring dioceses in Italy and Switzerland. The language used is now usually Italian, rather than Latin. With some variant texts and minor difference in the order of readings, it is similar in form to the Roman Rite. Its classification as Gallican-related is disputed.\\n\\nRite of Braga\\n\\nThe Rite of Braga is used, but since 18 November 1971 only on an optional basis, in the Archdiocese of Braga in northern Portugal.\\n\\nMozarabic Rite\\n\\nThe Mozarabic Rite, which was prevalent throughout Spain in Visigothic times, is now celebrated only in limited locations, principally the cathedral of Toledo.\\n\\nCarthusian Rite\\nThe Carthusian rite is in use in a version revised in 1981. Apart from the new elements in this revision, it is substantially the rite of Grenoble in the 12th century, with some admixture from other sources. Among other differences from the Roman Order of Mass, the deacon prepares the gifts while the Epistle is being sung, the celebrating priest washes his hands twice at the offertory and says the eucharistic prayer with arms extended in the form of a cross except when using his hands for some specific action, and there is no blessing at the end of Mass.\\n\\nThis is now the only extant Mass rite of a Catholic religious order; but by virtue of the Ecclesia Dei indult some individuals or small groups are authorized to use some now defunct rites.\\n\\nWestern Rite of sui generis type\\n\\nBenedictine Rite\\n\\nThe Order of Saint Benedict has never had a rite of the Mass peculiar to it, but it keeps its very ancient Benedictine Rite of the Liturgy of the Hours.\\n\\nDefunct Catholic Western liturgical rites\\n\\nAfrican Rite\\n\\nIn Africa Proconsulare, located in present-day Tunisia (of which Carthage was the capital), the African Rite was used before the 7th-century Arab conquest. It was very close to the Roman Rite; so much so that Western liturgical traditions have been classified as belonging to two streams, the North African-Rome tradition, and the Gallican (in the broad sense) tradition encompassing the rest of the Western Roman Empire, including northern Italy.\\n\\nCeltic Rite \\n\\nThe ancient Celtic Rite was a composite of non-Roman ritual structures (possibly Antiochian) and texts not exempt from Roman influence, that was similar to the Mozarabic Rite in many respects and would have been used at least in parts of Ireland, Scotland, the northern part of England and perhaps even Wales, Cornwall and Somerset, before being authoritatively replaced by the Roman Rite in the early Middle Ages. \"Celtic\" is possibly a misnomer and it may owe its origins to Augustine\\'s re-evangelisation of the British Isles in the 6th century. Little is known of it, though several texts and liturgies survive. \\n\\nSome Christians–typically groups not in communion with the Roman Catholic Church, especially some Western Orthodox Christian communities in communion with Eastern Orthodox Churches, e.g. Celtic Orthodoxy–have attempted to breathe life into a reconstruction of the Celtic Rite the historical accuracy of which is debated. Historical evidence of this rite is found in the remnants of the Stowe (Lorrha) Missal.\\n\\nGallican Rite\\n\\nThe Gallican Rite is a retrospective term applied to the sum of the local variants, on similar lines to that designated elsewhere as the Celtic Rite (above) and the Mozarabic Rite, which faded from use in France by the end of the first millennium. It should not be confused with the so-called Neo-Gallican liturgical books published in various French dioceses after the Council of Trent, which had little or nothing to do with it.\\n\\nRegional Latin rites or uses\\nSeveral local rites (more properly uses or variants of the Roman Rite (most with Gallican elements some with Byzantine liturgical and tradition elements) of limited scope existed, but are now defunct.\\nThe Sarum Rite (more properly Sarum Use), a defunct variant on the Roman rite originating in the Diocese of Salisbury, which had come to be widely practised in England and Scotland around the 1530s, while the Protestant Reformation swept across continental Europe; practised alongside limited other variants such as the Use of York, Lincoln Use, Bangor Use, and Hereford Use. It has a legacy in its influence on later Anglican liturgy.\\nThe Cologne Use, used in the diocese of Cologne () prior to 1570.\\nThe Metz Use, created by Arnulf of Metz and Amalarius of Metz in the ninth century–used in Alsace-Lorraine, the Netherlands, and Flemish and Wallonian lands until the beginning of the twentieth century.\\nThe Lyonese Rite of the Diocese of Lyon, France, which some consider to have been (rather than Milan) the centre of diffusion of the Gallican liturgy; it is maintained in a few parishes in Lyon.\\nThe Nidaros Use, long defunct, based mainly on imported English liturgical books, used in pre-Reformation Norway.\\nThe Uppsala Use, suppressed during the Reformation, formerly the dominant variant of the Roman Rite used in northern Sweden.\\nThe  Aquileian Rite, a defunct rite originating in the former patriarchate of Aquileia in northern Italy.\\nThe Benevento Rite, a defunct Latin rite originated in this city in Italy.\\nThe Durham Rite (defunct: Durham, England)\\nThe Esztergom Use (defunct: Archdiocese of Esztergom, used between the 12th and 17th centuries primarily in the Archdiocese of Esztergom, and in its suffragan dioceses. Similar rites were also in Slovakia and in southern, central, and western Poland. These usages of Roman liturgy was the closest to Roman (today Vatican) rites with some small Byzantine-Slavic elements.\\n\\nRites of religious orders\\n\\nSome religious orders celebrated Mass according to rites of their own, dating from more than 200 years before the papal bull Quo primum. These rites were based on local usages and combined elements of the Roman and Gallican Rites. Following the Second Vatican Council, they have mostly been abandoned, except for the Carthusian Rite (see above). Religious orders of more recent origin have never had special rites.\\n\\nThe following previously existing rites of Mass, distinct from the Roman Rite, continue to be used on a limited basis by the permission of ecclesiastical superiors:\\nCarmelite Rite\\nCistercian Rite\\nDominican Rite\\nPremonstratensian or Norbertine Rite\\n\\nThe Catholic Encyclopedia applied the word \"rite\" also to the practices followed (to some extent even now, a century later) by certain Catholic religious orders, while at the same time stating that they in fact followed the Roman Rite:\\nFranciscan Rite\\nFriars Minor Capuchin Rite\\nServite Rite\\n\\nSee also\\nAlexandrian Rite\\nAntiochene Rite\\nArmenian Rite \\nByzantine Rite\\nCatholic particular churches and liturgical rites\\nEast Syriac Rite\\nWest Syriac Rite\\nGeneral Roman Calendar\\n\\nReferences\\n\\nExternal links\\nDom Fernand Cabrol\\'s The Mass of the Western Rites\\nNon-Roman Latin or Western Rites\\nAn African Interpretation of Liturgical Inculturation: The Rite Zairois\\n\\n \\nCategory:Western Christianity', 'Fernaldia pandurata (common name: loroco ) is a vine with edible flowers, widespread in Mexico and Central America.\\n\\nFernaldia pandurata is an important source of food in El Salvador and Guatemala. The plant\\'s buds and flowers are used for cooking in a variety of ways, including in pupusas.\\n\\nThe name \"loroco\" is used throughout Mesoamerica to refer to Fernaldia pandurata.\\n\\nFernaldia pandurata is an herbaceous vine with oblong-elliptical to broadly ovate leaves . long, 1.5–8\\xa0cm broad, inflorescences are generally somewhat shorter than the leaves, with 8–18 flowers, the pedicels 4–6\\xa0mm. long; bracts ovate,  long; calyx lobes ovate, acute or obtuse, 2–3\\xa0mm. long; corolla white within, greenish outside.\\n\\nReferences\\n\\n \\n León, J., H. Goldbach & J. Engels, 1979: Die genetischen Ressourcen der Kulturpflanzen Zentralamerikas., Int. Genbank CATIE/GTZ in Turrialba, Costa Rica, San Juan de Tibás, Costa Rica, 32 pp.\\n Morton, J. F., E. Alvarez & C. Quiñonez, 1990: Loroco, Fernaldia pandurata\\'\\' (Apocynaceae): a popular edible flower of Central America. Economic Botany 44, 301–310.\\n\\nExternal links\\n\\n Loroco in World Crops (English)\\n\\nCategory:Echiteae\\nCategory:Edible plants\\nCategory:Flora of Central America\\nCategory:Flora of Mexico\\nCategory:Salvadoran cuisine\\nCategory:Guatemalan cuisine\\nCategory:Plants described in 1844', 'Chester Earl Merrow (November 15, 1906 – February 10, 1974) was a U.S. Representative from New Hampshire.\\n\\nBorn in Center Ossipee, New Hampshire, Merrow attended the public schools and Brewster Free Academy in Wolfeboro from 1921 to 1925. He was graduated from Colby College, Waterville, Maine, in 1929 and from Teachers College (summers) (Columbia University), New York City, in 1937.\\n\\nMerrow was an instructor of science at Kents Hill School in Maine in 1929 and 1930 and at Montpelier Seminary from 1930 to 1937. He served as assistant headmaster of Montpelier Seminary from 1935 to 1938.  He was an instructor of political science and history at Vermont Junior College in Montpelier in 1937 and 1938.\\n\\nMerrow was a member of the New Hampshire House of Representatives in 1939 and 1940.  He became a radio news commentator and lecturer, and served as delegate to an international conference on education and cultural relations of the United Nations, held in London in 1945.  He was a congressional adviser to the first conference of the United Nations Educational, Scientific, and Cultural Organization (UNESCO) held in Paris in 1946. He served as a member of the United States delegation to UNESCO 1946-1949.\\n\\nMerrow was elected as a Republican to the Seventy-eighth and to the nine succeeding Congresses (January 3, 1943 – January 3, 1963). Merrow voted in favor of the Civil Rights Acts of 1957 and 1960. He was not a candidate for reelection in 1962 to the Eighty-eighth Congress, but was unsuccessful for nomination to the United States Senate.  Subsequently, he was Special Adviser on Community Relations, Department of State, from 1963 to 1968. He was an unsuccessful candidate for election in 1970 to the Ninety-second Congress and in 1972 to the Ninety-third Congress. He resided in Center Ossipee until his death there, February 10, 1974.\\nHe was interred in Chickville Cemetery.\\n\\nReferences\\n\\nCategory:1906 births\\nCategory:1974 deaths\\nCategory:Members of the United States House of Representatives from New Hampshire\\nCategory:Colby College alumni\\nCategory:Teachers College, Columbia University alumni\\nCategory:New Hampshire Republicans\\nCategory:Republican Party members of the United States House of Representatives\\nCategory:Members of the New Hampshire House of Representatives\\nCategory:20th-century American politicians\\nCategory:People from Ossipee, New Hampshire', 'HIS (\"Hightech Information System Limited\"; established 1987), is a Hong Kong-based graphics card manufacturer that produces AMD (formerly known as ATI) Radeon graphics cards. Its headquarters are in Hong Kong, with additional sales offices and distribution networks in Europe, the Middle East, North America and Asia Pacific Regions. The current distributor in Hong Kong is JunMax Technology.\\n\\nProducts \\nHIS manufactures and sells AMD Radeon series video cards. They are known for their IceQ cooling technology as well as producing the latest and fastest PCI cards like AMD Radeon RX 590, RX 5700 and RX 5700 XT.\\n\\nIn 2019, HIS launched new versions of the RX 5700 XT in pink and blue.\\n\\nReferences\\n\\nExternal links\\n HIS Ltd.\\n\\nCategory:Computer companies of Hong Kong\\nCategory:Graphics hardware companies\\nCategory:Computer companies established in 1987\\nCategory:Electronics companies established in 1987\\nCategory:Hong Kong brands', '__NOTOC__\\nAD 47 (XLVII) was a common year starting on Sunday (link will display the full calendar) of the Julian calendar. \"At the time, it was known as the Year of the Consulship of Claudius and Vitellius (or, less frequently, year 800 Ab urbe condita). The denomination AD 47 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.\\n\\nEvents\\n\\nBy place\\n\\nRoman Empire \\n Claudius revives the censorship and ludi saeculares, and organises the order of the Haruspices, with 60 members.\\n Gnaeus Domitius Corbulo is made commander of the Roman army in Germania Inferior. He conquers the Chauci and fights against the Cherusci and Frisians.\\n Cauci pirates led by the Roman deserter Gannascus ravage the Gallic coast; Corbulo uses the Rhine fleet against them. The Frisian revolt is suppressed.\\n Publius Ostorius Scapula replaces Aulus Plautius as governor of Britain. The south-east of the island is now a Roman province, while certain states on the south coast are ruled as a nominally independent client kingdom by Tiberius Claudius Cogidubnus, whose seat is probably at Fishbourne near Chichester. Ostorius immediately faces incursions from unconquered areas, which he puts down.\\n Corbulo orders the construction of the canal Fossa Corbulonis, between the Rhine and Meuse in the Netherlands, which connects the city Forum Hadriani (Voorburg).\\n Romans build the Traiectum fortification near the mouth of the Rhine, which will later grow to be the city of Utrecht.\\n Claudius founds the city Forum Claudii Vallensium (modern Martigny) in the Alpes Poeninae (Switzerland).\\n Musselburgh is founded in Roman Britain (Scotland).\\n\\nBy topic\\n\\nReligion \\n Ananias becomes high priest in Judaea.\\n Paul starts his evangelistic work.\\n</onlyinclude>\\n\\nBirths \\n Taejodae, Korean ruler of Goguryeo (d. 165)\\n\\nDeaths \\n Decimus Valerius Asiaticus, Roman politician and consul\\n Gaius Sallustius Crispus Passienus, Roman consul\\n Gnaeus Pompeius Magnus, Roman nobleman \\n Quintus Sanquinius Maximus, Roman politician\\n Vardanes I, king of the Parthian Empire\\n\\nReferences \\n\\n0047\\n\\nals:40er#47'])])\n",
            "first 10 titles:  ['Yangliuqing', 'Orana Australia Ltd', \"St. Mary's Church, Sønderborg\", 'Kalitta', 'Where Is Freedom?', 'Latin liturgical rites', 'Fernaldia pandurata', 'Chester Earl Merrow', 'Hightech Information System', 'AD 47']\n",
            "first 10 articles:  ['Yangliuqing () is a market town in Xiqing District, in the western suburbs of Tianjin, People\\'s Republic of China. Despite its relatively small size, it has been named since 2006 in the \"famous historical and cultural market towns in China\".\\n\\nIt is best known in China for creating nianhua or Yangliuqing nianhua. For more than 400 years, Yangliuqing has in effect specialised in the creation of these woodcuts for the New Year.  wood block prints using vivid colourschemes to portray traditional scenes of children\\'s games often interwoven with auspiciouse objects.\\n\\n, it had 27 residential communities () and 25 villages under its administration.\\n\\nShi Family Grand Courtyard\\n\\nShi Family Grand Courtyard (Tiānjīn Shí Jiā Dà Yuàn, 天津石家大院) is situated in Yangliuqing Town of Xiqing District, which is the former residence of wealthy merchant Shi Yuanshi - the 4th son of Shi Wancheng, one of the eight great masters in Tianjin. First built in 1875, it covers over 6,000 square meters, including large and small yards and over 200 folk houses, a theater and over 275 rooms that served as apartments and places of business and worship for this powerful family. Shifu Garden, which finished its expansion in October 2003, covers 1,200 square meters, incorporates the elegance of imperial garden and delicacy of south garden. Now the courtyard of Shi family covers about 10,000 square meters, which is called the first mansion in North China. Now it serves as the folk custom museum in Yangliuqing, which has a large collection of folk custom museum in Yanliuqing, which has a large collection of folk art pieces like Yanliuqing New Year pictures, brick sculpture.\\n\\nShi\\'s ancestor came from Dong\\'e County in Shandong Province, engaged in water transport of grain. As the wealth gradually accumulated, the Shi Family moved to Yangliuqing and bought large tracts of land and set up their residence. Shi Yuanshi came from the fourth generation of the family, who was a successful businessman and a good household manager, and the residence was thus enlarged for several times until it acquired the present scale. It is believed to be the first mansion in the west of Tianjin.\\n\\nThe residence is symmetric based on the axis formed by a passageway in the middle, on which there are four archways. On the east side of the courtyard, there are traditional single-story houses with rows of rooms around the four sides, which was once the living area for the Shi Family. The rooms on north side were the accountants\\' office. On the west are the major constructions including the family hall for worshipping Buddha, theater and the south reception room. On both sides of the residence are side yard rooms for maids and servants.\\n\\nToday, the Shi mansion, located in the township of Yangliuqing to the west of central Tianjin, stands as a surprisingly well-preserved monument to China\\'s pre-revolution mercantile spirit. It also serves as an on-location shoot for many of China\\'s popular historical dramas. Many of the rooms feature period furniture, paintings and calligraphy, and the extensive Shifu Garden.\\n\\nPart of the complex has been turned into the Yangliuqing Museum, which includes displays focused on symbolic aspects of the courtyards\\'  construction, local folk art and customs, and traditional period furnishings and crafts.\\n\\nSee also \\n\\nList of township-level divisions of Tianjin\\n\\nReferences \\n\\n http://arts.cultural-china.com/en/65Arts4795.html\\n\\nCategory:Towns in Tianjin', 'Orana Australia Ltd is a not-for-profit organisation that provides a diverse range of training and support services to over 650 people with disabilities and their families in South Australia.\\n\\nHistory\\nThe Mentally Retarded Children’s Society of SA Inc. was established in 1950 by a group of parents who wanted education, employment and accommodation opportunities for their children within the local community at a time when institutionalised care in Adelaide was their only alternative.\\n\\nThe society’s aims were to seek education or training facilities for people with intellectual disabilities, to establish sheltered workshops, and to establish residential hostels.\\n\\nA number of sheltered workshops were established, and in 1980, the name was changed to the Aboriginal word \"Orana\", which means \"Welcome\".\\n\\nToday, Orana provides assisted employment, assisted accommodation and respite services to people with intellectual disabilities.\\n\\nOrana\\'s current and previous clients include Mitsubishi Motors, Clipsal, RAA, Elders Limited, and Billycart Kids.\\n\\nOrana was one of the first disability service organisations to achieve Quality Accreditation.  The services and products they offer are:\\n\\n Packaging\\n Assembly\\n Sewing\\n Collating & Mailing\\n Furniture - Retail\\n Furniture – Manufacture for Commercial Market\\n Worm Farming\\n Work Crews\\n Pet & Grain – Retail\\n\\nIn 2018, after 65 years of bettering people’s lives, Orana identified a community need and expanded their operations into the aged care sector.\\n\\nAfter the unveiling of the Australian Government’s Commonwealth Home Support Programme (CHSP) and seeing it as a natural step of progression, Orana now provides quality tailored aged care at home.\\n\\nThe well-resourced organization delivers help across a range of areas, helping the elderly remain where they want to be - in the comfort of their own home during their later years.\\n\\nOrana continues with its mission to support people remain independent, valued and productive members of the community.\\n\\nReferences\\n\\nExternal links \\n \\n\\nCategory:Disability organisations based in Australia\\nCategory:Organisations based in South Australia', \"The St. Mary's Church is a church owned by the Church of Denmark in Sønderborg, Denmark and the church of the parish with the same name. Thanks to its location on a hill, the church building is very iconic for the city.\\n\\nHistory \\nIn the Middle Ages there was a leper colony on a hill just outside the city. It was named after Saint George and around 1300 the chapel of this leper colony stood in the place of the present St. Mary's Church. After the old parish church of the city, the St. Nicholas Church, was demolished around 1530, the Saint-George chapel became the new main church. Towards the end of the 16th century, John II, Duke of Schleswig-Holstein-Sonderburg commissioned the enlargement of the building in order to make it suitable for the function of the parish church of his city.\\n\\nThe current St. Mary's Church \\nIn 1595 a start was made on the partial demolition of the old church and the construction of the new church. Only parts of the old medieval church remained. From the medieval church, a medieval wooden wall cupboard dating from about 1400 remained. The solemn inauguration of the new parish church took place just before Christmas in 1600. In 1649 the George Church was renamed as the Mary Church. The name of Saint George stayed in the Danish names Sankt Jørgensgade and Jørgensbjerg.\\n\\nReferences \\n\\nCategory:Buildings and structures in Sønderborg Municipality\\nCategory:Churches in Denmark\\nCategory:Church of Denmark churches\", 'Kalitta may refer to:\\n\\nConnie Kalitta (born 1938), a retired American drag racer and CEO of the eponymous Kallita Air.\\nDoug Kalitta (born 1964), an American drag racer, nephew of Connie Kalitta and owner of Kalitta Charters.\\nScott Kalitta (1962-2008), an American drag racer and son of Connie Kalitta.\\nKalitta Air, a cargo airline flying Boeing 747 aircraft.\\nKalitta Charters, a cargo airline flying medium-sized aircraft.', \"Where Is Freedom? () is a 1954 Italian comedy-drama film directed by Roberto Rossellini. \\n \\nThe film had a troubled production because, after shooting some scenes, Rossellini lost interest in the film and abandoned the set. The work was completed after about a year, mainly from Mario Monicelli, with some scenes also shot by Lucio Fulci and Federico Fellini. Despite that, Rossellini is the sole credited director of the film.\\n\\nPlot \\nDifficulties and troubles of an ex-convict. Embittered and disillusioned by life, he will soon plan his return to prison.\\n\\nCast \\nTotò: Salvatore Lo Jacono \\nVera Molnar: Agnesina \\nNita Dover:  maratoneta di danza \\nFranca Faldini: Maria \\nLeopoldo Trieste: Abramo Piperno \\nAntonio Nicotra: maresciallo \\nSalvo Libassi:  maresciallo #2 \\nGiacomo Rondinella:  carcerato \\nUgo D'Alessio:  giudice\\nMario Castellani: pubblico ministero \\nVincenzo Talarico: avvocato difensore\\nPietro Carloni:  Pietro\\n\\nReferences\\n\\nExternal links\\n\\n   \\n\\nCategory:1954 films\\nCategory:Italian comedy-drama films\\nCategory:1950s comedy-drama films\\nCategory:Films directed by Roberto Rossellini\\nCategory:Commedia all'italiana\\nCategory:Films set in Rome\\nCategory:Italian films\\nCategory:Films produced by Dino De Laurentiis\\nCategory:Films produced by Carlo Ponti\", 'Latin liturgical rites, or Western liturgical rites, are Catholic liturgical rites employed by the Latin Church, the largest particular church sui iuris of the Catholic Church, that originated in Europe where the Latin language once dominated. Its language is now known as Ecclesiastical Latin. The most used rite is the Roman Rite.\\n\\nThe Latin rites were for many centuries no less numerous than the liturgical rites of the Eastern autonomous particular Churches. Their number is now much reduced. In the aftermath of the Council of Trent, in 1568 and 1570 Pope Pius V suppressed the Breviaries and Missals that could not be shown to have an antiquity of at least two centuries (see Tridentine Mass and Roman Missal). Many local rites that remained legitimate even after this decree were abandoned voluntarily, especially in the 19th century. In the second half of the 20th century, most of the religious orders that had a distinct liturgical rite chose to adopt in its place the Roman Rite as revised in accordance with the decrees of the Second Vatican Council (see Mass of Paul VI). A few such liturgical rites persist today for the celebration of Mass, since 1965–1970 in revised forms, but the distinct liturgical rites for celebrating the other sacraments have been almost completely abandoned.\\n\\nLiturgical rites currently in use within the Latin Church\\n\\nRoman Rite\\n\\nThe Roman Rite is by far the most widely used. Like other liturgical rites, it developed over time, with newer forms replacing the older. It underwent many changes in the first millennium and a half of its existence (see Pre-Tridentine Mass). The forms that Pope Pius V, as requested by the Council of Trent, established in the 1560s and 1570s underwent repeated minor variations in the centuries immediately following. Each new typical edition (the edition to which other printings are to conform) of the Roman Missal (see Tridentine Mass) and of the other liturgical books superseded the previous one.\\n\\nThe 20th century saw more profound changes. Pope Pius X radically rearranged the Psalter of the Breviary and altered the rubrics of the Mass. Later popes continued to make such changes, beginning with Pope Pius XII, who significantly revised the Holy Week ceremonies and certain other aspects of the Roman Missal in 1955.\\n\\nOrdinary Form\\n\\nThe Second Vatican Council (1962–1965) was followed by a general revision of the rites of all the Roman Rite sacraments, including the Eucharist. As before, each new typical edition of an official liturgical book supersedes the previous one. Thus, the 1970 Roman Missal, which superseded the 1962 edition, was superseded by the edition of 1975. The 2002 edition in turn supersedes the 1975 edition both in Latin and, as official translations into each language appear, also in the vernacular languages. Under the terms of Summorum Pontificum by Pope Benedict XVI, the Mass of Paul VI is known as the Ordinary Form of the Roman Rite.\\n\\nExtraordinary Form\\n\\nThe Tridentine Mass, as in the 1962 Roman Missal, is still authorized for use as an extraordinary form of the Roman Rite under the conditions indicated in the document Summorum Pontificum.\\n\\nOrdinariate Use\\n\\nThe Ordinariate Use is a form or variation of the Roman Rite, rather than a unique rite itself. During the Liturgy of the Eucharist, especially the Eucharistic Prayer, it is closest to other forms of the Roman Rite, while it differs more during the Liturgy of the Word and the Penitential Rite. The language used, which differs from that of the ICEL translation of the Roman Rite of Mass, is based upon the Book of Common Prayer, originally written in the 16th century. Prior to the establishment of the personal ordinariates, parishes in the United States were called \"Anglican Use\" and used the Book of Divine Worship, an adaptation of the Book of Common Prayer. The Book of Divine Worship has been replaced with the similar Divine Worship: The Missal for use in the ordinariates worldwide. Anglican liturgical rituals, whether those used in the ordinariates of the Catholic Church or in the various prayer books and missals of the Anglican Communion and other denominations trace their origin back to the Sarum Use, which was a variation of the Roman Rite used in England before introduction during the reign of Edward VI of the 1549 Book of Common Prayer, following the break from the Roman church under the previous monarch Henry VIII.\\n\\nIn the United States, under a Pastoral Provision in 1980, personal parishes were established that introduced adapted Anglican traditions to the Catholic Church from members\\' former Episcopal parishes. That provision also permitted, as an exception and on a case by case basis, the ordination of married former Episcopal ministers as Catholic priests. As personal parishes, these parishes were formerly part of the local Roman Catholic diocese, but accepted as members any former Anglican who wished to make use of the provision.\\n\\nOn 9 November 2009, Pope Benedict XVI established a worldwide provision for Anglicans who joined the church. This process set up personal ordinariates for former Anglicans and other persons entering the full communion of the Catholic Church. These ordinariates would be similar to dioceses, but encompassing entire regions or nations. Parishes belonging to an ordinariate would not be part of the local diocese. These ordinariates are charged with maintaining the Anglican liturgical, spiritual and pastoral traditions, and they have full faculties to celebrate the Eucharist and the other sacraments, the Liturgy of the Hours and other liturgical functions in accordance with the liturgical books proper to Anglican tradition, in revisions approved by the Holy See. This faculty does not exclude liturgical celebrations according to the Roman Rite.\\n\\nThe Personal Ordinariate of Our Lady of Walsingham was set up for England and Wales on 15 January 2011, and the Personal Ordinariate of the Chair of Saint Peter for the United States and Canada on 1 January 2012, and the Personal Ordinariate of Our Lady of the Southern Cross for Australia on 15 June 2012. As of 2017 it was decreed that all parishes in the United States established under the Pastoral Provision be transferred to the Ordinariate. Bishop Steven Lopes of the Personal Ordinariate of the Chair of Saint Peter has requested that terms such as \"Anglican Use\" and \"Anglican Ordinariate\" be avoided, saying \"Our clergy and faithful do not like being called Anglican, both because this is insensitive to actual Anglicans, and because it is a subtle way of suggesting that their entrance into full communion is less that total. We are Catholic in every sense.\"\\n\\nAlgonquian and Iroquoian Uses\\n\\nAlso called \"Indian Masses\", a number of variations on the Roman Rite developed in the Indian missions of Canada and the United States. These originated in the 17th century, and some remained in use until the Second Vatican Council. The priest\\'s parts remained in Latin, while the ordinaries sung by the choir were translated into the vernacular (e.g., Mohawk, Algonquin, Micmac, and Huron). They also generally featured a reduced cycle of native-language propers and hymns. At present they are rarely used.\\n\\nZaire Use\\n\\nThe Zaire Use is an inculturated variation of the Ordinary Form of the Roman Rite of the Roman Catholic Church. It has been used to a very limited extent in some African countries since the late 1970s.\\n\\nWestern Rites of \"Gallican\" type\\n\\nAmbrosian Rite\\n\\nThe Ambrosian Rite is celebrated in most of the Archdiocese of Milan, Italy, and in parts of some neighbouring dioceses in Italy and Switzerland. The language used is now usually Italian, rather than Latin. With some variant texts and minor difference in the order of readings, it is similar in form to the Roman Rite. Its classification as Gallican-related is disputed.\\n\\nRite of Braga\\n\\nThe Rite of Braga is used, but since 18 November 1971 only on an optional basis, in the Archdiocese of Braga in northern Portugal.\\n\\nMozarabic Rite\\n\\nThe Mozarabic Rite, which was prevalent throughout Spain in Visigothic times, is now celebrated only in limited locations, principally the cathedral of Toledo.\\n\\nCarthusian Rite\\nThe Carthusian rite is in use in a version revised in 1981. Apart from the new elements in this revision, it is substantially the rite of Grenoble in the 12th century, with some admixture from other sources. Among other differences from the Roman Order of Mass, the deacon prepares the gifts while the Epistle is being sung, the celebrating priest washes his hands twice at the offertory and says the eucharistic prayer with arms extended in the form of a cross except when using his hands for some specific action, and there is no blessing at the end of Mass.\\n\\nThis is now the only extant Mass rite of a Catholic religious order; but by virtue of the Ecclesia Dei indult some individuals or small groups are authorized to use some now defunct rites.\\n\\nWestern Rite of sui generis type\\n\\nBenedictine Rite\\n\\nThe Order of Saint Benedict has never had a rite of the Mass peculiar to it, but it keeps its very ancient Benedictine Rite of the Liturgy of the Hours.\\n\\nDefunct Catholic Western liturgical rites\\n\\nAfrican Rite\\n\\nIn Africa Proconsulare, located in present-day Tunisia (of which Carthage was the capital), the African Rite was used before the 7th-century Arab conquest. It was very close to the Roman Rite; so much so that Western liturgical traditions have been classified as belonging to two streams, the North African-Rome tradition, and the Gallican (in the broad sense) tradition encompassing the rest of the Western Roman Empire, including northern Italy.\\n\\nCeltic Rite \\n\\nThe ancient Celtic Rite was a composite of non-Roman ritual structures (possibly Antiochian) and texts not exempt from Roman influence, that was similar to the Mozarabic Rite in many respects and would have been used at least in parts of Ireland, Scotland, the northern part of England and perhaps even Wales, Cornwall and Somerset, before being authoritatively replaced by the Roman Rite in the early Middle Ages. \"Celtic\" is possibly a misnomer and it may owe its origins to Augustine\\'s re-evangelisation of the British Isles in the 6th century. Little is known of it, though several texts and liturgies survive. \\n\\nSome Christians–typically groups not in communion with the Roman Catholic Church, especially some Western Orthodox Christian communities in communion with Eastern Orthodox Churches, e.g. Celtic Orthodoxy–have attempted to breathe life into a reconstruction of the Celtic Rite the historical accuracy of which is debated. Historical evidence of this rite is found in the remnants of the Stowe (Lorrha) Missal.\\n\\nGallican Rite\\n\\nThe Gallican Rite is a retrospective term applied to the sum of the local variants, on similar lines to that designated elsewhere as the Celtic Rite (above) and the Mozarabic Rite, which faded from use in France by the end of the first millennium. It should not be confused with the so-called Neo-Gallican liturgical books published in various French dioceses after the Council of Trent, which had little or nothing to do with it.\\n\\nRegional Latin rites or uses\\nSeveral local rites (more properly uses or variants of the Roman Rite (most with Gallican elements some with Byzantine liturgical and tradition elements) of limited scope existed, but are now defunct.\\nThe Sarum Rite (more properly Sarum Use), a defunct variant on the Roman rite originating in the Diocese of Salisbury, which had come to be widely practised in England and Scotland around the 1530s, while the Protestant Reformation swept across continental Europe; practised alongside limited other variants such as the Use of York, Lincoln Use, Bangor Use, and Hereford Use. It has a legacy in its influence on later Anglican liturgy.\\nThe Cologne Use, used in the diocese of Cologne () prior to 1570.\\nThe Metz Use, created by Arnulf of Metz and Amalarius of Metz in the ninth century–used in Alsace-Lorraine, the Netherlands, and Flemish and Wallonian lands until the beginning of the twentieth century.\\nThe Lyonese Rite of the Diocese of Lyon, France, which some consider to have been (rather than Milan) the centre of diffusion of the Gallican liturgy; it is maintained in a few parishes in Lyon.\\nThe Nidaros Use, long defunct, based mainly on imported English liturgical books, used in pre-Reformation Norway.\\nThe Uppsala Use, suppressed during the Reformation, formerly the dominant variant of the Roman Rite used in northern Sweden.\\nThe  Aquileian Rite, a defunct rite originating in the former patriarchate of Aquileia in northern Italy.\\nThe Benevento Rite, a defunct Latin rite originated in this city in Italy.\\nThe Durham Rite (defunct: Durham, England)\\nThe Esztergom Use (defunct: Archdiocese of Esztergom, used between the 12th and 17th centuries primarily in the Archdiocese of Esztergom, and in its suffragan dioceses. Similar rites were also in Slovakia and in southern, central, and western Poland. These usages of Roman liturgy was the closest to Roman (today Vatican) rites with some small Byzantine-Slavic elements.\\n\\nRites of religious orders\\n\\nSome religious orders celebrated Mass according to rites of their own, dating from more than 200 years before the papal bull Quo primum. These rites were based on local usages and combined elements of the Roman and Gallican Rites. Following the Second Vatican Council, they have mostly been abandoned, except for the Carthusian Rite (see above). Religious orders of more recent origin have never had special rites.\\n\\nThe following previously existing rites of Mass, distinct from the Roman Rite, continue to be used on a limited basis by the permission of ecclesiastical superiors:\\nCarmelite Rite\\nCistercian Rite\\nDominican Rite\\nPremonstratensian or Norbertine Rite\\n\\nThe Catholic Encyclopedia applied the word \"rite\" also to the practices followed (to some extent even now, a century later) by certain Catholic religious orders, while at the same time stating that they in fact followed the Roman Rite:\\nFranciscan Rite\\nFriars Minor Capuchin Rite\\nServite Rite\\n\\nSee also\\nAlexandrian Rite\\nAntiochene Rite\\nArmenian Rite \\nByzantine Rite\\nCatholic particular churches and liturgical rites\\nEast Syriac Rite\\nWest Syriac Rite\\nGeneral Roman Calendar\\n\\nReferences\\n\\nExternal links\\nDom Fernand Cabrol\\'s The Mass of the Western Rites\\nNon-Roman Latin or Western Rites\\nAn African Interpretation of Liturgical Inculturation: The Rite Zairois\\n\\n \\nCategory:Western Christianity', 'Fernaldia pandurata (common name: loroco ) is a vine with edible flowers, widespread in Mexico and Central America.\\n\\nFernaldia pandurata is an important source of food in El Salvador and Guatemala. The plant\\'s buds and flowers are used for cooking in a variety of ways, including in pupusas.\\n\\nThe name \"loroco\" is used throughout Mesoamerica to refer to Fernaldia pandurata.\\n\\nFernaldia pandurata is an herbaceous vine with oblong-elliptical to broadly ovate leaves . long, 1.5–8\\xa0cm broad, inflorescences are generally somewhat shorter than the leaves, with 8–18 flowers, the pedicels 4–6\\xa0mm. long; bracts ovate,  long; calyx lobes ovate, acute or obtuse, 2–3\\xa0mm. long; corolla white within, greenish outside.\\n\\nReferences\\n\\n \\n León, J., H. Goldbach & J. Engels, 1979: Die genetischen Ressourcen der Kulturpflanzen Zentralamerikas., Int. Genbank CATIE/GTZ in Turrialba, Costa Rica, San Juan de Tibás, Costa Rica, 32 pp.\\n Morton, J. F., E. Alvarez & C. Quiñonez, 1990: Loroco, Fernaldia pandurata\\'\\' (Apocynaceae): a popular edible flower of Central America. Economic Botany 44, 301–310.\\n\\nExternal links\\n\\n Loroco in World Crops (English)\\n\\nCategory:Echiteae\\nCategory:Edible plants\\nCategory:Flora of Central America\\nCategory:Flora of Mexico\\nCategory:Salvadoran cuisine\\nCategory:Guatemalan cuisine\\nCategory:Plants described in 1844', 'Chester Earl Merrow (November 15, 1906 – February 10, 1974) was a U.S. Representative from New Hampshire.\\n\\nBorn in Center Ossipee, New Hampshire, Merrow attended the public schools and Brewster Free Academy in Wolfeboro from 1921 to 1925. He was graduated from Colby College, Waterville, Maine, in 1929 and from Teachers College (summers) (Columbia University), New York City, in 1937.\\n\\nMerrow was an instructor of science at Kents Hill School in Maine in 1929 and 1930 and at Montpelier Seminary from 1930 to 1937. He served as assistant headmaster of Montpelier Seminary from 1935 to 1938.  He was an instructor of political science and history at Vermont Junior College in Montpelier in 1937 and 1938.\\n\\nMerrow was a member of the New Hampshire House of Representatives in 1939 and 1940.  He became a radio news commentator and lecturer, and served as delegate to an international conference on education and cultural relations of the United Nations, held in London in 1945.  He was a congressional adviser to the first conference of the United Nations Educational, Scientific, and Cultural Organization (UNESCO) held in Paris in 1946. He served as a member of the United States delegation to UNESCO 1946-1949.\\n\\nMerrow was elected as a Republican to the Seventy-eighth and to the nine succeeding Congresses (January 3, 1943 – January 3, 1963). Merrow voted in favor of the Civil Rights Acts of 1957 and 1960. He was not a candidate for reelection in 1962 to the Eighty-eighth Congress, but was unsuccessful for nomination to the United States Senate.  Subsequently, he was Special Adviser on Community Relations, Department of State, from 1963 to 1968. He was an unsuccessful candidate for election in 1970 to the Ninety-second Congress and in 1972 to the Ninety-third Congress. He resided in Center Ossipee until his death there, February 10, 1974.\\nHe was interred in Chickville Cemetery.\\n\\nReferences\\n\\nCategory:1906 births\\nCategory:1974 deaths\\nCategory:Members of the United States House of Representatives from New Hampshire\\nCategory:Colby College alumni\\nCategory:Teachers College, Columbia University alumni\\nCategory:New Hampshire Republicans\\nCategory:Republican Party members of the United States House of Representatives\\nCategory:Members of the New Hampshire House of Representatives\\nCategory:20th-century American politicians\\nCategory:People from Ossipee, New Hampshire', 'HIS (\"Hightech Information System Limited\"; established 1987), is a Hong Kong-based graphics card manufacturer that produces AMD (formerly known as ATI) Radeon graphics cards. Its headquarters are in Hong Kong, with additional sales offices and distribution networks in Europe, the Middle East, North America and Asia Pacific Regions. The current distributor in Hong Kong is JunMax Technology.\\n\\nProducts \\nHIS manufactures and sells AMD Radeon series video cards. They are known for their IceQ cooling technology as well as producing the latest and fastest PCI cards like AMD Radeon RX 590, RX 5700 and RX 5700 XT.\\n\\nIn 2019, HIS launched new versions of the RX 5700 XT in pink and blue.\\n\\nReferences\\n\\nExternal links\\n HIS Ltd.\\n\\nCategory:Computer companies of Hong Kong\\nCategory:Graphics hardware companies\\nCategory:Computer companies established in 1987\\nCategory:Electronics companies established in 1987\\nCategory:Hong Kong brands', '__NOTOC__\\nAD 47 (XLVII) was a common year starting on Sunday (link will display the full calendar) of the Julian calendar. \"At the time, it was known as the Year of the Consulship of Claudius and Vitellius (or, less frequently, year 800 Ab urbe condita). The denomination AD 47 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.\\n\\nEvents\\n\\nBy place\\n\\nRoman Empire \\n Claudius revives the censorship and ludi saeculares, and organises the order of the Haruspices, with 60 members.\\n Gnaeus Domitius Corbulo is made commander of the Roman army in Germania Inferior. He conquers the Chauci and fights against the Cherusci and Frisians.\\n Cauci pirates led by the Roman deserter Gannascus ravage the Gallic coast; Corbulo uses the Rhine fleet against them. The Frisian revolt is suppressed.\\n Publius Ostorius Scapula replaces Aulus Plautius as governor of Britain. The south-east of the island is now a Roman province, while certain states on the south coast are ruled as a nominally independent client kingdom by Tiberius Claudius Cogidubnus, whose seat is probably at Fishbourne near Chichester. Ostorius immediately faces incursions from unconquered areas, which he puts down.\\n Corbulo orders the construction of the canal Fossa Corbulonis, between the Rhine and Meuse in the Netherlands, which connects the city Forum Hadriani (Voorburg).\\n Romans build the Traiectum fortification near the mouth of the Rhine, which will later grow to be the city of Utrecht.\\n Claudius founds the city Forum Claudii Vallensium (modern Martigny) in the Alpes Poeninae (Switzerland).\\n Musselburgh is founded in Roman Britain (Scotland).\\n\\nBy topic\\n\\nReligion \\n Ananias becomes high priest in Judaea.\\n Paul starts his evangelistic work.\\n</onlyinclude>\\n\\nBirths \\n Taejodae, Korean ruler of Goguryeo (d. 165)\\n\\nDeaths \\n Decimus Valerius Asiaticus, Roman politician and consul\\n Gaius Sallustius Crispus Passienus, Roman consul\\n Gnaeus Pompeius Magnus, Roman nobleman \\n Quintus Sanquinius Maximus, Roman politician\\n Vardanes I, king of the Parthian Empire\\n\\nReferences \\n\\n0047\\n\\nals:40er#47']\n",
            "first article:  Yangliuqing () is a market town in Xiqing District, in the western suburbs of Tianjin, People's Republic of China. Despite its relatively small size, it has been named since 2006 in the \"famous historical and cultural market towns in China\".\n",
            "\n",
            "It is best known in China for creating nianhua or Yangliuqing nianhua. For more than 400 years, Yangliuqing has in effect specialised in the creation of these woodcuts for the New Year.  wood block prints using vivid colourschemes to portray traditional scenes of children's games often interwoven with auspiciouse objects.\n",
            "\n",
            ", it had 27 residential communities () and 25 villages under its administration.\n",
            "\n",
            "Shi Family Grand Courtyard\n",
            "\n",
            "Shi Family Grand Courtyard (Tiānjīn Shí Jiā Dà Yuàn, 天津石家大院) is situated in Yangliuqing Town of Xiqing District, which is the former residence of wealthy merchant Shi Yuanshi - the 4th son of Shi Wancheng, one of the eight great masters in Tianjin. First built in 1875, it covers over 6,000 square meters, including large and small yards and over 200 folk houses, a theater and over 275 rooms that served as apartments and places of business and worship for this powerful family. Shifu Garden, which finished its expansion in October 2003, covers 1,200 square meters, incorporates the elegance of imperial garden and delicacy of south garden. Now the courtyard of Shi family covers about 10,000 square meters, which is called the first mansion in North China. Now it serves as the folk custom museum in Yangliuqing, which has a large collection of folk custom museum in Yanliuqing, which has a large collection of folk art pieces like Yanliuqing New Year pictures, brick sculpture.\n",
            "\n",
            "Shi's ancestor came from Dong'e County in Shandong Province, engaged in water transport of grain. As the wealth gradually accumulated, the Shi Family moved to Yangliuqing and bought large tracts of land and set up their residence. Shi Yuanshi came from the fourth generation of the family, who was a successful businessman and a good household manager, and the residence was thus enlarged for several times until it acquired the present scale. It is believed to be the first mansion in the west of Tianjin.\n",
            "\n",
            "The residence is symmetric based on the axis formed by a passageway in the middle, on which there are four archways. On the east side of the courtyard, there are traditional single-story houses with rows of rooms around the four sides, which was once the living area for the Shi Family. The rooms on north side were the accountants' office. On the west are the major constructions including the family hall for worshipping Buddha, theater and the south reception room. On both sides of the residence are side yard rooms for maids and servants.\n",
            "\n",
            "Today, the Shi mansion, located in the township of Yangliuqing to the west of central Tianjin, stands as a surprisingly well-preserved monument to China's pre-revolution mercantile spirit. It also serves as an on-location shoot for many of China's popular historical dramas. Many of the rooms feature period furniture, paintings and calligraphy, and the extensive Shifu Garden.\n",
            "\n",
            "Part of the complex has been turned into the Yangliuqing Museum, which includes displays focused on symbolic aspects of the courtyards'  construction, local folk art and customs, and traditional period furnishings and crafts.\n",
            "\n",
            "See also \n",
            "\n",
            "List of township-level divisions of Tianjin\n",
            "\n",
            "References \n",
            "\n",
            " http://arts.cultural-china.com/en/65Arts4795.html\n",
            "\n",
            "Category:Towns in Tianjin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DriQikaMnxv"
      },
      "source": [
        "### 5.1.3 Process Pretraining Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCXBiH9uiNBO"
      },
      "source": [
        "Some theory on preprocessing: https://mlexplained.com/2019/11/06/a-deep-dive-into-the-wonderful-world-of-preprocessing-in-nlp/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mwz_T5I5v_zw"
      },
      "source": [
        "# A likely source of weak model performance might be the tokenization step in \n",
        "# split_sentences_by_words() because it ignores punctuation and creates seperate\n",
        "# tokens for all punctuation symbols. This might not be anything the model can't\n",
        "# handle as the punctiuation symbols each have their own token ID so they are\n",
        "# part of the vocab and might even be expected to appear during model training.\n",
        "# It might be of use to experiment with a batch where all punctiuation is\n",
        "# removed, especially if loss doesn't decrease as desired during training.\n",
        "# Maybe use NLTK for punctuation removal!\n",
        "\n",
        "class PretrainingData(Dataset):\n",
        "  \n",
        "  def __init__(self, tokenizer):\n",
        "    # Initialize pretrained tokenizer\n",
        "    self.tokenizer = tokenizer #transformers.BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "    # Download and process the English Wikipedia dump\n",
        "    self.wiki_dump = wiki_data_test # dataset #load_dataset('wikipedia', '20200501.en', split='train')\n",
        "    self.wiki_dump_sentences_split = self.split_documents_by_sentences(self.wiki_dump)\n",
        "    self.wiki_dump_words_split = self.split_sentences_by_words(self.wiki_dump_sentences_split)\n",
        "    self.max_sentence_length = self.get_max_sent_length(self.wiki_dump_words_split)\n",
        "\n",
        "\n",
        "  ##############################################################################\n",
        "  # Preprocessing steps\n",
        "  ##############################################################################\n",
        "\n",
        "  def split_documents_by_sentences(self, data_ordered_dict):\n",
        "    \"\"\"\n",
        "    Splits every document into sentences\n",
        "\n",
        "    Args:\n",
        "      data_ordered_dict: A Python orderedDict containing all training documents\n",
        "        where the raw text dumps are indexed by 'text'\n",
        "\n",
        "    Returns:\n",
        "      doc_split_by_sentences: A list of lists where list[_] contains a full\n",
        "        document and list[_][_] contains a sentence from a given document\n",
        "    \"\"\"\n",
        "    docs_split_by_sentences = []\n",
        "    for doc in data_ordered_dict['text']:\n",
        "      sentences = sent_tokenize(doc)\n",
        "      docs_split_by_sentences.append(sentences)\n",
        "    return docs_split_by_sentences\n",
        "\n",
        "\n",
        "  def split_sentences_by_words(self, docsSplitBySentences):\n",
        "    \"\"\"\n",
        "    Reads the output of split_documents_by_sentences() and uses splits every\n",
        "      sentence (list[_][_]) on individual words. The pre-tokenization makes it\n",
        "      easier to determine the max sentence length in the dataset to which the\n",
        "      other sentences should be padded\n",
        "\n",
        "    Args:\n",
        "      docsSplitBySentences: List of lists\n",
        "        docsSplitBySentences[_] returns a document\n",
        "        docsSplitBySentences[_][_] returns a sentence from a document\n",
        "\n",
        "    Returns:\n",
        "      tokenized_wiki_data: List of list of lists\n",
        "        tokenized_wiki_data[_] returns a document\n",
        "        tokenized_wiki_data[_][_] returns a sentence from a document\n",
        "        tokenized_wiki_data[_][_][_] returns a word from a sentence from a doc\n",
        "    \"\"\"\n",
        "    # Constructs a list of lists where the length of the first dimension is the\n",
        "    # number of docuemnts in the batch for use in split_sentences_by_words()\n",
        "    tokenized_wiki_data = [[] for _ in range(len(docsSplitBySentences))]\n",
        "    # The new empty list gets zipped with our documents batch, nothing gets cut \n",
        "    # off as the new list was made to have the same length as the batch.\n",
        "    # We iterate over the zipped list and for every entry (representing a document)\n",
        "    # we iterate again over all sentences in that document in order to tokenize\n",
        "    # each one and append the sentences (now lists of individual tokens) to the\n",
        "    # respective entry (the according document).\n",
        "    for doc, entry in zip(docsSplitBySentences, tokenized_wiki_data):\n",
        "      for sent in doc:\n",
        "        entry.append(self.tokenizer.tokenize(sent))\n",
        "    return tokenized_wiki_data\n",
        "\n",
        "\n",
        "    # TODO can we replace this with batch_encode_plus/encode_plus/prepare_for_model and move it into __getitem__() for more memory efficiecy?\n",
        "    # Also add return_tensors='pt'\n",
        "    # https://huggingface.co/transformers/internal/tokenization_utils.html\n",
        "    def preprocess_dataset(batch, max_sequence_length):\n",
        "      preprocessed_batch = []\n",
        "      for doc in batch:\n",
        "        preprocessed_batch.append(tokenizer(text=doc, \n",
        "                                            padding='max_length', \n",
        "                                            max_length=max_sequence_length,\n",
        "                                            return_special_tokens_mask=True))\n",
        "      return preprocessed_dataset\n",
        "\n",
        "\n",
        "  ##############################################################################\n",
        "  # Helper functions\n",
        "  ##############################################################################\n",
        "  \n",
        "  def get_batch_size(self):\n",
        "    \"\"\"\n",
        "    Returns the number of docuemnts in a batch\n",
        "    \"\"\"\n",
        "    return len(self.wiki_dump_words_split)\n",
        "\n",
        "\n",
        "  # TODO get_max_doc_length() and get_max_sent_length() can be made into one function\n",
        "\n",
        "  def get_max_doc_length(self, tokenized_batch):\n",
        "    \"\"\"\n",
        "    Returns the number of senteces the longest document in the batch has\n",
        "\n",
        "    Args:\n",
        "      tokenized_batch:\n",
        "\n",
        "    Returns:\n",
        "      longest_doc_len: The number of sentences of the longest document, when \n",
        "      measured by how many sentences it consits of.\n",
        "    \"\"\"\n",
        "    longest_doc_len = 0\n",
        "    for doc in tokenized_batch:\n",
        "      if len(doc) >= longest_doc_len:\n",
        "        longest_doc_len = len(doc)\n",
        "      else:\n",
        "        continue\n",
        "    return longest_doc_len\n",
        "\n",
        "  # Don't compute this for the whole dataset, we just need it for a batch so we can get a tensor of uniform size\n",
        "  def get_max_sent_length(self, tokenized_batch):\n",
        "    longest_sentence_len = 0\n",
        "    for doc in tokenized_batch:\n",
        "      # Tokenized documents have a 1 added as the last entry instead of a sentence\n",
        "      for sent in doc[:-1]:\n",
        "        if len(sent) >= longest_sentence_len:\n",
        "          longest_sentence_len = len(sent)\n",
        "        else:\n",
        "          continue\n",
        "    return longest_sentence_len\n",
        "\n",
        "  ##############################################################################\n",
        "  # Interface functions\n",
        "  ##############################################################################\n",
        "\n",
        "  def __len__(self):\n",
        "    # References a different function with a better name for more readability\n",
        "    get_batch_size()\n",
        "\n",
        "  def __getitem__(self):\n",
        "    # References a different function with a better name for more readability\n",
        "    get_doc_from_batch()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXQFUfbHU76V"
      },
      "source": [
        "### 5.1.4 Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhoVcy9J_wME",
        "outputId": "d9a33381-84d4-4abb-c953-c4aa948cb91c"
      },
      "source": [
        "doc1sent1 = \"this is supposed to be the very first sentence in the dummy dataset for fast experimentation\"\n",
        "doc1sent2 = \"we're using this as a small test case to see how the DataLoader and Dataset class work in detail\"\n",
        "doc1sent3 = \"I'm going to add a few more sentences at the end of every document to inspect the batching a bit more\"\n",
        "doc1sent4 = \"let's also make the docuemnts of unequal size to see if that's a problem\"\n",
        "doc2sent1 = \"also we have two documents here to see which encoding function from huggingface works best\"\n",
        "doc2sent2 = \"let's write a very rudimentary and fast tokenization pipeline and encode these texts into tensor outputs\"\n",
        "doc2sent3 = \"hopefully the fact that the documents have an unequal number of sentences is not a problem\"\n",
        "doc3sent1 = \"and what happens if we even add a third docuemnt\"\n",
        "doc3sent2 = \"wow insanity I wonder if our DataLoader can handle such vast amounts of data\"\n",
        "doc3sent3 = \"ok let us add another sentence here for ultimate clarity\"\n",
        "\n",
        "dummy_data = [[doc1sent1, doc1sent2, doc1sent3], [doc2sent1, doc2sent2, doc2sent3], [doc3sent1, doc3sent2, doc3sent3]]\n",
        "\n",
        "dummy_data_pretokenized = [[] for _ in range(len(dummy_data))]\n",
        "\n",
        "for doc_id, doc in enumerate(dummy_data):\n",
        "  for sent in doc:\n",
        "    dummy_data_pretokenized[doc_id].append(sent.split())\n",
        "\n",
        "\n",
        "class DummyData(Dataset):\n",
        "  def __init__(self, tokenizer):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.dataset = dummy_data_pretokenized # has the exact same format as our wiki dataset so everything that works here should work there too\n",
        "    self.max_length = 30\n",
        "    self.padding_strategy = transformers.tokenization_utils_base.PaddingStrategy('max_length')\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "\n",
        "  def __getitem__(self, id): \n",
        "    return self.tokenizer.batch_encode_plus(self.dataset[id], # batch-encodes a whole document at [id] at once\n",
        "                                            padding=self.padding_strategy, \n",
        "                                            is_split_into_words=True, \n",
        "                                            return_tensors='pt',\n",
        "                                            return_special_tokens_mask=True)\n",
        "\n",
        "tokenizer = transformers.BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# The following will happen inside train()\n",
        "dummy_data_processed = DummyData(tokenizer)\n",
        "dataloader = DataLoader(dummy_data_processed, batch_size=2, shuffle=False)\n",
        "\n",
        "document_inputs_labels = []\n",
        "document_attention_masks = []\n",
        "\n",
        "sentence_config = BertConfig()\n",
        "sentence_model = HATESentenceModel(sentence_config)\n",
        "document_config = BertConfig()\n",
        "#document_model = HATEDocumentModel(document_config)\n",
        "sentence_model_bert = transformers.BertForMaskedLM(sentence_config)\n",
        "\n",
        "\n",
        "\n",
        "# What if we tokenize only on the fly for every batch? I think we don't need universal sentence lengths anymore...\n",
        "for batch in dataloader:\n",
        "  # Get number of sentences in longest doc in the batch\n",
        "  max_doc_length = get_max_doc_length(batch)\n",
        "\n",
        "  # Everything between the two hashtag lines should happen inside HATEModel\n",
        "  ##############################################################################\n",
        "  intermediary_embeddings = []\n",
        "  intermediary_attention_mask = []\n",
        "  intermediary_special_tokens_mask = []\n",
        "  # Iterate over documents and apply the Sentence Model per doc\n",
        "  for input_ids, token_type_ids, attention_mask, special_tokens_mask in zip(batch['input_ids'], batch['token_type_ids'], batch['attention_mask'], batch['special_tokens_mask']):\n",
        "    # Mask input IDs for one document\n",
        "    sentence_embeddings_per_doc = [torch.randn(768)]\n",
        "    attention_mask_per_doc = [0]\n",
        "    special_token_mask_per_doc= [1]\n",
        "\n",
        "    sentence_counter = 0\n",
        "    masking_output = mask_input_ids(input_ids, tokenizer, special_tokens_mask)\n",
        "    sentence_model_output = sentence_model(input_ids=masking_output[0], \n",
        "                                         attention_mask=attention_mask, \n",
        "                                         token_type_ids=token_type_ids,\n",
        "                                         inputs_embeds=None,\n",
        "                                         labels=masking_output[1],\n",
        "                                         output_attentions=True,\n",
        "                                         output_hidden_states=True)\n",
        "    # Iterate over sentence embeddings returned by the model for one document\n",
        "    for hidden_states in sentence_model_output['last_hidden_state']:\n",
        "      # CLS embedding for a sentence at sentence_counter position in the document\n",
        "      # CLS is at position 0 out of 512 of last_hidden_states[sentence_counter]\n",
        "      sentence_embeddings_per_doc.append(sentence_model_output['last_hidden_states'][sentence_counter][0])\n",
        "      attention_mask_per_doc.append()\n",
        "      special_tokens_mask_per_doc.append()\n",
        "      sentence_counter += 1\n",
        "\n",
        "\n",
        "    # TODO need a function to tell us longest doc in batch in order to pad inputs for doc model\n",
        "    intermediary_embeddings.append(torch.stack(sentence_embeddings_per_doc))\n",
        "    intermediary_attention_mask.append(torch.stack(attention_mask_per_doc))\n",
        "    intermediary_special_tokens_mask.append(torch.stack(special_tokens_mask_per_doc))\n",
        "\n",
        "\n",
        "  intermediary_embeddings = torch.stack()\n",
        "  intermediary_attention_mask = torch.stack()\n",
        "  intermediary_special_tokens_mask = torch.stack()\n",
        "\n",
        "  input_embeddings, masked_input_embeddings, label_embeddings, label_mask = mask_input_embeddings(intermediary_embeddings, intermediary_special_tokens_mask)\n",
        "  document_model_output = document_model()\n",
        "  ##############################################################################"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([[[  101,  2023,  2003,  ...,     0,     0,     0],\n",
            "         [  101,  2057,  1005,  ...,     0,     0,     0],\n",
            "         [  101,  1045,  1005,  ...,     0,     0,     0]],\n",
            "\n",
            "        [[  101,  2036,  2057,  ...,     0,     0,     0],\n",
            "         [  101,  2292,  1005,  ...,     0,     0,     0],\n",
            "         [  101, 11504,  1996,  ...,     0,     0,     0]]]), 'token_type_ids': tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]]]), 'attention_mask': tensor([[[1, 1, 1,  ..., 0, 0, 0],\n",
            "         [1, 1, 1,  ..., 0, 0, 0],\n",
            "         [1, 1, 1,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[1, 1, 1,  ..., 0, 0, 0],\n",
            "         [1, 1, 1,  ..., 0, 0, 0],\n",
            "         [1, 1, 1,  ..., 0, 0, 0]]]), 'special_tokens_mask': tensor([[[1, 0, 0,  ..., 1, 1, 1],\n",
            "         [1, 0, 0,  ..., 1, 1, 1],\n",
            "         [1, 0, 0,  ..., 1, 1, 1]],\n",
            "\n",
            "        [[1, 0, 0,  ..., 1, 1, 1],\n",
            "         [1, 0, 0,  ..., 1, 1, 1],\n",
            "         [1, 0, 0,  ..., 1, 1, 1]]])}\n",
            "input ids for one doc:  tensor([[ 101, 2023, 2003,  ...,    0,    0,    0],\n",
            "        [ 101, 2057, 1005,  ...,    0,    0,    0],\n",
            "        [ 101, 1045, 1005,  ...,    0,    0,    0]])\n",
            "last hidden states:  torch.Size([768])\n",
            "input ids for one doc:  tensor([[  101,  2036,  2057,  ...,     0,     0,     0],\n",
            "        [  101,  2292,  1005,  ...,     0,     0,     0],\n",
            "        [  101, 11504,  1996,  ...,     0,     0,     0]])\n",
            "last hidden states:  torch.Size([768])\n",
            "{'input_ids': tensor([[[  101,  1998,  2054,  ...,     0,     0,     0],\n",
            "         [  101, 10166, 19272,  ...,     0,     0,     0],\n",
            "         [  101,  7929,  2292,  ...,     0,     0,     0]]]), 'token_type_ids': tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]]]), 'attention_mask': tensor([[[1, 1, 1,  ..., 0, 0, 0],\n",
            "         [1, 1, 1,  ..., 0, 0, 0],\n",
            "         [1, 1, 1,  ..., 0, 0, 0]]]), 'special_tokens_mask': tensor([[[1, 0, 0,  ..., 1, 1, 1],\n",
            "         [1, 0, 0,  ..., 1, 1, 1],\n",
            "         [1, 0, 0,  ..., 1, 1, 1]]])}\n",
            "input ids for one doc:  tensor([[  101,  1998,  2054,  ...,     0,     0,     0],\n",
            "        [  101, 10166, 19272,  ...,     0,     0,     0],\n",
            "        [  101,  7929,  2292,  ...,     0,     0,     0]])\n",
            "last hidden states:  torch.Size([768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQWBgEbxblTN"
      },
      "source": [
        "def train():\n",
        "\n",
        "  file_path = \"C:\\Users\\Marco Moldovan\\Documents\\Studium\\Bachelorarbeit\\datasets\\enwiki-20200920-pages-articles-multistream1.xml-p1p41242\"\n",
        "\n",
        "  tokenizer = transformers.BertTokenizerFast()\n",
        "\n",
        "  dataset = PretrainingData(file_path, tokenizer)\n",
        "\n",
        "  dataloader = DataLoader(dataset)\n",
        "\n",
        "  model = HateModel(sentence_configuration, document_configuration) # what if we use a pretrained BertModel as the Sentence mode and fix the weights and only train the Document Mode?\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "  outputs = model(data)\n",
        "\n",
        "  loss = criterion(outputs, ground_truth)\n",
        "\n",
        "  loss.backwards()\n",
        "  \n",
        "  # possibly add gradient clipping here: torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "  # https://huggingface.co/transformers/training.html\n",
        "  \n",
        "  optimizer.step()\n",
        "\n",
        "  \"\"\"\n",
        "  Training logic works as follows:\n",
        "  - the dataset consists of documents which consist of sentences which consist\n",
        "    of words\n",
        "  - for every sentence some words are masked and for every document some sentences\n",
        "    are masked\n",
        "  - for every datapoint (which is a document) the model predicts some masked words\n",
        "    and some masked sentences\n",
        "  - these predictions are compared against the ground truth by the criterion \n",
        "    and result in some losses\n",
        "  - the losses for both the masked words and masked sentences are added together\n",
        "  - the combined loss gets backpropagated through the model\n",
        "\n",
        "  Important things to figure out:\n",
        "  - how to build a proper vocabulary for sentences that we can predict over --> take elements from __getitem__().\n",
        "  - how to store ground truth for sentences --> need masked_sentence_loss() function\n",
        "  \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjjlTmQRMz5h"
      },
      "source": [
        "### 5.1.5 Perform Pretraining"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcC8fKiAuzx-"
      },
      "source": [
        "Possibly train here: https://www.rz.ifi.lmu.de/infos/slurm_de.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZunx0yb6v81"
      },
      "source": [
        "# TensorBoard of loss and all other sorts of important info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUDbAwcVUkqJ"
      },
      "source": [
        "### 5.1.6 Save Pretrained Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_Zdk-zSjp8X"
      },
      "source": [
        "Imagine not saving your model and losing all training progress #uff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9qJXR9fju4q"
      },
      "source": [
        "### 5.1.7 Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVJ-Kqp2j0Ro"
      },
      "source": [
        "Write all the trained modules in such a compact way that we can just hand it a document or sentence and it will infer its representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nw1qyXh4rMm"
      },
      "source": [
        "class HATEModel(nn.Module):"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3s9HHqxQlUz"
      },
      "source": [
        "## 5.2 Finetuning on MS MARCO Document Ranking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcvB0xjtQtNE"
      },
      "source": [
        "### 5.2.1 Details on the MS MARCO Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-vDGYbWdz3l"
      },
      "source": [
        "- https://microsoft.github.io/msmarco (has list of other document reranking models)\n",
        "\n",
        "- MS MARCO: A Human Generated MAchine Reading COmprehension Dataset https://arxiv.org/pdf/1611.09268.pdf\n",
        "\n",
        "- https://microsoft.github.io/TREC-2020-Deep-Learning/\n",
        "\n",
        "\n",
        "\n",
        "also relevant for finetuning:\n",
        "\n",
        "- RepBERT: Contextualized Text Embeddings for First-Stage Retrieval https://arxiv.org/pdf/2006.15498.pdf\n",
        "- TwinBERT: Distilling Knowledge to Twin-Structured BERT Models for Efficient Retrieval (1) https://arxiv.org/pdf/2002.06275.pdf (2) https://github.com/deepampatel/TwinBert/blob/master/TwinBert.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyOv4ZUquCdd"
      },
      "source": [
        "Alternative Tasks:\n",
        "- https://research.google/tools/datasets/ Wikipedia and arXiv similarity triplets\n",
        "- https://github.com/LiqunW/Long-document-dataset with this paper: Long Document Classification From Local Word Glimpses via Recurrent Attention Learning https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8675939\n",
        "- https://datasets.quantumstat.com/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-to3CpsX02s"
      },
      "source": [
        "### 5.2.2 Load and Tokenize Finetuning Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlY-Oj9GX-v8"
      },
      "source": [
        "### 5.2.3 Document Ranking Task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22wOTsScwy40"
      },
      "source": [
        "- https://github.com/microsoft/MSMARCO-Document-Ranking\n",
        "\n",
        "- Baseline: Longformer: The Long-Document Transformer (1) https://arxiv.org/pdf/2004.05150.pdf (2) https://github.com/isekulic/longformer-marco/blob/master/src/TransformerMarco.py\n",
        "Baseline: Conformer-Kernel with Query Term Independence for Document Retrieval (1) https://arxiv.org/pdf/2007.10434.pdf "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnjX9LYNahuY"
      },
      "source": [
        "### 5.2.4 *TODO* \n",
        "- loss function for ranking task -> cosine contrastive loss? check this TwinBERT implementation https://github.com/deepampatel/TwinBert/blob/master/TwinBert.ipynb\n",
        "- what training objective is actually used in the finetuning here? is it already a ranking task? could the loss function be the same as in pretraining? \n",
        "- ranking algorithm for the document representations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5tLrkSmQn8_"
      },
      "source": [
        "# **6.** Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hw0eV70xRVyi"
      },
      "source": [
        "## 6.1 Document Ranking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJcXlFRuRYvp"
      },
      "source": [
        "## 6.2 Answer Passage Highlighting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsn-oha4yFWi"
      },
      "source": [
        "# **7.** Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBc__W4vyL5U"
      },
      "source": [
        "# **8.** Outlook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-KmTKws5rqj"
      },
      "source": [
        "- Deeper look at attention: transformers are graph neural networks https://thegradient.pub/transformers-are-graph-neural-networks/\n",
        "- Generalization to Hopfield Nets: (1) https://ml-jku.github.io/hopfield-layers/ (2) http://franksworld.com/2020/08/10/explaining-the-paper-hopfield-networks-is-all-you-need/ (3) https://analyticsindiamag.com/modern-hopfield-network-transformers-attention/ (4) https://towardsdatascience.com/hopfield-networks-are-useless-heres-why-you-should-learn-them-f0930ebeadcd\n",
        "- Implications of these findings for language representation?\n",
        "- Check http://nlp.seas.harvard.edu/code/ for more ideas"
      ]
    }
  ]
}